{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Repo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import os\n",
    "from supabase import create_client\n",
    "import uuid\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import re\n",
    "import tempfile\n",
    "import shutil\n",
    "from git import Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_repo_info(repo_path):\n",
    "    \"\"\"\n",
    "    Extract information from a Git repository.\n",
    "    Returns a dictionary with repo metadata and file contents.\n",
    "    Only processes relevant text files for code analysis.\n",
    "    \"\"\"\n",
    "    repo = None\n",
    "    try:\n",
    "        repo = git.Repo(repo_path)\n",
    "        repo_info = {\n",
    "            'repo_name': os.path.basename(repo_path),\n",
    "            'commit_count': len(list(repo.iter_commits())),\n",
    "            'branches': [branch.name for branch in repo.branches],\n",
    "            'files': []\n",
    "        }\n",
    "        \n",
    "        # Define file extensions to INCLUDE (whitelist approach)\n",
    "        relevant_extensions = {\n",
    "            '.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.cpp', '.c', '.h',\n",
    "            '.cs', '.php', '.rb', '.go', '.rs', '.swift', '.kt', '.scala',\n",
    "            '.html', '.css', '.scss', '.sass', '.less',\n",
    "            '.md', '.txt', '.rst', '.json', '.yaml', '.yml', '.toml',\n",
    "            '.sql', '.sh', '.bat', '.ps1',\n",
    "            '.dockerfile', '.gitignore', '.env.example'\n",
    "        }\n",
    "        \n",
    "        # Define files to INCLUDE by name (regardless of extension)\n",
    "        relevant_filenames = {\n",
    "            'README', 'LICENSE', 'CHANGELOG', 'CONTRIBUTING', 'INSTALL',\n",
    "            'Dockerfile', 'Makefile', 'requirements.txt', 'package.json',\n",
    "            'setup.py', 'pyproject.toml', 'Cargo.toml', 'pom.xml',\n",
    "            'build.gradle', 'composer.json', 'Gemfile'\n",
    "        }\n",
    "        \n",
    "        # Define directories to SKIP\n",
    "        skip_directories = {\n",
    "            'node_modules', '.git', '__pycache__', '.pytest_cache',\n",
    "            'venv', 'env', '.env', 'build', 'dist', 'target',\n",
    "            '.idea', '.vscode', 'logs', 'tmp', 'temp',\n",
    "            'images', 'assets', 'static/images', 'public/images'\n",
    "        }\n",
    "        \n",
    "        # Get file contents from the latest commit in the active branch\n",
    "        tree = repo.head.commit.tree\n",
    "        for item in tree.traverse():\n",
    "            if item.type == 'blob':  # Only process files, not directories\n",
    "                # Skip files in unwanted directories\n",
    "                if any(skip_dir in item.path for skip_dir in skip_directories):\n",
    "                    continue\n",
    "                \n",
    "                # Check if file should be included\n",
    "                file_ext = os.path.splitext(item.path)[1].lower()\n",
    "                filename = os.path.basename(item.path)\n",
    "                filename_no_ext = os.path.splitext(filename)[0].upper()\n",
    "                \n",
    "                # Include if extension or filename matches our criteria\n",
    "                should_include = (\n",
    "                    file_ext in relevant_extensions or\n",
    "                    filename_no_ext in relevant_filenames or\n",
    "                    filename in relevant_filenames\n",
    "                )\n",
    "                \n",
    "                if not should_include:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Try to decode as UTF-8, skip if it fails\n",
    "                    content = item.data_stream.read().decode('utf-8')\n",
    "                    # Skip files with null bytes (binary content)\n",
    "                    if '\\x00' in content:\n",
    "                        continue\n",
    "                    \n",
    "                    # Skip very large files (>100KB)\n",
    "                    if len(content) > 100000:\n",
    "                        continue\n",
    "                    \n",
    "                    file_info = {\n",
    "                        'path': item.path,\n",
    "                        'content': content\n",
    "                    }\n",
    "                    repo_info['files'].append(file_info)\n",
    "                except UnicodeDecodeError:\n",
    "                    # Skip files that can't be decoded as text\n",
    "                    continue\n",
    "        \n",
    "        print(f\"Processed {len(repo_info['files'])} relevant files from {repo_info['repo_name']}\")\n",
    "        return repo_info\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing Git repo: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Close the repository to release file handles\n",
    "        if repo is not None:\n",
    "            repo.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repo_name': 'tmpqv23rcta',\n",
       " 'commit_count': 6,\n",
       " 'branches': ['main'],\n",
       " 'files': [{'path': 'README.md',\n",
       "   'content': '# LinkedIn-Booster 🚀\\n\\nAn intelligent AI-powered LinkedIn post creation workflow that combines the power of LangChain, LangGraph, and multiple AI services to create engaging LinkedIn content with matching visuals.\\n\\n![LangGraph Workflow](images/LangGraph-workflow-diagram.png)\\n\\n## ✨ Features\\n\\n- **🤖 AI-Powered Generation**: Google Gemini for high-quality LinkedIn post drafting\\n- **🔍 Web Research**: Tavily search integration for up-to-date context and insights\\n- **🔄 Interactive Feedback Loop**: Intelligent feedback classification and content refinement\\n- **🎨 Image Generation**: DALL·E 3 integration for compelling visual content\\n- **📧 Email Delivery**: Gmail API integration for seamless content distribution\\n- **🖥️ Multiple Interfaces**: CLI and web-based Chainlit UI options\\n- **📊 Workflow Visualization**: LangGraph Studio integration for workflow monitoring\\n\\n## 🏗️ Architecture\\n\\nThe project follows a modular architecture with clear separation of concerns:\\n\\n```\\nLinkedIn-Booster/\\n├── 📄 main.py                    # CLI interface\\n├── 🌐 app.py                     # Chainlit web interface  \\n├── 📁 workflow/\\n│   ├── __init__.py\\n│   ├── state.py                  # Workflow state management\\n│   ├── tools.py                  # External API integrations (Tavily)\\n│   ├── nodes.py                  # Core workflow nodes\\n│   └── graph.py                  # LangGraph workflow orchestration\\n├── 📁 utils/\\n│   ├── __init__.py\\n│   └── config.py                 # Configuration and environment setup\\n├── 📁 studio/                    # LangGraph Studio configuration\\n├── 📁 notebooks/                 # Development notebooks\\n├── 📁 images/                    # Documentation assets\\n├── 📄 requirements.txt\\n└── 📄 .env                       # Environment variables\\n```\\n\\n## 🚀 Quick Start\\n\\n### Prerequisites\\n- Python 3.8+\\n- API keys for OpenAI, Google Gemini, and Tavily\\n- Gmail API credentials (optional, for email functionality)\\n\\n### Installation\\n\\n1. **Clone and install dependencies**\\n```bash\\ngit clone <repository-url>\\ncd LinkedIn-Booster\\npip install -r requirements.txt\\n```\\n\\n2. **Configure environment variables**\\nCreate a `.env` file in the root directory:\\n```bash\\n# AI Service APIs\\nOPENAI_API_KEY=your_openai_api_key\\nGOOGLE_API_KEY=your_google_gemini_api_key\\nTAVILY_API_KEY=your_tavily_api_key\\n\\n# Gmail Integration (Optional)\\nGMAIL_CREDENTIALS_FILE=credentials.json\\nGMAIL_TOKEN_FILE=token.json\\nRECIPIENT_EMAIL=recipient@example.com\\n```\\n\\n3. **Gmail Setup (Optional)**\\nFor email functionality:\\n- Enable Gmail API in [Google Cloud Console](https://console.cloud.google.com/)\\n- Create OAuth2 credentials and download as `credentials.json`\\n- Place the file in the project root\\n- First run will generate `token.json` automatically\\n\\n## 💻 Usage\\n\\n### Option 1: Web Interface (Recommended)\\nLaunch the interactive Chainlit web interface:\\n\\n```bash\\nchainlit run app.py\\n```\\n\\n![Chainlit Interface](images/ChainLit-user%20Interface.png)\\n\\nThe web interface provides:\\n- 🎯 Guided workflow with clear instructions\\n- 📱 Real-time feedback and refinement\\n- 🖼️ Visual preview of generated content\\n- ✅ Easy approval and sending process\\n\\n### Option 2: Command Line Interface\\nFor quick automation or scripting:\\n\\n```bash\\npython main.py \"Create a post about the importance of drinking water\"\\n```\\n\\n### Option 3: LangGraph Studio\\nFor workflow development and debugging:\\n\\n![LangGraph Studio](images/LangGraphStudio-workflow-diagram.png)\\n\\n```bash\\ncd studio\\nlanggraph dev\\n```\\n\\n### Option 4: n8n Integration\\nFor no-code workflow automation:\\n\\n![n8n Workflow](images/n8n-workflow-diagram.png)\\n\\nImport the workflow from `n8n/main (NO img2img).json` into your n8n instance.\\n\\n## 🔄 Workflow Process\\n\\nThe LinkedIn Booster follows an intelligent multi-step process:\\n\\n1. **📝 Content Creation**: AI generates LinkedIn post based on your topic\\n2. **🎨 Image Generation**: Creates matching visual content using DALL·E 3\\n3. **👀 Review & Feedback**: Present content for user review\\n4. **🤖 Smart Classification**: AI analyzes feedback and determines next action\\n5. **✨ Refinement**: Improves text or image based on feedback\\n6. **✅ Approval & Delivery**: Sends final content via email when approved\\n\\nThe workflow uses LangGraph\\'s stateful execution with strategic interrupts, allowing for human-in-the-loop refinement at key decision points.\\n\\n## 🛠️ Development\\n\\n### Running Tests\\n```bash\\npython test_imports.py\\npython test_create_post.py\\npython send_mail_test.py\\n```\\n\\n### Jupyter Notebook\\nExplore the original development process:\\n```bash\\njupyter notebook notebooks/booster_workflow_v0.2.ipynb\\n```\\n\\n## 📋 Requirements\\n\\nKey dependencies include:\\n- `langchain` - LLM framework and integrations\\n- `langgraph` - Workflow orchestration\\n- `chainlit` - Web interface\\n- `google-api-python-client` - Gmail integration\\n- `openai` - DALL·E 3 image generation\\n- `tavily-python` - Web search capabilities\\n\\nSee `requirements.txt` for complete list.\\n\\n## 🤝 Contributing\\n\\n1. Fork the repository\\n2. Create a feature branch\\n3. Make your changes\\n4. Add tests if applicable\\n5. Submit a pull request\\n\\n## 📄 License\\n\\nMIT License - see LICENSE file for details.\\n'},\n",
       "  {'path': 'app.py',\n",
       "   'content': '\"\"\"Chainlit GUI for LinkedIn Booster workflow.\"\"\"\\nimport chainlit as cl\\nfrom typing import Dict, Any\\nimport asyncio\\nfrom utils import load_env\\nfrom workflow.graph import start_workflow, continue_workflow\\n\\n\\n# Load environment variables\\nload_env()\\n\\n# Store user sessions\\nuser_sessions: Dict[str, Dict[str, Any]] = {}\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    \"\"\"Initialize the chat session.\"\"\"\\n    session_id = cl.user_session.get(\"id\")\\n    user_sessions[session_id] = {\\n        \"thread_id\": f\"chainlit_{session_id}\",\\n        \"current_state\": None,\\n        \"step\": \"initial\"\\n    }\\n    \\n    await cl.Message(\\n        content=\"🚀 **Welcome to LinkedIn Booster!**\\\\n\\\\n\"\\n                \"I\\'ll help you create engaging LinkedIn posts with AI-generated images.\\\\n\\\\n\"\\n                \"**How it works:**\\\\n\"\\n                \"1. Tell me your post topic or idea\\\\n\"\\n                \"2. I\\'ll generate a professional post and matching image\\\\n\"\\n                \"3. You can review and refine both text and image\\\\n\"\\n                \"4. Once approved, I\\'ll send it via email\\\\n\\\\n\"\\n                \"**What would you like to post about today?**\"\\n    ).send()\\n\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n    \"\"\"Handle user messages.\"\"\"\\n    session_id = cl.user_session.get(\"id\")\\n    session = user_sessions.get(session_id, {})\\n    \\n    user_input = message.content.strip()\\n    \\n    if session.get(\"step\") == \"initial\":\\n        # Start the workflow\\n        await handle_initial_request(user_input, session)\\n    else:\\n        # Continue with feedback\\n        await handle_feedback(user_input, session)\\n\\n\\nasync def handle_initial_request(topic: str, session: Dict[str, Any]):\\n    \"\"\"Handle the initial post creation request.\"\"\"\\n    session_id = cl.user_session.get(\"id\")\\n    thread_id = session[\"thread_id\"]\\n    \\n    # Show loading message\\n    loading_msg = await cl.Message(content=\"🔄 Creating your LinkedIn post...\").send()\\n    \\n    try:\\n        # Start the workflow\\n        state = await asyncio.to_thread(start_workflow, topic, thread_id)\\n        session[\"current_state\"] = state\\n        \\n        if state.error_message:\\n            await cl.Message(content=f\"❌ **Error:** {state.error_message}\").send()\\n            return\\n        \\n        # Show success message\\n        await cl.Message(content=\"✅ **Post Created Successfully!**\").send()\\n        \\n        # Display the generated post\\n        await cl.Message(\\n            content=f\"📝 **Generated LinkedIn Post:**\\\\n\\\\n{state.post_text}\"\\n        ).send()\\n        \\n        # Display the image if available\\n        if state.image_url:\\n            await cl.Message(\\n                content=f\"🖼️ **Generated Image:**\\\\n\\\\n![Generated Image]({state.image_url})\"\\n            ).send()\\n        \\n        # Ask for feedback\\n        session[\"step\"] = \"feedback\"\\n        await cl.Message(\\n            content=\"**What would you like to do next?**\\\\n\\\\n\"\\n                   \"• Type **\\'approve\\'** to send the post via email\\\\n\"\\n                   \"• Type **\\'refine text [your suggestions]\\'** to improve the text\\\\n\"\\n                   \"• Type **\\'refine image [your suggestions]\\'** to improve the image\\\\n\"\\n                   \"• Type **\\'quit\\'** to end the session\"\\n        ).send()\\n        \\n    except Exception as e:\\n        await cl.Message(content=f\"❌ **Error:** {str(e)}\").send()\\n\\n\\nasync def handle_feedback(feedback: str, session: Dict[str, Any]):\\n    \"\"\"Handle user feedback on the generated content.\"\"\"\\n    session_id = cl.user_session.get(\"id\")\\n    thread_id = session[\"thread_id\"]\\n    \\n    if feedback.lower() in {\"quit\", \"exit\", \"q\"}:\\n        await cl.Message(content=\"👋 **Thanks for using LinkedIn Booster!** Have a great day!\").send()\\n        session[\"step\"] = \"completed\"\\n        return\\n    \\n    # Show processing message\\n    await cl.Message(content=\"🔄 Processing your feedback...\").send()\\n    \\n    try:\\n        # Continue the workflow with feedback\\n        state = await asyncio.to_thread(continue_workflow, feedback, thread_id)\\n        session[\"current_state\"] = state\\n        \\n        if state.error_message:\\n            await cl.Message(content=f\"❌ **Error:** {state.error_message}\").send()\\n            return\\n        \\n        # Handle different workflow states\\n        if state.current_step == \"completed\":\\n            await cl.Message(content=\"✅ **Email sent successfully!**\").send()\\n            await cl.Message(\\n                content=\"🎉 **Workflow Completed!**\\\\n\\\\n\"\\n                       \"Your LinkedIn post has been sent via email. \"\\n                       \"You can start a new post by refreshing the page.\"\\n            ).send()\\n            session[\"step\"] = \"completed\"\\n            \\n        elif state.current_step == \"post_content\" and state.classification == \"Approved\":\\n            # This means the post was approved, trigger email sending\\n            await cl.Message(content=\"✅ **Post approved! Sending email...**\").send()\\n            # The workflow should automatically proceed to send_email\\n            \\n        elif state.current_step == \"terminate\":\\n            await cl.Message(content=\"🛑 **Workflow terminated.**\").send()\\n            await cl.Message(\\n                content=\"The workflow has been terminated. \"\\n                       \"You can start a new post by refreshing the page.\"\\n            ).send()\\n            session[\"step\"] = \"completed\"\\n            \\n        elif state.current_step == \"wait_feedback\":\\n            await cl.Message(content=\"✅ **Content updated!**\").send()\\n            \\n            # Always show the current content after updates\\n            if state.post_text:\\n                await cl.Message(\\n                    content=f\"📝 **Updated LinkedIn Post:**\\\\n\\\\n{state.post_text}\"\\n                ).send()\\n            \\n            if state.image_url:\\n                await cl.Message(\\n                    content=f\"🖼️ **Updated Image:**\\\\n\\\\n![Updated Image]({state.image_url})\"\\n                ).send()\\n            \\n            # Ask for next action\\n            await cl.Message(\\n                content=\"**What would you like to do next?**\\\\n\\\\n\"\\n                       \"• Type **\\'approve\\'** to send the post via email\\\\n\"\\n                       \"• Type **\\'refine text [your suggestions]\\'** to improve the text further\\\\n\"\\n                       \"• Type **\\'refine image [your suggestions]\\'** to improve the image further\\\\n\"\\n                       \"• Type **\\'quit\\'** to end the session\"\\n            ).send()\\n        \\n        else:\\n            # Handle any other states or show current content\\n            await cl.Message(content=f\"**Current Status:** {state.current_step}\").send()\\n            if state.classification:\\n                await cl.Message(content=f\"**Classification:** {state.classification}\").send()\\n            \\n            # Show current content\\n            if state.post_text:\\n                await cl.Message(\\n                    content=f\"📝 **Current LinkedIn Post:**\\\\n\\\\n{state.post_text}\"\\n                ).send()\\n            \\n            if state.image_url:\\n                await cl.Message(\\n                    content=f\"🖼️ **Current Image:**\\\\n\\\\n![Current Image]({state.image_url})\"\\n                ).send()\\n            \\n            # Ask for next action\\n            await cl.Message(\\n                content=\"**What would you like to do next?**\\\\n\\\\n\"\\n                       \"• Type **\\'approve\\'** to send the post via email\\\\n\"\\n                       \"• Type **\\'refine text [your suggestions]\\'** to improve the text\\\\n\"\\n                       \"• Type **\\'refine image [your suggestions]\\'** to improve the image\\\\n\"\\n                       \"• Type **\\'quit\\'** to end the session\"\\n            ).send()\\n        \\n    except Exception as e:\\n        await cl.Message(content=f\"❌ **Error:** {str(e)}\").send()\\n\\n\\n@cl.on_chat_end\\nasync def end():\\n    \"\"\"Clean up when chat ends.\"\"\"\\n    session_id = cl.user_session.get(\"id\")\\n    if session_id in user_sessions:\\n        del user_sessions[session_id]'},\n",
       "  {'path': 'main.py',\n",
       "   'content': 'import sys\\nfrom utils import load_env\\nfrom workflow.graph import start_workflow, continue_workflow\\n\\n\\ndef main() -> None:\\n    load_env()\\n    if len(sys.argv) < 2:\\n        print(\"Usage: python main.py \\\\\"Your post topic...\\\\\"\")\\n        sys.exit(1)\\n    topic = sys.argv[1]\\n    thread_id = \"cli\"\\n    state = start_workflow(topic, thread_id)\\n    if state.current_step != \"wait_feedback\":\\n        print(f\"Error: {state.error_message}\")\\n        sys.exit(1)\\n    print(\"\\\\nGenerated Post:\\\\n\")\\n    print(state.post_text)\\n    print(\"\\\\nImage URL:\\\\n\")\\n    print(state.image_url)\\n    while True:\\n        feedback = input(\"\\\\nFeedback (approve/refine text .../refine image .../quit): \").strip()\\n        if feedback.lower() in {\"quit\", \"exit\", \"q\"}:\\n            print(\"Goodbye!\")\\n            break\\n        state = continue_workflow(feedback, thread_id)\\n        if state.current_step == \"completed\":\\n            print(\"Email sent. Workflow completed.\")\\n            break\\n        if state.current_step == \"terminate\":\\n            print(\"Workflow terminated.\")\\n            break\\n        if state.current_step == \"wait_feedback\":\\n            if state.classification == \"Refine Text\":\\n                print(\"\\\\nUpdated Post:\\\\n\")\\n                print(state.post_text)\\n            elif state.classification == \"Refine Image\":\\n                print(\"\\\\nUpdated Image URL:\\\\n\")\\n                print(state.image_url)\\n\\n\\nif __name__ == \"__main__\":\\n    main()'},\n",
       "  {'path': 'requirements.txt',\n",
       "   'content': 'chainlit>=1.0.0\\nlangchain>=0.1.0\\nlangchain-community>=0.1.0\\nlangchain-google-genai>=1.0.0\\nlangchain-google-community>=0.1.0\\nlanggraph>=0.0.40\\nrequests>=2.31.0\\ntyping-extensions>=4.5.0\\npython-dotenv>=1.0.0\\ngoogle-auth>=2.0.0\\ngoogle-auth-oauthlib>=1.0.0\\ngoogle-auth-httplib2>=0.2.0\\ngoogle-api-python-client>=2.0.0\\nopenai>=1.0.0\\ntavily-python>=0.3.0'},\n",
       "  {'path': 'n8n/main (NO img2img).json',\n",
       "   'content': '{\\n  \"name\": \"main (NO img2img)\",\\n  \"nodes\": [\\n    {\\n      \"parameters\": {\\n        \"options\": {}\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatGoogleGemini\",\\n      \"typeVersion\": 1,\\n      \"position\": [\\n        832,\\n        352\\n      ],\\n      \"id\": \"575c62c0-07d4-4c73-9d70-4fdafe125e86\",\\n      \"name\": \"Google Gemini Chat Model\",\\n      \"credentials\": {\\n        \"googlePalmApi\": {\\n          \"id\": \"mP5d64x2rkeTRzbi\",\\n          \"name\": \"Google Gemini(PaLM) Api account 2\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"toolDescription\": \"Web search tool.\",\\n        \"method\": \"POST\",\\n        \"url\": \"https://api.tavily.com/search\",\\n        \"sendHeaders\": true,\\n        \"headerParameters\": {\\n          \"parameters\": [\\n            {\\n              \"name\": \"Authorization\",\\n              \"value\": \"Bearer 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\"\\n            }\\n          ]\\n        },\\n        \"sendBody\": true,\\n        \"bodyParameters\": {\\n          \"parameters\": [\\n            {\\n              \"name\": \"query\",\\n              \"value\": \"={{ /*n8n-auto-generated-fromAI-override*/ $fromAI(\\'parameters0_Value\\', ``, \\'string\\') }}\"\\n            }\\n          ]\\n        },\\n        \"options\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.httpRequestTool\",\\n      \"typeVersion\": 4.2,\\n      \"position\": [\\n        -496,\\n        -832\\n      ],\\n      \"id\": \"f6bfb373-1e88-448b-8dfd-da7c90016a8d\",\\n      \"name\": \"Tavily\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"inputText\": \"={{ $json.data.text }}\",\\n        \"categories\": {\\n          \"categories\": [\\n            {\\n              \"category\": \"Approved\",\\n              \"description\": \"Approval message refers that its ok to proceed with the current status.\"\\n            },\\n            {\\n              \"category\": \"Refine Text\",\\n              \"description\": \"Request to Refine the post text\"\\n            },\\n            {\\n              \"category\": \"Terminate\",\\n              \"description\": \"Termination request for the process.\"\\n            },\\n            {\\n              \"category\": \"Refine Image\",\\n              \"description\": \"Request to Refine the post Image\"\\n            }\\n          ]\\n        },\\n        \"options\": {}\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.textClassifier\",\\n      \"typeVersion\": 1.1,\\n      \"position\": [\\n        624,\\n        -464\\n      ],\\n      \"id\": \"abf5e791-c5f8-4b32-8a4b-58804e853434\",\\n      \"name\": \"Text Classifier\"\\n    },\\n    {\\n      \"parameters\": {},\\n      \"type\": \"n8n-nodes-base.noOp\",\\n      \"typeVersion\": 1,\\n      \"position\": [\\n        1296,\\n        64\\n      ],\\n      \"id\": \"04ec9776-b607-4c47-8140-bfeb62086c9c\",\\n      \"name\": \"No Operation, do nothing\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"authentication\": \"communityManagement\",\\n        \"person\": \"=Mohamed Mowina\",\\n        \"text\": \"={{ $(\\'Post writer\\').item.json.output }}\",\\n        \"shareMediaCategory\": \"IMAGE\",\\n        \"additionalFields\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.linkedIn\",\\n      \"typeVersion\": 1,\\n      \"position\": [\\n        1616,\\n        -1008\\n      ],\\n      \"id\": \"c56079bb-4e3e-471b-819c-3a6f73f2496b\",\\n      \"name\": \"Create a post\",\\n      \"disabled\": true\\n    },\\n    {\\n      \"parameters\": {\\n        \"updates\": [\\n          \"message\"\\n        ],\\n        \"additionalFields\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.telegramTrigger\",\\n      \"typeVersion\": 1.2,\\n      \"position\": [\\n        -1168,\\n        -1008\\n      ],\\n      \"id\": \"c6d21d0a-52b5-405d-b16d-e7568dd8e839\",\\n      \"name\": \"Telegram Trigger\",\\n      \"webhookId\": \"931413d8-41c9-43f0-85bc-1b1b1fb80fa4\",\\n      \"credentials\": {\\n        \"telegramApi\": {\\n          \"id\": \"EFoDusKQAfWdyYaO\",\\n          \"name\": \"Telegram account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"promptType\": \"define\",\\n        \"text\": \"={{ $json.message.text }}\",\\n        \"options\": {\\n          \"systemMessage\": \"=Your tone should be:\\\\n\\\\nProfessional yet approachable: Confident and authoritative, but also conversational and relatable.\\\\n\\\\nConcise and clear: Get straight to the point while using compelling language.\\\\n\\\\nAction-oriented: Focus on providing value and prompting engagement.\\\\n\\\\nCore Functionality\\\\nYou will perform the following tasks in a structured workflow:\\\\n\\\\nInput Analysis & Goal Clarification:\\\\n\\\\nUpon receiving a request, first identify the core topic and the user\\'s primary goal (e.g., generate leads, build thought leadership, share a company update, get job offers).\\\\n\\\\nIdentify the target audience and their pain points or interests.\\\\n\\\\nRecognize any specific constraints (e.g., character limit, inclusion of a specific link, mention of a name).\\\\n\\\\nStrategy & Brainstorming:\\\\n\\\\nBased on your analysis, brainstorm a minimum of three potential angles or hooks for the post. Each hook should be designed to capture attention and align with the user\\'s goal.\\\\n\\\\nConsider different post formats: a personal story, a listicle, a thought-provoking question, a concise tip, a problem/solution narrative.\\\\n\\\\nDetermine the most effective approach for the given topic and goal.\\\\n\\\\nDraft Generation:\\\\n\\\\nWrite a draft of the post using the chosen angle. Follow these rules for the highest quality:\\\\n\\\\nHook: Start with a strong, attention-grabbing first sentence.\\\\n\\\\nBody: Break up text into short, single-sentence paragraphs. Use line breaks frequently for scannability.\\\\n\\\\nElegance: Incorporate relevant emojis to break up text and add personality, but use them sparingly and strategically (1-3 emojis is usually sufficient).\\\\n\\\\nValue: Provide clear, actionable value, a unique perspective, or a compelling story.\\\\n\\\\nCall-to-Action (CTA): End with a clear, concise CTA that encourages engagement (e.g., \\\\\"What are your thoughts?\\\\\", \\\\\"Share your experience below!\\\\\", \\\\\"Comment \\'Yes\\' if you agree.\\\\\").\\\\n\\\\nHashtags: Suggest 3-5 relevant, specific, and popular hashtags at the end of the post.\\\\n\\\\nOutput:\\\\n\\\\nDon\\'t forget you Output only the post, Do not ask flowup questions and don\\'t provide other suggestions.\\\\n\"\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\\n      \"typeVersion\": 2.2,\\n      \"position\": [\\n        -848,\\n        -1008\\n      ],\\n      \"id\": \"2cac353e-25c0-4125-8b49-c13448885f92\",\\n      \"name\": \"Post writer\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"promptType\": \"define\",\\n        \"text\": \"=Original post:  {{ $(\\'Send a photo message\\').item.json.result.caption }}\\\\n\\\\nRequest: {{ $json.data.text }}\",\\n        \"options\": {\\n          \"systemMessage\": \"You are a helpful assistant who improve and refine Linkedin post.\\\\n\\\\nOutput only the post text.\"\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\\n      \"typeVersion\": 2.2,\\n      \"position\": [\\n        1552,\\n        -368\\n      ],\\n      \"id\": \"49ee2b7d-5cac-4a5d-9696-8d835045a536\",\\n      \"name\": \"Post refiner\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"operation\": \"sendAndWait\",\\n        \"chatId\": \"={{ $(\\'Telegram Trigger\\').item.json.message.chat.id }}\",\\n        \"message\": \"=Your LinkedIn post draft is ready (text + image).\\\\nYou can now decide what to do next. Simply type your instruction in plain language, and the system will handle it.\\\\nExamples:\\\\n\\\\n‘Looks good, post it.’ → Post\\\\n\\\\n‘Change the text, it feels too formal.’ → Edit text\\\\n\\\\n‘The image should focus more on teamwork.’ → Edit image\\\\n\\\\n‘Cancel this, I don’t want to continue.’ → Terminate\\\\n\\\\n👉 You are free to write anything — the system will classify your input and act accordingly.\",\\n        \"responseType\": \"freeText\",\\n        \"options\": {\\n          \"appendAttribution\": false\\n        }\\n      },\\n      \"type\": \"n8n-nodes-base.telegram\",\\n      \"typeVersion\": 1.2,\\n      \"position\": [\\n        320,\\n        112\\n      ],\\n      \"id\": \"07eb16f1-1579-41e4-8443-c2ef5445eb45\",\\n      \"name\": \"Wait for feedback\",\\n      \"webhookId\": \"a39f1d60-d759-43ca-b5b1-9e0b565f9e8a\",\\n      \"credentials\": {\\n        \"telegramApi\": {\\n          \"id\": \"EFoDusKQAfWdyYaO\",\\n          \"name\": \"Telegram account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Create post\\'s text\",\\n        \"height\": 448,\\n        \"width\": 640,\\n        \"color\": 7\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        -976,\\n        -1104\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"22b6a2cf-2715-479b-81d5-ce549dc5c223\",\\n      \"name\": \"Sticky Note\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Create post\\'s Image\",\\n        \"height\": 448,\\n        \"width\": 640,\\n        \"color\": 7\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        -752,\\n        -576\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"9ece431d-1cba-486c-abff-28e2d035c987\",\\n      \"name\": \"Sticky Note1\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Feedback\",\\n        \"height\": 448,\\n        \"width\": 816\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        -352,\\n        -48\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"3dd8f3c0-3ebf-45b4-ad4d-c2ce905863dd\",\\n      \"name\": \"Sticky Note2\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Terminate operation\",\\n        \"height\": 352,\\n        \"width\": 368,\\n        \"color\": 3\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1152,\\n        -96\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"f507679e-8023-4ee4-9a91-f067273627c9\",\\n      \"name\": \"Sticky Note3\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Post\",\\n        \"height\": 272,\\n        \"width\": 336,\\n        \"color\": 4\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1152,\\n        -1088\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"b9503555-ca57-49dc-8d12-d9ec43450bd4\",\\n      \"name\": \"Sticky Note4\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Refine post\",\\n        \"height\": 688,\\n        \"width\": 848,\\n        \"color\": 5\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1152,\\n        -800\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"4597f92e-43d9-4e6f-9aa9-10036656f7e5\",\\n      \"name\": \"Sticky Note5\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"## Refine Image\",\\n        \"height\": 272,\\n        \"width\": 576,\\n        \"color\": 6\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1376,\\n        -784\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"e0a0047b-199a-4590-9c25-155788058cb5\",\\n      \"name\": \"Sticky Note6\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"## Refine Text\",\\n        \"height\": 256,\\n        \"width\": 368,\\n        \"color\": 6\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1472,\\n        -432\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"c603d205-3fe1-409d-9ef4-aa4b4b44f86e\",\\n      \"name\": \"Sticky Note7\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"sendTo\": \"mohammed-mowina@outlook.com\",\\n        \"subject\": \"LinkedIn Booster\",\\n        \"emailType\": \"text\",\\n        \"message\": \"={{ $(\\'Merge\\').item.json.output }}\",\\n        \"options\": {\\n          \"appendAttribution\": false,\\n          \"attachmentsUi\": {\\n            \"attachmentsBinary\": [\\n              {\\n                \"property\": \"={{ $(\\'Merge\\').item.json.fileName }}\"\\n              }\\n            ]\\n          }\\n        }\\n      },\\n      \"type\": \"n8n-nodes-base.gmail\",\\n      \"typeVersion\": 2.1,\\n      \"position\": [\\n        1360,\\n        -992\\n      ],\\n      \"id\": \"b9b5dfed-369a-4e5d-86e5-8e8324b820a6\",\\n      \"name\": \"Send a message\",\\n      \"webhookId\": \"9e818c1d-3573-4592-8465-6953a6754cef\",\\n      \"credentials\": {\\n        \"gmailOAuth2\": {\\n          \"id\": \"Z3qIvY9PLNzWAis2\",\\n          \"name\": \"Gmail account 2\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"operation\": \"sendPhoto\",\\n        \"chatId\": \"={{ $(\\'Telegram Trigger\\').item.json.message.chat.id }}\",\\n        \"binaryData\": true,\\n        \"additionalFields\": {\\n          \"caption\": \"={{ $(\\'Post\\').item.json.output }}\"\\n        }\\n      },\\n      \"type\": \"n8n-nodes-base.telegram\",\\n      \"typeVersion\": 1.2,\\n      \"position\": [\\n        96,\\n        112\\n      ],\\n      \"id\": \"f6891562-43fb-4bb4-9d63-4494ef2e8b51\",\\n      \"name\": \"Send a photo message\",\\n      \"webhookId\": \"94294c72-0650-4b8b-8444-c6ec80b12d2a\",\\n      \"credentials\": {\\n        \"telegramApi\": {\\n          \"id\": \"EFoDusKQAfWdyYaO\",\\n          \"name\": \"Telegram account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"assignments\": {\\n          \"assignments\": [\\n            {\\n              \"id\": \"ef82052d-4309-4359-aa18-b6f2c8313036\",\\n              \"name\": \"output\",\\n              \"value\": \"={{ $json.output }}\",\\n              \"type\": \"string\"\\n            }\\n          ]\\n        },\\n        \"options\": {\\n          \"ignoreConversionErrors\": true,\\n          \"dotNotation\": true\\n        }\\n      },\\n      \"type\": \"n8n-nodes-base.set\",\\n      \"typeVersion\": 3.4,\\n      \"position\": [\\n        -304,\\n        112\\n      ],\\n      \"id\": \"f2baa1ec-2bcb-466e-881b-eb8f8b926d6e\",\\n      \"name\": \"Post\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"promptType\": \"define\",\\n        \"text\": \"=Old Prompt: {{ $(\\'AI Prompt Writer\\').item.json.output }}\\\\nUser comment: {{ $(\\'Wait for feedback\\').item.json.data.text }}\",\\n        \"options\": {\\n          \"systemMessage\": \"=**Overview**:\\\\nYou are an expert LinkedIn Image Prompt Engineer. For each request, generate AI image prompts following this structure. You work in a workflow and your responsability is to refine an already generated image by making a modified propmt to the Image Generator Model.\\\\n\\\\n*Input*:\\\\n- The old prompt for the generated Image.\\\\n- The user comment on the Image to change it.\\\\n\\\\n*Instructions*:\\\\nA modern, minimalistic professional LinkedIn illustration about the topic\\\\nAvoid text images just some\\\\n\\\\n*Style*:\\\\n3D Object Generator with clean flat vector, LinkedIn color palette (blue, white, grey), simple, corporate-appropriate. \\\\nAvoid clutter, make it inspiring and easy to understand.\\\\n\\\\n*Output*:\\\\nOnly output the Prompt no other suggestions or flowup questions.\"\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\\n      \"typeVersion\": 2.2,\\n      \"position\": [\\n        1456,\\n        -704\\n      ],\\n      \"id\": \"b464c76d-0cde-41f1-ab56-7e3f4690bdd7\",\\n      \"name\": \"AI Prompt Refiner\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"promptType\": \"define\",\\n        \"text\": \"={{ $json.output }}\",\\n        \"options\": {\\n          \"systemMessage\": \"=**Overview**:\\\\nYou are an expert LinkedIn Image Prompt Engineer. For each request, generate AI image prompts following this structure.\\\\n\\\\n*Input*:\\\\nYou will have the post\\'s text as the user Prompt.\\\\n\\\\n*Instructions*:\\\\nA modern, minimalistic professional LinkedIn illustration about the topic\\\\nAvoid text images just some\\\\n\\\\n*Style*:\\\\n3D Object Generator with clean flat vector, LinkedIn color palette (blue, white, grey), simple, corporate-appropriate. \\\\nAvoid clutter, make it inspiring and easy to understand.\\\\n\\\\n*Output*:\\\\nOnly output the Prompt no other suggestions or flowup questions.\"\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\\n      \"typeVersion\": 2.2,\\n      \"position\": [\\n        -656,\\n        -480\\n      ],\\n      \"id\": \"df8b4dfb-b2b9-4e65-bf2d-108a6fb99294\",\\n      \"name\": \"AI Prompt Writer\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"resource\": \"image\",\\n        \"prompt\": \"={{ $json.output }}\",\\n        \"options\": {\\n          \"returnImageUrls\": false\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.openAi\",\\n      \"typeVersion\": 1.8,\\n      \"position\": [\\n        -352,\\n        -480\\n      ],\\n      \"id\": \"85253bd1-a207-42be-8510-bec86925c7d9\",\\n      \"name\": \"OpenAI Post Image Generation\",\\n      \"credentials\": {\\n        \"openAiApi\": {\\n          \"id\": \"WSrQfOLqmZvD07lC\",\\n          \"name\": \"OpenAi account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"resource\": \"image\",\\n        \"prompt\": \"={{ $json.output }}\",\\n        \"options\": {\\n          \"returnImageUrls\": false\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.openAi\",\\n      \"typeVersion\": 1.8,\\n      \"position\": [\\n        1760,\\n        -704\\n      ],\\n      \"id\": \"a9bb6412-f5a5-4514-881f-20f2f19643e8\",\\n      \"name\": \"Regenerate an image\",\\n      \"credentials\": {\\n        \"openAiApi\": {\\n          \"id\": \"WSrQfOLqmZvD07lC\",\\n          \"name\": \"OpenAi account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"mode\": \"combine\",\\n        \"combineBy\": \"combineByPosition\",\\n        \"options\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.merge\",\\n      \"typeVersion\": 3.2,\\n      \"position\": [\\n        -96,\\n        112\\n      ],\\n      \"id\": \"33ab8e06-59b3-4428-9fb6-d376bd565ace\",\\n      \"name\": \"Merge\",\\n      \"retryOnFail\": false\\n    },\\n    {\\n      \"parameters\": {\\n        \"mode\": \"combine\",\\n        \"combineBy\": \"combineByPosition\",\\n        \"options\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.merge\",\\n      \"typeVersion\": 3.2,\\n      \"position\": [\\n        1200,\\n        -992\\n      ],\\n      \"id\": \"9e582664-2a82-469a-b642-20bdf2d52f01\",\\n      \"name\": \"Merge1\",\\n      \"retryOnFail\": false\\n    }\\n  ],\\n  \"pinData\": {},\\n  \"connections\": {\\n    \"Google Gemini Chat Model\": {\\n      \"ai_languageModel\": [\\n        [\\n          {\\n            \"node\": \"Post writer\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"Text Classifier\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"Post refiner\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"AI Prompt Writer\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"AI Prompt Refiner\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Tavily\": {\\n      \"ai_tool\": [\\n        [\\n          {\\n            \"node\": \"Post writer\",\\n            \"type\": \"ai_tool\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Text Classifier\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Merge1\",\\n            \"type\": \"main\",\\n            \"index\": 1\\n          }\\n        ],\\n        [\\n          {\\n            \"node\": \"Post refiner\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ],\\n        [\\n          {\\n            \"node\": \"No Operation, do nothing\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ],\\n        [\\n          {\\n            \"node\": \"AI Prompt Refiner\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Telegram Trigger\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Post writer\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Post writer\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"AI Prompt Writer\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"Post\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Post refiner\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Post\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Wait for feedback\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Text Classifier\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Create a post\": {\\n      \"main\": [\\n        []\\n      ]\\n    },\\n    \"Send a photo message\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Wait for feedback\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Post\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Merge\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"AI Prompt Refiner\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Regenerate an image\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"AI Prompt Writer\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"OpenAI Post Image Generation\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"OpenAI Post Image Generation\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Merge\",\\n            \"type\": \"main\",\\n            \"index\": 1\\n          }\\n        ]\\n      ]\\n    },\\n    \"Regenerate an image\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Merge\",\\n            \"type\": \"main\",\\n            \"index\": 1\\n          }\\n        ]\\n      ]\\n    },\\n    \"Merge\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Send a photo message\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"Merge1\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Merge1\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Send a message\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    }\\n  },\\n  \"active\": false,\\n  \"settings\": {\\n    \"executionOrder\": \"v1\"\\n  },\\n  \"versionId\": \"d01124c9-2c7a-40fc-ac9e-7edaa4535fc4\",\\n  \"meta\": {\\n    \"templateCredsSetupCompleted\": true,\\n    \"instanceId\": \"e42899c7dfda6042a4f9f2f2b7e15acf58d2ab8ee4dfbe7dec48ee2a3be4e756\"\\n  },\\n  \"id\": \"dlis4ObYbraVh6Bt\",\\n  \"tags\": []\\n}'},\n",
       "  {'path': 'studio/__init__.py',\n",
       "   'content': '# This makes studio a proper Python package'},\n",
       "  {'path': 'studio/graph.py',\n",
       "   'content': '\"\"\"Standalone graph file for LangGraph Studio.\"\"\"\\nimport sys\\nimport os\\nfrom typing import Optional\\nfrom dataclasses import dataclass\\n\\n# Add the parent directory to the path so we can import from workflow\\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\\n\\nfrom langgraph.graph import StateGraph, END\\n\\n# Define state directly to avoid import conflicts\\n@dataclass\\nclass WorkflowState:\\n    \"\"\"State management for the LinkedIn post creation workflow.\"\"\"\\n    user_input: str = \"\"\\n    post_text: str = \"\"\\n    image_prompt: str = \"\"\\n    image_data: Optional[bytes] = None\\n    image_url: str = \"\"\\n    user_feedback: str = \"\"\\n    classification: str = \"\"\\n    refined_text: str = \"\"\\n    refined_image_data: Optional[bytes] = None\\n    current_step: str = \"start\"\\n    error_message: str = \"\"\\n    final_post_ready: bool = False\\n\\n# Import nodes after defining state\\ntry:\\n    from workflow.nodes import (\\n        create_post_text,\\n        generate_image,\\n        classify_feedback,\\n        refine_text,\\n        refine_image,\\n        send_email,\\n    )\\nexcept ImportError as e:\\n    # Fallback dummy functions for studio\\n    def create_post_text(state: WorkflowState) -> WorkflowState:\\n        state.post_text = \"Sample LinkedIn post created\"\\n        return state\\n    \\n    def generate_image(state: WorkflowState) -> WorkflowState:\\n        state.image_url = \"https://example.com/image.jpg\"\\n        return state\\n    \\n    def classify_feedback(state: WorkflowState) -> WorkflowState:\\n        state.classification = \"approved\"\\n        return state\\n    \\n    def refine_text(state: WorkflowState) -> WorkflowState:\\n        state.refined_text = \"Refined post text\"\\n        return state\\n    \\n    def refine_image(state: WorkflowState) -> WorkflowState:\\n        state.refined_image_data = b\"refined_image_data\"\\n        return state\\n    \\n    def send_email(state: WorkflowState) -> WorkflowState:\\n        state.final_post_ready = True\\n        return state\\n\\n\\ndef route_feedback(state: WorkflowState) -> str:\\n    \"\"\"Route based on feedback classification.\"\"\"\\n    classification = state.classification.lower()\\n    if \"approved\" in classification:\\n        return \"approved\"\\n    if \"refine text\" in classification or \"refine_text\" in classification:\\n        return \"refine_text\"\\n    if \"refine image\" in classification or \"refine_image\" in classification:\\n        return \"refine_image\"\\n    if \"terminate\" in classification:\\n        return \"terminate\"\\n    return \"terminate\"\\n\\n\\ndef create_studio_workflow():\\n    \"\"\"Create the LangGraph workflow for Studio (without checkpointer).\"\"\"\\n    workflow = StateGraph(WorkflowState)\\n    workflow.add_node(\"create_post\", create_post_text)\\n    workflow.add_node(\"generate_image\", generate_image)\\n    workflow.add_node(\"classify_feedback\", classify_feedback)\\n    workflow.add_node(\"refine_text\", refine_text)\\n    workflow.add_node(\"refine_image\", refine_image)\\n    workflow.add_node(\"send_email\", send_email)\\n    workflow.set_entry_point(\"create_post\")\\n    workflow.add_edge(\"create_post\", \"generate_image\")\\n    workflow.add_edge(\"generate_image\", \"classify_feedback\")\\n    workflow.add_conditional_edges(\\n        \"classify_feedback\",\\n        route_feedback,\\n        {\\n            \"approved\": \"send_email\",\\n            \"refine_text\": \"refine_text\",\\n            \"refine_image\": \"refine_image\",\\n            \"terminate\": END,\\n        },\\n    )\\n    workflow.add_edge(\"refine_text\", \"classify_feedback\")\\n    workflow.add_edge(\"refine_image\", \"classify_feedback\")\\n    workflow.add_edge(\"send_email\", END)\\n    \\n    # Compile without checkpointer for LangGraph Studio\\n    app = workflow.compile(\\n        interrupt_after=[\"generate_image\", \"refine_text\", \"refine_image\"],\\n    )\\n    return app\\n\\n\\n# Create the app for LangGraph Studio\\napp = create_studio_workflow()\\n\\n# Export the app for LangGraph Studio\\n__all__ = [\"app\"]'},\n",
       "  {'path': 'studio/langgraph.json',\n",
       "   'content': '{\\n    \"dockerfile_lines\": [],\\n    \"graphs\": {\\n      \"chatbot\": \"./graph.py:app\"\\n    },\\n    \"env\": \"./.env\",\\n    \"python_version\": \"3.11\",\\n    \"dependencies\": [\\n      \".\"\\n    ]\\n  }'},\n",
       "  {'path': 'utils/config.py',\n",
       "   'content': 'import os\\nfrom dataclasses import dataclass\\nfrom dotenv import load_dotenv\\n\\n\\ndef load_env() -> None:\\n    \"\"\"Load environment variables from a .env file if present.\"\"\"\\n    load_dotenv()\\n\\n\\n@dataclass\\nclass Config:\\n    OPENAI_API_KEY: str | None = os.getenv(\"OPENAI_API_KEY\")\\n    GOOGLE_API_KEY: str | None = os.getenv(\"GOOGLE_API_KEY\")\\n    TAVILY_API_KEY: str | None = os.getenv(\"TAVILY_API_KEY\")\\n    GMAIL_CREDENTIALS_FILE: str = os.getenv(\"GMAIL_CREDENTIALS_FILE\", \"credentials.json\")\\n    GMAIL_TOKEN_FILE: str = os.getenv(\"GMAIL_TOKEN_FILE\", \"token.json\")\\n    RECIPIENT_EMAIL: str = os.getenv(\"RECIPIENT_EMAIL\", \"mohammed-mowina@outlook.com\")\\n\\n\\n'},\n",
       "  {'path': 'workflow/__init__.py',\n",
       "   'content': '\"\"\"LinkedIn Booster workflow package.\"\"\"\\n\\nfrom .graph import app, create_workflow, start_workflow, continue_workflow\\n\\n__all__ = [\"app\", \"create_workflow\", \"start_workflow\", \"continue_workflow\"]'},\n",
       "  {'path': 'workflow/graph.py',\n",
       "   'content': 'from langgraph.graph import StateGraph, END\\nfrom langgraph.checkpoint.memory import MemorySaver\\n\\nfrom workflow.state import WorkflowState\\nfrom workflow.nodes import (\\n    create_post_text,\\n    generate_image,\\n    classify_feedback,\\n    refine_text,\\n    refine_image,\\n    send_email,\\n)\\n\\n\\ndef route_feedback(state: WorkflowState) -> str:\\n    \"\"\"Route based on feedback classification.\"\"\"\\n    classification = state.classification.lower()\\n    if \"approved\" in classification:\\n        return \"approved\"\\n    if \"refine text\" in classification or \"refine_text\" in classification:\\n        return \"refine_text\"\\n    if \"refine image\" in classification or \"refine_image\" in classification:\\n        return \"refine_image\"\\n    if \"terminate\" in classification:\\n        return \"terminate\"\\n    return \"terminate\"\\n\\n\\ndef create_workflow():\\n    \"\"\"Create the LangGraph workflow with interrupts configured.\"\"\"\\n    workflow = StateGraph(WorkflowState)\\n    workflow.add_node(\"create_post\", create_post_text)\\n    workflow.add_node(\"generate_image\", generate_image)\\n    workflow.add_node(\"classify_feedback\", classify_feedback)\\n    workflow.add_node(\"refine_text\", refine_text)\\n    workflow.add_node(\"refine_image\", refine_image)\\n    workflow.add_node(\"send_email\", send_email)\\n    workflow.set_entry_point(\"create_post\")\\n    workflow.add_edge(\"create_post\", \"generate_image\")\\n    workflow.add_edge(\"generate_image\", \"classify_feedback\")\\n    workflow.add_conditional_edges(\\n        \"classify_feedback\",\\n        route_feedback,\\n        {\\n            \"approved\": \"send_email\",\\n            \"refine_text\": \"refine_text\",\\n            \"refine_image\": \"refine_image\",\\n            \"terminate\": END,\\n        },\\n    )\\n    workflow.add_edge(\"refine_text\", \"classify_feedback\")\\n    workflow.add_edge(\"refine_image\", \"classify_feedback\")\\n    workflow.add_edge(\"send_email\", END)\\n    memory = MemorySaver()\\n    app = workflow.compile(\\n        checkpointer=memory,\\n        interrupt_after=[\"generate_image\", \"refine_text\", \"refine_image\"],\\n    )\\n    return app\\n\\n\\n# Convenience API mirroring notebook helpers\\napp = create_workflow()\\n\\n\\ndef start_workflow(user_input: str, thread_id: str = \"default\") -> WorkflowState:\\n    initial_state = WorkflowState(user_input=user_input)\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\n    result = app.invoke(initial_state, config)\\n    return WorkflowState(**result)\\n\\n\\ndef continue_workflow(user_feedback: str, thread_id: str = \"default\") -> WorkflowState:\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\n    app.update_state(config, {\"user_feedback\": user_feedback})\\n    result = app.invoke(None, config)\\n    return WorkflowState(**result)\\n\\n\\n'},\n",
       "  {'path': 'workflow/nodes.py',\n",
       "   'content': 'import os\\nfrom typing import List\\n\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain.schema import SystemMessage, HumanMessage\\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\\nfrom langgraph.errors import GraphRecursionError\\nfrom langgraph.prebuilt import create_react_agent\\n# Gmail imports will be done lazily in send_email function\\n\\nfrom workflow.state import WorkflowState\\nfrom workflow.tools import tavily_tool\\n\\n\\n# Initialize models\\ngemini_model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\\n\\ndef create_post_text(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Generate LinkedIn post text based on user input\"\"\"\\n    system_message = \"\"\"\\n    You are a professional LinkedIn content creator. Create engaging, professional LinkedIn posts.\\n\\n    Your tone should be:\\n    - Professional yet approachable\\n    - Concise and clear\\n    - Action-oriented with clear value\\n\\n    Format your post with:\\n    - A strong hook in the first sentence\\n    - Short, scannable paragraphs\\n    - 1-3 relevant emojis strategically placed\\n    - A clear call-to-action\\n    - 3-5 relevant hashtags at the end\\n\\n    Output ONLY the post text, nothing else.\"\"\"\\n    \\n    try:\\n        max_iterations = 3\\n        recursion_limit = 2 * max_iterations + 1\\n        \\n        agent = create_react_agent(\\n            model=\"openai:gpt-4o-mini\",\\n            tools=[tavily_tool],\\n            prompt=system_message,\\n            debug=True\\n        )\\n        \\n        try:\\n            response = agent.invoke(\\n                {\"messages\": [{\"role\": \"user\", \"content\": state.user_input}]},\\n                {\"recursion_limit\": recursion_limit},\\n            )\\n            \\n            state.post_text = response[\\'messages\\'][-1].content\\n            state.current_step = \"create_image\"\\n            \\n        except GraphRecursionError:\\n            print(\"Agent stopped due to max iterations.\")\\n            state.error_message = \"Agent stopped due to max iterations\"\\n            state.current_step = \"error\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error creating post text: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef generate_image(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Generate image directly from post text\"\"\"\\n    \\n    try:\\n        post_text = state.post_text\\n        \\n        # Create a proper prompt for image generation\\n        image_prompt_template = \"\"\"\\n        Create a modern, minimalistic professional LinkedIn illustration for the following post: {post_content}\\n        \\n        Style: 3D Object Generator with clean flat vector, LinkedIn color palette (blue, white, grey), simple, corporate-appropriate.\\n        Avoid clutter, make it inspiring and easy to understand.\\n        No text in the image, just visual elements.\\n        \\n        Generate only the image description prompt, no other text.\\n        \"\"\"\\n        \\n        # Create messages for the LLM\\n        messages = [\\n            SystemMessage(content=\"You are an expert LinkedIn Image Prompt Engineer. Generate concise AI image prompts.\"),\\n            HumanMessage(content=image_prompt_template.format(post_content=post_text))\\n        ]\\n        \\n        # Get the image prompt from Gemini\\n        response = gemini_model.invoke(messages)\\n        image_prompt = response.content.strip()\\n        \\n        print(f\"Image prompt: {image_prompt}\")\\n        \\n        # Generate image using DALL-E\\n        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\\n        image_url = dalle_wrapper.run(image_prompt)\\n        \\n        # Update state\\n        state.image_url = image_url\\n        state.image_prompt = image_prompt\\n        state.current_step = \"wait_feedback\" \\n        \\n    except Exception as e:\\n        state.error_message = f\"Error generating image: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef classify_feedback(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Classify user feedback into categories\"\"\"\\n    system_message = \"\"\"You are a text classifier. Classify the input text into one of these categories:\\n\\n        1. Approved - Approval message refers that its ok to proceed with the current status.\\n        2. Refine Text - Request to refine the post text\\n        3. Terminate - Termination request for the process.\\n        4. Refine Image - Request to refine the post image\\n\\n        Output only the category name, nothing else.\"\"\"\\n\\n    try:\\n        messages = [\\n            SystemMessage(content=system_message),\\n            HumanMessage(content=state.user_feedback)\\n        ]\\n        \\n        response = gemini_model.invoke(messages)\\n        classification = response.content.strip()\\n        \\n        # Map classification to next step\\n        if \"Approved\" in classification:\\n            state.classification = \"Approved\"\\n            state.current_step = \"post_content\"\\n            state.final_post_ready = True\\n        elif \"Refine Text\" in classification:\\n            state.classification = \"Refine Text\"\\n            state.current_step = \"refine_text\"\\n        elif \"Terminate\" in classification:\\n            state.classification = \"Terminate\"\\n            state.current_step = \"terminate\"\\n        elif \"Refine Image\" in classification:\\n            state.classification = \"Refine Image\"\\n            state.current_step = \"refine_image\"\\n        else:\\n            state.classification = \"Unknown\"\\n            state.current_step = \"wait_feedback\"\\n            state.error_message = f\"Unrecognized feedback classification: {classification}\"\\n        \\n        print(f\"DEBUG: Updated state - classification: {state.classification}, current_step: {state.current_step}\")\\n            \\n    except Exception as e:\\n        state.error_message = f\"Error classifying feedback: {str(e)}\"\\n        state.current_step = \"error\"\\n        print(f\"DEBUG: Exception in classify_feedback: {str(e)}\")\\n    \\n    return state\\n\\n\\ndef refine_text(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Refine the post text based on user feedback\"\"\"\\n    system_message = \"\"\"\\n        You are a helpful assistant who improve and refine LinkedIn post.\\n\\n        Output only the post text.\"\"\"\\n\\n    try:\\n        prompt = f\"Original post: {state.post_text}\\\\\\\\n\\\\\\\\nRequest: {state.user_feedback}\"\\n        \\n        messages = [\\n            SystemMessage(content=system_message),\\n            HumanMessage(content=prompt)\\n        ]\\n        \\n        response = gemini_model.invoke(messages)\\n        state.refined_text = response.content\\n        state.post_text = state.refined_text  # Update the main post text\\n        state.current_step = \"wait_feedback\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error refining text: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef refine_image(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Refine the image based on user feedback\"\"\"\\n    try:\\n        # Use the existing image prompt and add user feedback\\n        modified_prompt = f\"{state.image_prompt}. {state.user_feedback}\"\\n        \\n        print(f\"Refined image prompt: {modified_prompt}\")\\n        \\n        # Generate image using DALL-E with the modified prompt\\n        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\\n        image_url = dalle_wrapper.run(modified_prompt)\\n        \\n        # Update state with the refined image\\n        state.image_url = image_url\\n        state.image_prompt = modified_prompt  # Update the prompt with feedback\\n        state.current_step = \"wait_feedback\"  # Go back to wait for more feedback\\n        \\n        print(f\"Refined image URL: {image_url}\")\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error refining image: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef send_email(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Send email with the final post content\"\"\"\\n    try:\\n        # Lazy import Gmail functionality\\n        from langchain_google_community import GmailToolkit\\n        from langchain_google_community.gmail.utils import (\\n            build_resource_service,\\n            get_gmail_credentials,\\n        )\\n\\n        # Test imports without initializing toolkit\\n        print(\"Imports successful!\")\\n\\n            \\n        # Initialize Gmail toolkit\\n        credentials = get_gmail_credentials(\\n            token_file=\"token.json\",\\n            scopes=[\"https://mail.google.com/\"],\\n            client_secrets_file=\"credentials.json\",\\n        )\\n        \\n        api_resource = build_resource_service(credentials=credentials)\\n        gmail_toolkit = GmailToolkit(api_resource=api_resource)\\n        \\n        tools = gmail_toolkit.get_tools()\\n        print(f\\'🚩DEBUG: send mail tool: {tools[1]}\\')\\n        # Compose email content\\n        subject = \"LinkedIn Post Ready for Publishing\"\\n        body = f\"\"\"\\n            Your LinkedIn post is ready:\\n\\n            {state.post_text}\\n\\n            Image URL: {state.image_url}\\n\\n            Best regards,\\n            LinkedIn Post Creator\\n            \"\"\"\\n        \\n        # Send email using Gmail toolkit\\n        send_message_tool = gmail_toolkit.get_tools()[1]  # send_gmail_message tool\\n        \\n        email_result = send_message_tool.run({\\n            \"to\": \"mohammed-mowina@outlook.com\",\\n            \"subject\": subject,\\n            \"message\": body\\n        })\\n        \\n        state.current_step = \"completed\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"🚩Error sending email: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state'},\n",
       "  {'path': 'workflow/state.py',\n",
       "   'content': 'from dataclasses import dataclass\\nfrom typing import Optional\\n\\n\\n@dataclass\\nclass WorkflowState:\\n    \"\"\"State management for the LinkedIn post creation workflow.\"\"\"\\n    user_input: str = \"\"\\n    post_text: str = \"\"\\n    image_prompt: str = \"\"\\n    image_data: Optional[bytes] = None\\n    image_url: str = \"\"\\n    user_feedback: str = \"\"\\n    classification: str = \"\"\\n    refined_text: str = \"\"\\n    refined_image_data: Optional[bytes] = None\\n    current_step: str = \"start\"\\n    error_message: str = \"\"\\n    final_post_ready: bool = False\\n\\n\\n'},\n",
       "  {'path': 'workflow/tools.py',\n",
       "   'content': 'import os\\nimport requests\\nfrom typing import Any, Dict\\n\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\nfrom langchain.tools import Tool\\nfrom langchain_core.tools import tool\\n\\ndef tavily_search(query: str) -> str:\\n    \"\"\"Search the web using Tavily API\"\"\"\\n    try:\\n        url = \"https://api.tavily.com/search\"\\n        headers = {\"Authorization\": f\"Bearer {Config.TAVILY_API_KEY}\"}\\n        data = {\"query\": query}\\n        \\n        response = requests.post(url, headers=headers, json=data)\\n        response.raise_for_status()\\n        \\n        results = response.json()\\n        # Extract relevant information from search results\\n        search_results = []\\n        for result in results.get(\"results\", [])[:3]:  # Top 3 results\\n            search_results.append(f\"Title: {result.get(\\'title\\', \\'\\')}\\\\nContent: {result.get(\\'content\\', \\'\\')[:200]}...\")\\n        \\n        return \"\\\\n\\\\n\".join(search_results)\\n    except Exception as e:\\n        return f\"Search failed: {str(e)}\"\\n\\n\\n# Create Tavily search tool\\ntavily_tool = Tool(\\n    name=\"web_search\",\\n    description=\"Search the web for current information\",\\n    func=tavily_search\\n)\\n'}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Git utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import stat\n",
    "import git\n",
    "from git import Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_repo_info(repo_path):\n",
    "    \"\"\"\n",
    "    Extract information from a Git repository.\n",
    "    Returns a dictionary with repo metadata and file contents.\n",
    "    Only processes relevant text files for code analysis.\n",
    "    \"\"\"\n",
    "    repo = None\n",
    "    try:\n",
    "        repo = git.Repo(repo_path)\n",
    "        repo_info = {\n",
    "            'repo_name': os.path.basename(repo_path),\n",
    "            'commit_count': len(list(repo.iter_commits())),\n",
    "            'branches': [branch.name for branch in repo.branches],\n",
    "            'files': []\n",
    "        }\n",
    "        \n",
    "        # Define file extensions to INCLUDE (whitelist approach)\n",
    "        relevant_extensions = {\n",
    "            '.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.cpp', '.c', '.h',\n",
    "            '.cs', '.php', '.rb', '.go', '.rs', '.swift', '.kt', '.scala',\n",
    "            '.html', '.css', '.scss', '.sass', '.less',\n",
    "            '.md', '.txt', '.rst', '.json', '.yaml', '.yml', '.toml',\n",
    "            '.sql', '.sh', '.bat', '.ps1',\n",
    "            '.dockerfile', '.gitignore', '.env.example'\n",
    "        }\n",
    "        \n",
    "        # Define files to INCLUDE by name (regardless of extension)\n",
    "        relevant_filenames = {\n",
    "            'README', 'LICENSE', 'CHANGELOG', 'CONTRIBUTING', 'INSTALL',\n",
    "            'Dockerfile', 'Makefile', 'requirements.txt', 'package.json',\n",
    "            'setup.py', 'pyproject.toml', 'Cargo.toml', 'pom.xml',\n",
    "            'build.gradle', 'composer.json', 'Gemfile'\n",
    "        }\n",
    "        \n",
    "        # Define directories to SKIP\n",
    "        skip_directories = {\n",
    "            'node_modules', '.git', '__pycache__', '.pytest_cache',\n",
    "            'venv', 'env', '.env', 'build', 'dist', 'target',\n",
    "            '.idea', '.vscode', 'logs', 'tmp', 'temp',\n",
    "            'images', 'assets', 'static/images', 'public/images'\n",
    "        }\n",
    "        \n",
    "        # Get file contents from the latest commit in the active branch\n",
    "        tree = repo.head.commit.tree\n",
    "        for item in tree.traverse():\n",
    "            if item.type == 'blob':  # Only process files, not directories\n",
    "                # Skip files in unwanted directories\n",
    "                if any(skip_dir in item.path for skip_dir in skip_directories):\n",
    "                    continue\n",
    "                \n",
    "                # Check if file should be included\n",
    "                file_ext = os.path.splitext(item.path)[1].lower()\n",
    "                filename = os.path.basename(item.path)\n",
    "                filename_no_ext = os.path.splitext(filename)[0].upper()\n",
    "                \n",
    "                # Include if extension or filename matches our criteria\n",
    "                should_include = (\n",
    "                    file_ext in relevant_extensions or\n",
    "                    filename_no_ext in relevant_filenames or\n",
    "                    filename in relevant_filenames\n",
    "                )\n",
    "                \n",
    "                if not should_include:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Try to decode as UTF-8, skip if it fails\n",
    "                    content = item.data_stream.read().decode('utf-8')\n",
    "                    # Skip files with null bytes (binary content)\n",
    "                    if '\\x00' in content:\n",
    "                        continue\n",
    "                    \n",
    "                    # Skip very large files (>100KB)\n",
    "                    if len(content) > 100000:\n",
    "                        continue\n",
    "                    \n",
    "                    file_info = {\n",
    "                        'path': item.path,\n",
    "                        'content': content\n",
    "                    }\n",
    "                    repo_info['files'].append(file_info)\n",
    "                except UnicodeDecodeError:\n",
    "                    # Skip files that can't be decoded as text\n",
    "                    continue\n",
    "        \n",
    "        print(f\"Processed {len(repo_info['files'])} relevant files from {repo_info['repo_name']}\")\n",
    "        return repo_info\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing Git repo: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Close the repository to release file handles\n",
    "        if repo is not None:\n",
    "            repo.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_name_from_url(url):\n",
    "    \"\"\"Extract repository name from GitHub URL.\"\"\"\n",
    "    path = urlparse(url).path  # \"/M-Mowina/LinkedIn-Booster\"\n",
    "    return path.strip(\"/\").split(\"/\")[-1]  # \"LinkedIn-Booster\"\n",
    "\n",
    "def remove_readonly(func, path, _):\n",
    "    \"\"\"Clear the readonly bit and reattempt the removal\"\"\"\n",
    "    os.chmod(path, stat.S_IWRITE)\n",
    "    func(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_repo_info(github_url, branch=None):\n",
    "    \"\"\"\n",
    "    Clone a GitHub repo temporarily and extract its information.\n",
    "    Returns repository information or None if error occurs.\n",
    "    \"\"\"\n",
    "    repo_name = get_repo_name_from_url(github_url)\n",
    "    base_tmp = tempfile.mkdtemp()   # e.g. /tmp/tmpabcd1234\n",
    "    repo_dir = os.path.join(base_tmp, repo_name)\n",
    "    repo = None\n",
    "    \n",
    "    try:\n",
    "        repo = Repo.clone_from(github_url, repo_dir, branch=branch)\n",
    "        result = get_git_repo_info(repo_dir)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error cloning GitHub repo: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Close the repository to release file handles\n",
    "        if repo is not None:\n",
    "            repo.close()\n",
    "        \n",
    "        # Use onerror callback to handle readonly files on Windows\n",
    "        try:\n",
    "            shutil.rmtree(base_tmp, onerror=remove_readonly)\n",
    "        except Exception as cleanup_error:\n",
    "            print(f\"Warning: Could not clean up temp directory: {cleanup_error}\")\n",
    "            # Try alternative cleanup method\n",
    "            try:\n",
    "                import subprocess\n",
    "                subprocess.run(['rmdir', '/s', '/q', base_tmp], shell=True, check=False)\n",
    "            except:\n",
    "                pass  # If all cleanup methods fail, just continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 14 relevant files from LinkedIn-Booster\n",
      "LinkedIn-Booster\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "github_url = \"https://github.com/M-Mowina/LinkedIn-Booster\"\n",
    "repo_info = get_github_repo_info(github_url)\n",
    "print(repo_info['repo_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repo_name': 'LinkedIn-Booster',\n",
       " 'commit_count': 6,\n",
       " 'branches': ['main'],\n",
       " 'files': [{'path': 'README.md',\n",
       "   'content': '# LinkedIn-Booster 🚀\\n\\nAn intelligent AI-powered LinkedIn post creation workflow that combines the power of LangChain, LangGraph, and multiple AI services to create engaging LinkedIn content with matching visuals.\\n\\n![LangGraph Workflow](images/LangGraph-workflow-diagram.png)\\n\\n## ✨ Features\\n\\n- **🤖 AI-Powered Generation**: Google Gemini for high-quality LinkedIn post drafting\\n- **🔍 Web Research**: Tavily search integration for up-to-date context and insights\\n- **🔄 Interactive Feedback Loop**: Intelligent feedback classification and content refinement\\n- **🎨 Image Generation**: DALL·E 3 integration for compelling visual content\\n- **📧 Email Delivery**: Gmail API integration for seamless content distribution\\n- **🖥️ Multiple Interfaces**: CLI and web-based Chainlit UI options\\n- **📊 Workflow Visualization**: LangGraph Studio integration for workflow monitoring\\n\\n## 🏗️ Architecture\\n\\nThe project follows a modular architecture with clear separation of concerns:\\n\\n```\\nLinkedIn-Booster/\\n├── 📄 main.py                    # CLI interface\\n├── 🌐 app.py                     # Chainlit web interface  \\n├── 📁 workflow/\\n│   ├── __init__.py\\n│   ├── state.py                  # Workflow state management\\n│   ├── tools.py                  # External API integrations (Tavily)\\n│   ├── nodes.py                  # Core workflow nodes\\n│   └── graph.py                  # LangGraph workflow orchestration\\n├── 📁 utils/\\n│   ├── __init__.py\\n│   └── config.py                 # Configuration and environment setup\\n├── 📁 studio/                    # LangGraph Studio configuration\\n├── 📁 notebooks/                 # Development notebooks\\n├── 📁 images/                    # Documentation assets\\n├── 📄 requirements.txt\\n└── 📄 .env                       # Environment variables\\n```\\n\\n## 🚀 Quick Start\\n\\n### Prerequisites\\n- Python 3.8+\\n- API keys for OpenAI, Google Gemini, and Tavily\\n- Gmail API credentials (optional, for email functionality)\\n\\n### Installation\\n\\n1. **Clone and install dependencies**\\n```bash\\ngit clone <repository-url>\\ncd LinkedIn-Booster\\npip install -r requirements.txt\\n```\\n\\n2. **Configure environment variables**\\nCreate a `.env` file in the root directory:\\n```bash\\n# AI Service APIs\\nOPENAI_API_KEY=your_openai_api_key\\nGOOGLE_API_KEY=your_google_gemini_api_key\\nTAVILY_API_KEY=your_tavily_api_key\\n\\n# Gmail Integration (Optional)\\nGMAIL_CREDENTIALS_FILE=credentials.json\\nGMAIL_TOKEN_FILE=token.json\\nRECIPIENT_EMAIL=recipient@example.com\\n```\\n\\n3. **Gmail Setup (Optional)**\\nFor email functionality:\\n- Enable Gmail API in [Google Cloud Console](https://console.cloud.google.com/)\\n- Create OAuth2 credentials and download as `credentials.json`\\n- Place the file in the project root\\n- First run will generate `token.json` automatically\\n\\n## 💻 Usage\\n\\n### Option 1: Web Interface (Recommended)\\nLaunch the interactive Chainlit web interface:\\n\\n```bash\\nchainlit run app.py\\n```\\n\\n![Chainlit Interface](images/ChainLit-user%20Interface.png)\\n\\nThe web interface provides:\\n- 🎯 Guided workflow with clear instructions\\n- 📱 Real-time feedback and refinement\\n- 🖼️ Visual preview of generated content\\n- ✅ Easy approval and sending process\\n\\n### Option 2: Command Line Interface\\nFor quick automation or scripting:\\n\\n```bash\\npython main.py \"Create a post about the importance of drinking water\"\\n```\\n\\n### Option 3: LangGraph Studio\\nFor workflow development and debugging:\\n\\n![LangGraph Studio](images/LangGraphStudio-workflow-diagram.png)\\n\\n```bash\\ncd studio\\nlanggraph dev\\n```\\n\\n### Option 4: n8n Integration\\nFor no-code workflow automation:\\n\\n![n8n Workflow](images/n8n-workflow-diagram.png)\\n\\nImport the workflow from `n8n/main (NO img2img).json` into your n8n instance.\\n\\n## 🔄 Workflow Process\\n\\nThe LinkedIn Booster follows an intelligent multi-step process:\\n\\n1. **📝 Content Creation**: AI generates LinkedIn post based on your topic\\n2. **🎨 Image Generation**: Creates matching visual content using DALL·E 3\\n3. **👀 Review & Feedback**: Present content for user review\\n4. **🤖 Smart Classification**: AI analyzes feedback and determines next action\\n5. **✨ Refinement**: Improves text or image based on feedback\\n6. **✅ Approval & Delivery**: Sends final content via email when approved\\n\\nThe workflow uses LangGraph\\'s stateful execution with strategic interrupts, allowing for human-in-the-loop refinement at key decision points.\\n\\n## 🛠️ Development\\n\\n### Running Tests\\n```bash\\npython test_imports.py\\npython test_create_post.py\\npython send_mail_test.py\\n```\\n\\n### Jupyter Notebook\\nExplore the original development process:\\n```bash\\njupyter notebook notebooks/booster_workflow_v0.2.ipynb\\n```\\n\\n## 📋 Requirements\\n\\nKey dependencies include:\\n- `langchain` - LLM framework and integrations\\n- `langgraph` - Workflow orchestration\\n- `chainlit` - Web interface\\n- `google-api-python-client` - Gmail integration\\n- `openai` - DALL·E 3 image generation\\n- `tavily-python` - Web search capabilities\\n\\nSee `requirements.txt` for complete list.\\n\\n## 🤝 Contributing\\n\\n1. Fork the repository\\n2. Create a feature branch\\n3. Make your changes\\n4. Add tests if applicable\\n5. Submit a pull request\\n\\n## 📄 License\\n\\nMIT License - see LICENSE file for details.\\n'},\n",
       "  {'path': 'app.py',\n",
       "   'content': '\"\"\"Chainlit GUI for LinkedIn Booster workflow.\"\"\"\\nimport chainlit as cl\\nfrom typing import Dict, Any\\nimport asyncio\\nfrom utils import load_env\\nfrom workflow.graph import start_workflow, continue_workflow\\n\\n\\n# Load environment variables\\nload_env()\\n\\n# Store user sessions\\nuser_sessions: Dict[str, Dict[str, Any]] = {}\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    \"\"\"Initialize the chat session.\"\"\"\\n    session_id = cl.user_session.get(\"id\")\\n    user_sessions[session_id] = {\\n        \"thread_id\": f\"chainlit_{session_id}\",\\n        \"current_state\": None,\\n        \"step\": \"initial\"\\n    }\\n    \\n    await cl.Message(\\n        content=\"🚀 **Welcome to LinkedIn Booster!**\\\\n\\\\n\"\\n                \"I\\'ll help you create engaging LinkedIn posts with AI-generated images.\\\\n\\\\n\"\\n                \"**How it works:**\\\\n\"\\n                \"1. Tell me your post topic or idea\\\\n\"\\n                \"2. I\\'ll generate a professional post and matching image\\\\n\"\\n                \"3. You can review and refine both text and image\\\\n\"\\n                \"4. Once approved, I\\'ll send it via email\\\\n\\\\n\"\\n                \"**What would you like to post about today?**\"\\n    ).send()\\n\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n    \"\"\"Handle user messages.\"\"\"\\n    session_id = cl.user_session.get(\"id\")\\n    session = user_sessions.get(session_id, {})\\n    \\n    user_input = message.content.strip()\\n    \\n    if session.get(\"step\") == \"initial\":\\n        # Start the workflow\\n        await handle_initial_request(user_input, session)\\n    else:\\n        # Continue with feedback\\n        await handle_feedback(user_input, session)\\n\\n\\nasync def handle_initial_request(topic: str, session: Dict[str, Any]):\\n    \"\"\"Handle the initial post creation request.\"\"\"\\n    session_id = cl.user_session.get(\"id\")\\n    thread_id = session[\"thread_id\"]\\n    \\n    # Show loading message\\n    loading_msg = await cl.Message(content=\"🔄 Creating your LinkedIn post...\").send()\\n    \\n    try:\\n        # Start the workflow\\n        state = await asyncio.to_thread(start_workflow, topic, thread_id)\\n        session[\"current_state\"] = state\\n        \\n        if state.error_message:\\n            await cl.Message(content=f\"❌ **Error:** {state.error_message}\").send()\\n            return\\n        \\n        # Show success message\\n        await cl.Message(content=\"✅ **Post Created Successfully!**\").send()\\n        \\n        # Display the generated post\\n        await cl.Message(\\n            content=f\"📝 **Generated LinkedIn Post:**\\\\n\\\\n{state.post_text}\"\\n        ).send()\\n        \\n        # Display the image if available\\n        if state.image_url:\\n            await cl.Message(\\n                content=f\"🖼️ **Generated Image:**\\\\n\\\\n![Generated Image]({state.image_url})\"\\n            ).send()\\n        \\n        # Ask for feedback\\n        session[\"step\"] = \"feedback\"\\n        await cl.Message(\\n            content=\"**What would you like to do next?**\\\\n\\\\n\"\\n                   \"• Type **\\'approve\\'** to send the post via email\\\\n\"\\n                   \"• Type **\\'refine text [your suggestions]\\'** to improve the text\\\\n\"\\n                   \"• Type **\\'refine image [your suggestions]\\'** to improve the image\\\\n\"\\n                   \"• Type **\\'quit\\'** to end the session\"\\n        ).send()\\n        \\n    except Exception as e:\\n        await cl.Message(content=f\"❌ **Error:** {str(e)}\").send()\\n\\n\\nasync def handle_feedback(feedback: str, session: Dict[str, Any]):\\n    \"\"\"Handle user feedback on the generated content.\"\"\"\\n    session_id = cl.user_session.get(\"id\")\\n    thread_id = session[\"thread_id\"]\\n    \\n    if feedback.lower() in {\"quit\", \"exit\", \"q\"}:\\n        await cl.Message(content=\"👋 **Thanks for using LinkedIn Booster!** Have a great day!\").send()\\n        session[\"step\"] = \"completed\"\\n        return\\n    \\n    # Show processing message\\n    await cl.Message(content=\"🔄 Processing your feedback...\").send()\\n    \\n    try:\\n        # Continue the workflow with feedback\\n        state = await asyncio.to_thread(continue_workflow, feedback, thread_id)\\n        session[\"current_state\"] = state\\n        \\n        if state.error_message:\\n            await cl.Message(content=f\"❌ **Error:** {state.error_message}\").send()\\n            return\\n        \\n        # Handle different workflow states\\n        if state.current_step == \"completed\":\\n            await cl.Message(content=\"✅ **Email sent successfully!**\").send()\\n            await cl.Message(\\n                content=\"🎉 **Workflow Completed!**\\\\n\\\\n\"\\n                       \"Your LinkedIn post has been sent via email. \"\\n                       \"You can start a new post by refreshing the page.\"\\n            ).send()\\n            session[\"step\"] = \"completed\"\\n            \\n        elif state.current_step == \"post_content\" and state.classification == \"Approved\":\\n            # This means the post was approved, trigger email sending\\n            await cl.Message(content=\"✅ **Post approved! Sending email...**\").send()\\n            # The workflow should automatically proceed to send_email\\n            \\n        elif state.current_step == \"terminate\":\\n            await cl.Message(content=\"🛑 **Workflow terminated.**\").send()\\n            await cl.Message(\\n                content=\"The workflow has been terminated. \"\\n                       \"You can start a new post by refreshing the page.\"\\n            ).send()\\n            session[\"step\"] = \"completed\"\\n            \\n        elif state.current_step == \"wait_feedback\":\\n            await cl.Message(content=\"✅ **Content updated!**\").send()\\n            \\n            # Always show the current content after updates\\n            if state.post_text:\\n                await cl.Message(\\n                    content=f\"📝 **Updated LinkedIn Post:**\\\\n\\\\n{state.post_text}\"\\n                ).send()\\n            \\n            if state.image_url:\\n                await cl.Message(\\n                    content=f\"🖼️ **Updated Image:**\\\\n\\\\n![Updated Image]({state.image_url})\"\\n                ).send()\\n            \\n            # Ask for next action\\n            await cl.Message(\\n                content=\"**What would you like to do next?**\\\\n\\\\n\"\\n                       \"• Type **\\'approve\\'** to send the post via email\\\\n\"\\n                       \"• Type **\\'refine text [your suggestions]\\'** to improve the text further\\\\n\"\\n                       \"• Type **\\'refine image [your suggestions]\\'** to improve the image further\\\\n\"\\n                       \"• Type **\\'quit\\'** to end the session\"\\n            ).send()\\n        \\n        else:\\n            # Handle any other states or show current content\\n            await cl.Message(content=f\"**Current Status:** {state.current_step}\").send()\\n            if state.classification:\\n                await cl.Message(content=f\"**Classification:** {state.classification}\").send()\\n            \\n            # Show current content\\n            if state.post_text:\\n                await cl.Message(\\n                    content=f\"📝 **Current LinkedIn Post:**\\\\n\\\\n{state.post_text}\"\\n                ).send()\\n            \\n            if state.image_url:\\n                await cl.Message(\\n                    content=f\"🖼️ **Current Image:**\\\\n\\\\n![Current Image]({state.image_url})\"\\n                ).send()\\n            \\n            # Ask for next action\\n            await cl.Message(\\n                content=\"**What would you like to do next?**\\\\n\\\\n\"\\n                       \"• Type **\\'approve\\'** to send the post via email\\\\n\"\\n                       \"• Type **\\'refine text [your suggestions]\\'** to improve the text\\\\n\"\\n                       \"• Type **\\'refine image [your suggestions]\\'** to improve the image\\\\n\"\\n                       \"• Type **\\'quit\\'** to end the session\"\\n            ).send()\\n        \\n    except Exception as e:\\n        await cl.Message(content=f\"❌ **Error:** {str(e)}\").send()\\n\\n\\n@cl.on_chat_end\\nasync def end():\\n    \"\"\"Clean up when chat ends.\"\"\"\\n    session_id = cl.user_session.get(\"id\")\\n    if session_id in user_sessions:\\n        del user_sessions[session_id]'},\n",
       "  {'path': 'main.py',\n",
       "   'content': 'import sys\\nfrom utils import load_env\\nfrom workflow.graph import start_workflow, continue_workflow\\n\\n\\ndef main() -> None:\\n    load_env()\\n    if len(sys.argv) < 2:\\n        print(\"Usage: python main.py \\\\\"Your post topic...\\\\\"\")\\n        sys.exit(1)\\n    topic = sys.argv[1]\\n    thread_id = \"cli\"\\n    state = start_workflow(topic, thread_id)\\n    if state.current_step != \"wait_feedback\":\\n        print(f\"Error: {state.error_message}\")\\n        sys.exit(1)\\n    print(\"\\\\nGenerated Post:\\\\n\")\\n    print(state.post_text)\\n    print(\"\\\\nImage URL:\\\\n\")\\n    print(state.image_url)\\n    while True:\\n        feedback = input(\"\\\\nFeedback (approve/refine text .../refine image .../quit): \").strip()\\n        if feedback.lower() in {\"quit\", \"exit\", \"q\"}:\\n            print(\"Goodbye!\")\\n            break\\n        state = continue_workflow(feedback, thread_id)\\n        if state.current_step == \"completed\":\\n            print(\"Email sent. Workflow completed.\")\\n            break\\n        if state.current_step == \"terminate\":\\n            print(\"Workflow terminated.\")\\n            break\\n        if state.current_step == \"wait_feedback\":\\n            if state.classification == \"Refine Text\":\\n                print(\"\\\\nUpdated Post:\\\\n\")\\n                print(state.post_text)\\n            elif state.classification == \"Refine Image\":\\n                print(\"\\\\nUpdated Image URL:\\\\n\")\\n                print(state.image_url)\\n\\n\\nif __name__ == \"__main__\":\\n    main()'},\n",
       "  {'path': 'requirements.txt',\n",
       "   'content': 'chainlit>=1.0.0\\nlangchain>=0.1.0\\nlangchain-community>=0.1.0\\nlangchain-google-genai>=1.0.0\\nlangchain-google-community>=0.1.0\\nlanggraph>=0.0.40\\nrequests>=2.31.0\\ntyping-extensions>=4.5.0\\npython-dotenv>=1.0.0\\ngoogle-auth>=2.0.0\\ngoogle-auth-oauthlib>=1.0.0\\ngoogle-auth-httplib2>=0.2.0\\ngoogle-api-python-client>=2.0.0\\nopenai>=1.0.0\\ntavily-python>=0.3.0'},\n",
       "  {'path': 'n8n/main (NO img2img).json',\n",
       "   'content': '{\\n  \"name\": \"main (NO img2img)\",\\n  \"nodes\": [\\n    {\\n      \"parameters\": {\\n        \"options\": {}\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatGoogleGemini\",\\n      \"typeVersion\": 1,\\n      \"position\": [\\n        832,\\n        352\\n      ],\\n      \"id\": \"575c62c0-07d4-4c73-9d70-4fdafe125e86\",\\n      \"name\": \"Google Gemini Chat Model\",\\n      \"credentials\": {\\n        \"googlePalmApi\": {\\n          \"id\": \"mP5d64x2rkeTRzbi\",\\n          \"name\": \"Google Gemini(PaLM) Api account 2\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"toolDescription\": \"Web search tool.\",\\n        \"method\": \"POST\",\\n        \"url\": \"https://api.tavily.com/search\",\\n        \"sendHeaders\": true,\\n        \"headerParameters\": {\\n          \"parameters\": [\\n            {\\n              \"name\": \"Authorization\",\\n              \"value\": \"Bearer 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\"\\n            }\\n          ]\\n        },\\n        \"sendBody\": true,\\n        \"bodyParameters\": {\\n          \"parameters\": [\\n            {\\n              \"name\": \"query\",\\n              \"value\": \"={{ /*n8n-auto-generated-fromAI-override*/ $fromAI(\\'parameters0_Value\\', ``, \\'string\\') }}\"\\n            }\\n          ]\\n        },\\n        \"options\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.httpRequestTool\",\\n      \"typeVersion\": 4.2,\\n      \"position\": [\\n        -496,\\n        -832\\n      ],\\n      \"id\": \"f6bfb373-1e88-448b-8dfd-da7c90016a8d\",\\n      \"name\": \"Tavily\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"inputText\": \"={{ $json.data.text }}\",\\n        \"categories\": {\\n          \"categories\": [\\n            {\\n              \"category\": \"Approved\",\\n              \"description\": \"Approval message refers that its ok to proceed with the current status.\"\\n            },\\n            {\\n              \"category\": \"Refine Text\",\\n              \"description\": \"Request to Refine the post text\"\\n            },\\n            {\\n              \"category\": \"Terminate\",\\n              \"description\": \"Termination request for the process.\"\\n            },\\n            {\\n              \"category\": \"Refine Image\",\\n              \"description\": \"Request to Refine the post Image\"\\n            }\\n          ]\\n        },\\n        \"options\": {}\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.textClassifier\",\\n      \"typeVersion\": 1.1,\\n      \"position\": [\\n        624,\\n        -464\\n      ],\\n      \"id\": \"abf5e791-c5f8-4b32-8a4b-58804e853434\",\\n      \"name\": \"Text Classifier\"\\n    },\\n    {\\n      \"parameters\": {},\\n      \"type\": \"n8n-nodes-base.noOp\",\\n      \"typeVersion\": 1,\\n      \"position\": [\\n        1296,\\n        64\\n      ],\\n      \"id\": \"04ec9776-b607-4c47-8140-bfeb62086c9c\",\\n      \"name\": \"No Operation, do nothing\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"authentication\": \"communityManagement\",\\n        \"person\": \"=Mohamed Mowina\",\\n        \"text\": \"={{ $(\\'Post writer\\').item.json.output }}\",\\n        \"shareMediaCategory\": \"IMAGE\",\\n        \"additionalFields\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.linkedIn\",\\n      \"typeVersion\": 1,\\n      \"position\": [\\n        1616,\\n        -1008\\n      ],\\n      \"id\": \"c56079bb-4e3e-471b-819c-3a6f73f2496b\",\\n      \"name\": \"Create a post\",\\n      \"disabled\": true\\n    },\\n    {\\n      \"parameters\": {\\n        \"updates\": [\\n          \"message\"\\n        ],\\n        \"additionalFields\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.telegramTrigger\",\\n      \"typeVersion\": 1.2,\\n      \"position\": [\\n        -1168,\\n        -1008\\n      ],\\n      \"id\": \"c6d21d0a-52b5-405d-b16d-e7568dd8e839\",\\n      \"name\": \"Telegram Trigger\",\\n      \"webhookId\": \"931413d8-41c9-43f0-85bc-1b1b1fb80fa4\",\\n      \"credentials\": {\\n        \"telegramApi\": {\\n          \"id\": \"EFoDusKQAfWdyYaO\",\\n          \"name\": \"Telegram account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"promptType\": \"define\",\\n        \"text\": \"={{ $json.message.text }}\",\\n        \"options\": {\\n          \"systemMessage\": \"=Your tone should be:\\\\n\\\\nProfessional yet approachable: Confident and authoritative, but also conversational and relatable.\\\\n\\\\nConcise and clear: Get straight to the point while using compelling language.\\\\n\\\\nAction-oriented: Focus on providing value and prompting engagement.\\\\n\\\\nCore Functionality\\\\nYou will perform the following tasks in a structured workflow:\\\\n\\\\nInput Analysis & Goal Clarification:\\\\n\\\\nUpon receiving a request, first identify the core topic and the user\\'s primary goal (e.g., generate leads, build thought leadership, share a company update, get job offers).\\\\n\\\\nIdentify the target audience and their pain points or interests.\\\\n\\\\nRecognize any specific constraints (e.g., character limit, inclusion of a specific link, mention of a name).\\\\n\\\\nStrategy & Brainstorming:\\\\n\\\\nBased on your analysis, brainstorm a minimum of three potential angles or hooks for the post. Each hook should be designed to capture attention and align with the user\\'s goal.\\\\n\\\\nConsider different post formats: a personal story, a listicle, a thought-provoking question, a concise tip, a problem/solution narrative.\\\\n\\\\nDetermine the most effective approach for the given topic and goal.\\\\n\\\\nDraft Generation:\\\\n\\\\nWrite a draft of the post using the chosen angle. Follow these rules for the highest quality:\\\\n\\\\nHook: Start with a strong, attention-grabbing first sentence.\\\\n\\\\nBody: Break up text into short, single-sentence paragraphs. Use line breaks frequently for scannability.\\\\n\\\\nElegance: Incorporate relevant emojis to break up text and add personality, but use them sparingly and strategically (1-3 emojis is usually sufficient).\\\\n\\\\nValue: Provide clear, actionable value, a unique perspective, or a compelling story.\\\\n\\\\nCall-to-Action (CTA): End with a clear, concise CTA that encourages engagement (e.g., \\\\\"What are your thoughts?\\\\\", \\\\\"Share your experience below!\\\\\", \\\\\"Comment \\'Yes\\' if you agree.\\\\\").\\\\n\\\\nHashtags: Suggest 3-5 relevant, specific, and popular hashtags at the end of the post.\\\\n\\\\nOutput:\\\\n\\\\nDon\\'t forget you Output only the post, Do not ask flowup questions and don\\'t provide other suggestions.\\\\n\"\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\\n      \"typeVersion\": 2.2,\\n      \"position\": [\\n        -848,\\n        -1008\\n      ],\\n      \"id\": \"2cac353e-25c0-4125-8b49-c13448885f92\",\\n      \"name\": \"Post writer\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"promptType\": \"define\",\\n        \"text\": \"=Original post:  {{ $(\\'Send a photo message\\').item.json.result.caption }}\\\\n\\\\nRequest: {{ $json.data.text }}\",\\n        \"options\": {\\n          \"systemMessage\": \"You are a helpful assistant who improve and refine Linkedin post.\\\\n\\\\nOutput only the post text.\"\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\\n      \"typeVersion\": 2.2,\\n      \"position\": [\\n        1552,\\n        -368\\n      ],\\n      \"id\": \"49ee2b7d-5cac-4a5d-9696-8d835045a536\",\\n      \"name\": \"Post refiner\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"operation\": \"sendAndWait\",\\n        \"chatId\": \"={{ $(\\'Telegram Trigger\\').item.json.message.chat.id }}\",\\n        \"message\": \"=Your LinkedIn post draft is ready (text + image).\\\\nYou can now decide what to do next. Simply type your instruction in plain language, and the system will handle it.\\\\nExamples:\\\\n\\\\n‘Looks good, post it.’ → Post\\\\n\\\\n‘Change the text, it feels too formal.’ → Edit text\\\\n\\\\n‘The image should focus more on teamwork.’ → Edit image\\\\n\\\\n‘Cancel this, I don’t want to continue.’ → Terminate\\\\n\\\\n👉 You are free to write anything — the system will classify your input and act accordingly.\",\\n        \"responseType\": \"freeText\",\\n        \"options\": {\\n          \"appendAttribution\": false\\n        }\\n      },\\n      \"type\": \"n8n-nodes-base.telegram\",\\n      \"typeVersion\": 1.2,\\n      \"position\": [\\n        320,\\n        112\\n      ],\\n      \"id\": \"07eb16f1-1579-41e4-8443-c2ef5445eb45\",\\n      \"name\": \"Wait for feedback\",\\n      \"webhookId\": \"a39f1d60-d759-43ca-b5b1-9e0b565f9e8a\",\\n      \"credentials\": {\\n        \"telegramApi\": {\\n          \"id\": \"EFoDusKQAfWdyYaO\",\\n          \"name\": \"Telegram account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Create post\\'s text\",\\n        \"height\": 448,\\n        \"width\": 640,\\n        \"color\": 7\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        -976,\\n        -1104\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"22b6a2cf-2715-479b-81d5-ce549dc5c223\",\\n      \"name\": \"Sticky Note\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Create post\\'s Image\",\\n        \"height\": 448,\\n        \"width\": 640,\\n        \"color\": 7\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        -752,\\n        -576\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"9ece431d-1cba-486c-abff-28e2d035c987\",\\n      \"name\": \"Sticky Note1\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Feedback\",\\n        \"height\": 448,\\n        \"width\": 816\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        -352,\\n        -48\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"3dd8f3c0-3ebf-45b4-ad4d-c2ce905863dd\",\\n      \"name\": \"Sticky Note2\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Terminate operation\",\\n        \"height\": 352,\\n        \"width\": 368,\\n        \"color\": 3\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1152,\\n        -96\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"f507679e-8023-4ee4-9a91-f067273627c9\",\\n      \"name\": \"Sticky Note3\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Post\",\\n        \"height\": 272,\\n        \"width\": 336,\\n        \"color\": 4\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1152,\\n        -1088\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"b9503555-ca57-49dc-8d12-d9ec43450bd4\",\\n      \"name\": \"Sticky Note4\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Refine post\",\\n        \"height\": 688,\\n        \"width\": 848,\\n        \"color\": 5\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1152,\\n        -800\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"4597f92e-43d9-4e6f-9aa9-10036656f7e5\",\\n      \"name\": \"Sticky Note5\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"## Refine Image\",\\n        \"height\": 272,\\n        \"width\": 576,\\n        \"color\": 6\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1376,\\n        -784\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"e0a0047b-199a-4590-9c25-155788058cb5\",\\n      \"name\": \"Sticky Note6\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"## Refine Text\",\\n        \"height\": 256,\\n        \"width\": 368,\\n        \"color\": 6\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1472,\\n        -432\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"c603d205-3fe1-409d-9ef4-aa4b4b44f86e\",\\n      \"name\": \"Sticky Note7\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"sendTo\": \"mohammed-mowina@outlook.com\",\\n        \"subject\": \"LinkedIn Booster\",\\n        \"emailType\": \"text\",\\n        \"message\": \"={{ $(\\'Merge\\').item.json.output }}\",\\n        \"options\": {\\n          \"appendAttribution\": false,\\n          \"attachmentsUi\": {\\n            \"attachmentsBinary\": [\\n              {\\n                \"property\": \"={{ $(\\'Merge\\').item.json.fileName }}\"\\n              }\\n            ]\\n          }\\n        }\\n      },\\n      \"type\": \"n8n-nodes-base.gmail\",\\n      \"typeVersion\": 2.1,\\n      \"position\": [\\n        1360,\\n        -992\\n      ],\\n      \"id\": \"b9b5dfed-369a-4e5d-86e5-8e8324b820a6\",\\n      \"name\": \"Send a message\",\\n      \"webhookId\": \"9e818c1d-3573-4592-8465-6953a6754cef\",\\n      \"credentials\": {\\n        \"gmailOAuth2\": {\\n          \"id\": \"Z3qIvY9PLNzWAis2\",\\n          \"name\": \"Gmail account 2\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"operation\": \"sendPhoto\",\\n        \"chatId\": \"={{ $(\\'Telegram Trigger\\').item.json.message.chat.id }}\",\\n        \"binaryData\": true,\\n        \"additionalFields\": {\\n          \"caption\": \"={{ $(\\'Post\\').item.json.output }}\"\\n        }\\n      },\\n      \"type\": \"n8n-nodes-base.telegram\",\\n      \"typeVersion\": 1.2,\\n      \"position\": [\\n        96,\\n        112\\n      ],\\n      \"id\": \"f6891562-43fb-4bb4-9d63-4494ef2e8b51\",\\n      \"name\": \"Send a photo message\",\\n      \"webhookId\": \"94294c72-0650-4b8b-8444-c6ec80b12d2a\",\\n      \"credentials\": {\\n        \"telegramApi\": {\\n          \"id\": \"EFoDusKQAfWdyYaO\",\\n          \"name\": \"Telegram account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"assignments\": {\\n          \"assignments\": [\\n            {\\n              \"id\": \"ef82052d-4309-4359-aa18-b6f2c8313036\",\\n              \"name\": \"output\",\\n              \"value\": \"={{ $json.output }}\",\\n              \"type\": \"string\"\\n            }\\n          ]\\n        },\\n        \"options\": {\\n          \"ignoreConversionErrors\": true,\\n          \"dotNotation\": true\\n        }\\n      },\\n      \"type\": \"n8n-nodes-base.set\",\\n      \"typeVersion\": 3.4,\\n      \"position\": [\\n        -304,\\n        112\\n      ],\\n      \"id\": \"f2baa1ec-2bcb-466e-881b-eb8f8b926d6e\",\\n      \"name\": \"Post\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"promptType\": \"define\",\\n        \"text\": \"=Old Prompt: {{ $(\\'AI Prompt Writer\\').item.json.output }}\\\\nUser comment: {{ $(\\'Wait for feedback\\').item.json.data.text }}\",\\n        \"options\": {\\n          \"systemMessage\": \"=**Overview**:\\\\nYou are an expert LinkedIn Image Prompt Engineer. For each request, generate AI image prompts following this structure. You work in a workflow and your responsability is to refine an already generated image by making a modified propmt to the Image Generator Model.\\\\n\\\\n*Input*:\\\\n- The old prompt for the generated Image.\\\\n- The user comment on the Image to change it.\\\\n\\\\n*Instructions*:\\\\nA modern, minimalistic professional LinkedIn illustration about the topic\\\\nAvoid text images just some\\\\n\\\\n*Style*:\\\\n3D Object Generator with clean flat vector, LinkedIn color palette (blue, white, grey), simple, corporate-appropriate. \\\\nAvoid clutter, make it inspiring and easy to understand.\\\\n\\\\n*Output*:\\\\nOnly output the Prompt no other suggestions or flowup questions.\"\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\\n      \"typeVersion\": 2.2,\\n      \"position\": [\\n        1456,\\n        -704\\n      ],\\n      \"id\": \"b464c76d-0cde-41f1-ab56-7e3f4690bdd7\",\\n      \"name\": \"AI Prompt Refiner\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"promptType\": \"define\",\\n        \"text\": \"={{ $json.output }}\",\\n        \"options\": {\\n          \"systemMessage\": \"=**Overview**:\\\\nYou are an expert LinkedIn Image Prompt Engineer. For each request, generate AI image prompts following this structure.\\\\n\\\\n*Input*:\\\\nYou will have the post\\'s text as the user Prompt.\\\\n\\\\n*Instructions*:\\\\nA modern, minimalistic professional LinkedIn illustration about the topic\\\\nAvoid text images just some\\\\n\\\\n*Style*:\\\\n3D Object Generator with clean flat vector, LinkedIn color palette (blue, white, grey), simple, corporate-appropriate. \\\\nAvoid clutter, make it inspiring and easy to understand.\\\\n\\\\n*Output*:\\\\nOnly output the Prompt no other suggestions or flowup questions.\"\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\\n      \"typeVersion\": 2.2,\\n      \"position\": [\\n        -656,\\n        -480\\n      ],\\n      \"id\": \"df8b4dfb-b2b9-4e65-bf2d-108a6fb99294\",\\n      \"name\": \"AI Prompt Writer\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"resource\": \"image\",\\n        \"prompt\": \"={{ $json.output }}\",\\n        \"options\": {\\n          \"returnImageUrls\": false\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.openAi\",\\n      \"typeVersion\": 1.8,\\n      \"position\": [\\n        -352,\\n        -480\\n      ],\\n      \"id\": \"85253bd1-a207-42be-8510-bec86925c7d9\",\\n      \"name\": \"OpenAI Post Image Generation\",\\n      \"credentials\": {\\n        \"openAiApi\": {\\n          \"id\": \"WSrQfOLqmZvD07lC\",\\n          \"name\": \"OpenAi account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"resource\": \"image\",\\n        \"prompt\": \"={{ $json.output }}\",\\n        \"options\": {\\n          \"returnImageUrls\": false\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.openAi\",\\n      \"typeVersion\": 1.8,\\n      \"position\": [\\n        1760,\\n        -704\\n      ],\\n      \"id\": \"a9bb6412-f5a5-4514-881f-20f2f19643e8\",\\n      \"name\": \"Regenerate an image\",\\n      \"credentials\": {\\n        \"openAiApi\": {\\n          \"id\": \"WSrQfOLqmZvD07lC\",\\n          \"name\": \"OpenAi account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"mode\": \"combine\",\\n        \"combineBy\": \"combineByPosition\",\\n        \"options\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.merge\",\\n      \"typeVersion\": 3.2,\\n      \"position\": [\\n        -96,\\n        112\\n      ],\\n      \"id\": \"33ab8e06-59b3-4428-9fb6-d376bd565ace\",\\n      \"name\": \"Merge\",\\n      \"retryOnFail\": false\\n    },\\n    {\\n      \"parameters\": {\\n        \"mode\": \"combine\",\\n        \"combineBy\": \"combineByPosition\",\\n        \"options\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.merge\",\\n      \"typeVersion\": 3.2,\\n      \"position\": [\\n        1200,\\n        -992\\n      ],\\n      \"id\": \"9e582664-2a82-469a-b642-20bdf2d52f01\",\\n      \"name\": \"Merge1\",\\n      \"retryOnFail\": false\\n    }\\n  ],\\n  \"pinData\": {},\\n  \"connections\": {\\n    \"Google Gemini Chat Model\": {\\n      \"ai_languageModel\": [\\n        [\\n          {\\n            \"node\": \"Post writer\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"Text Classifier\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"Post refiner\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"AI Prompt Writer\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"AI Prompt Refiner\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Tavily\": {\\n      \"ai_tool\": [\\n        [\\n          {\\n            \"node\": \"Post writer\",\\n            \"type\": \"ai_tool\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Text Classifier\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Merge1\",\\n            \"type\": \"main\",\\n            \"index\": 1\\n          }\\n        ],\\n        [\\n          {\\n            \"node\": \"Post refiner\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ],\\n        [\\n          {\\n            \"node\": \"No Operation, do nothing\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ],\\n        [\\n          {\\n            \"node\": \"AI Prompt Refiner\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Telegram Trigger\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Post writer\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Post writer\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"AI Prompt Writer\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"Post\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Post refiner\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Post\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Wait for feedback\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Text Classifier\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Create a post\": {\\n      \"main\": [\\n        []\\n      ]\\n    },\\n    \"Send a photo message\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Wait for feedback\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Post\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Merge\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"AI Prompt Refiner\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Regenerate an image\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"AI Prompt Writer\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"OpenAI Post Image Generation\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"OpenAI Post Image Generation\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Merge\",\\n            \"type\": \"main\",\\n            \"index\": 1\\n          }\\n        ]\\n      ]\\n    },\\n    \"Regenerate an image\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Merge\",\\n            \"type\": \"main\",\\n            \"index\": 1\\n          }\\n        ]\\n      ]\\n    },\\n    \"Merge\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Send a photo message\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"Merge1\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Merge1\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Send a message\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    }\\n  },\\n  \"active\": false,\\n  \"settings\": {\\n    \"executionOrder\": \"v1\"\\n  },\\n  \"versionId\": \"d01124c9-2c7a-40fc-ac9e-7edaa4535fc4\",\\n  \"meta\": {\\n    \"templateCredsSetupCompleted\": true,\\n    \"instanceId\": \"e42899c7dfda6042a4f9f2f2b7e15acf58d2ab8ee4dfbe7dec48ee2a3be4e756\"\\n  },\\n  \"id\": \"dlis4ObYbraVh6Bt\",\\n  \"tags\": []\\n}'},\n",
       "  {'path': 'studio/__init__.py',\n",
       "   'content': '# This makes studio a proper Python package'},\n",
       "  {'path': 'studio/graph.py',\n",
       "   'content': '\"\"\"Standalone graph file for LangGraph Studio.\"\"\"\\nimport sys\\nimport os\\nfrom typing import Optional\\nfrom dataclasses import dataclass\\n\\n# Add the parent directory to the path so we can import from workflow\\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\\n\\nfrom langgraph.graph import StateGraph, END\\n\\n# Define state directly to avoid import conflicts\\n@dataclass\\nclass WorkflowState:\\n    \"\"\"State management for the LinkedIn post creation workflow.\"\"\"\\n    user_input: str = \"\"\\n    post_text: str = \"\"\\n    image_prompt: str = \"\"\\n    image_data: Optional[bytes] = None\\n    image_url: str = \"\"\\n    user_feedback: str = \"\"\\n    classification: str = \"\"\\n    refined_text: str = \"\"\\n    refined_image_data: Optional[bytes] = None\\n    current_step: str = \"start\"\\n    error_message: str = \"\"\\n    final_post_ready: bool = False\\n\\n# Import nodes after defining state\\ntry:\\n    from workflow.nodes import (\\n        create_post_text,\\n        generate_image,\\n        classify_feedback,\\n        refine_text,\\n        refine_image,\\n        send_email,\\n    )\\nexcept ImportError as e:\\n    # Fallback dummy functions for studio\\n    def create_post_text(state: WorkflowState) -> WorkflowState:\\n        state.post_text = \"Sample LinkedIn post created\"\\n        return state\\n    \\n    def generate_image(state: WorkflowState) -> WorkflowState:\\n        state.image_url = \"https://example.com/image.jpg\"\\n        return state\\n    \\n    def classify_feedback(state: WorkflowState) -> WorkflowState:\\n        state.classification = \"approved\"\\n        return state\\n    \\n    def refine_text(state: WorkflowState) -> WorkflowState:\\n        state.refined_text = \"Refined post text\"\\n        return state\\n    \\n    def refine_image(state: WorkflowState) -> WorkflowState:\\n        state.refined_image_data = b\"refined_image_data\"\\n        return state\\n    \\n    def send_email(state: WorkflowState) -> WorkflowState:\\n        state.final_post_ready = True\\n        return state\\n\\n\\ndef route_feedback(state: WorkflowState) -> str:\\n    \"\"\"Route based on feedback classification.\"\"\"\\n    classification = state.classification.lower()\\n    if \"approved\" in classification:\\n        return \"approved\"\\n    if \"refine text\" in classification or \"refine_text\" in classification:\\n        return \"refine_text\"\\n    if \"refine image\" in classification or \"refine_image\" in classification:\\n        return \"refine_image\"\\n    if \"terminate\" in classification:\\n        return \"terminate\"\\n    return \"terminate\"\\n\\n\\ndef create_studio_workflow():\\n    \"\"\"Create the LangGraph workflow for Studio (without checkpointer).\"\"\"\\n    workflow = StateGraph(WorkflowState)\\n    workflow.add_node(\"create_post\", create_post_text)\\n    workflow.add_node(\"generate_image\", generate_image)\\n    workflow.add_node(\"classify_feedback\", classify_feedback)\\n    workflow.add_node(\"refine_text\", refine_text)\\n    workflow.add_node(\"refine_image\", refine_image)\\n    workflow.add_node(\"send_email\", send_email)\\n    workflow.set_entry_point(\"create_post\")\\n    workflow.add_edge(\"create_post\", \"generate_image\")\\n    workflow.add_edge(\"generate_image\", \"classify_feedback\")\\n    workflow.add_conditional_edges(\\n        \"classify_feedback\",\\n        route_feedback,\\n        {\\n            \"approved\": \"send_email\",\\n            \"refine_text\": \"refine_text\",\\n            \"refine_image\": \"refine_image\",\\n            \"terminate\": END,\\n        },\\n    )\\n    workflow.add_edge(\"refine_text\", \"classify_feedback\")\\n    workflow.add_edge(\"refine_image\", \"classify_feedback\")\\n    workflow.add_edge(\"send_email\", END)\\n    \\n    # Compile without checkpointer for LangGraph Studio\\n    app = workflow.compile(\\n        interrupt_after=[\"generate_image\", \"refine_text\", \"refine_image\"],\\n    )\\n    return app\\n\\n\\n# Create the app for LangGraph Studio\\napp = create_studio_workflow()\\n\\n# Export the app for LangGraph Studio\\n__all__ = [\"app\"]'},\n",
       "  {'path': 'studio/langgraph.json',\n",
       "   'content': '{\\n    \"dockerfile_lines\": [],\\n    \"graphs\": {\\n      \"chatbot\": \"./graph.py:app\"\\n    },\\n    \"env\": \"./.env\",\\n    \"python_version\": \"3.11\",\\n    \"dependencies\": [\\n      \".\"\\n    ]\\n  }'},\n",
       "  {'path': 'utils/config.py',\n",
       "   'content': 'import os\\nfrom dataclasses import dataclass\\nfrom dotenv import load_dotenv\\n\\n\\ndef load_env() -> None:\\n    \"\"\"Load environment variables from a .env file if present.\"\"\"\\n    load_dotenv()\\n\\n\\n@dataclass\\nclass Config:\\n    OPENAI_API_KEY: str | None = os.getenv(\"OPENAI_API_KEY\")\\n    GOOGLE_API_KEY: str | None = os.getenv(\"GOOGLE_API_KEY\")\\n    TAVILY_API_KEY: str | None = os.getenv(\"TAVILY_API_KEY\")\\n    GMAIL_CREDENTIALS_FILE: str = os.getenv(\"GMAIL_CREDENTIALS_FILE\", \"credentials.json\")\\n    GMAIL_TOKEN_FILE: str = os.getenv(\"GMAIL_TOKEN_FILE\", \"token.json\")\\n    RECIPIENT_EMAIL: str = os.getenv(\"RECIPIENT_EMAIL\", \"mohammed-mowina@outlook.com\")\\n\\n\\n'},\n",
       "  {'path': 'workflow/__init__.py',\n",
       "   'content': '\"\"\"LinkedIn Booster workflow package.\"\"\"\\n\\nfrom .graph import app, create_workflow, start_workflow, continue_workflow\\n\\n__all__ = [\"app\", \"create_workflow\", \"start_workflow\", \"continue_workflow\"]'},\n",
       "  {'path': 'workflow/graph.py',\n",
       "   'content': 'from langgraph.graph import StateGraph, END\\nfrom langgraph.checkpoint.memory import MemorySaver\\n\\nfrom workflow.state import WorkflowState\\nfrom workflow.nodes import (\\n    create_post_text,\\n    generate_image,\\n    classify_feedback,\\n    refine_text,\\n    refine_image,\\n    send_email,\\n)\\n\\n\\ndef route_feedback(state: WorkflowState) -> str:\\n    \"\"\"Route based on feedback classification.\"\"\"\\n    classification = state.classification.lower()\\n    if \"approved\" in classification:\\n        return \"approved\"\\n    if \"refine text\" in classification or \"refine_text\" in classification:\\n        return \"refine_text\"\\n    if \"refine image\" in classification or \"refine_image\" in classification:\\n        return \"refine_image\"\\n    if \"terminate\" in classification:\\n        return \"terminate\"\\n    return \"terminate\"\\n\\n\\ndef create_workflow():\\n    \"\"\"Create the LangGraph workflow with interrupts configured.\"\"\"\\n    workflow = StateGraph(WorkflowState)\\n    workflow.add_node(\"create_post\", create_post_text)\\n    workflow.add_node(\"generate_image\", generate_image)\\n    workflow.add_node(\"classify_feedback\", classify_feedback)\\n    workflow.add_node(\"refine_text\", refine_text)\\n    workflow.add_node(\"refine_image\", refine_image)\\n    workflow.add_node(\"send_email\", send_email)\\n    workflow.set_entry_point(\"create_post\")\\n    workflow.add_edge(\"create_post\", \"generate_image\")\\n    workflow.add_edge(\"generate_image\", \"classify_feedback\")\\n    workflow.add_conditional_edges(\\n        \"classify_feedback\",\\n        route_feedback,\\n        {\\n            \"approved\": \"send_email\",\\n            \"refine_text\": \"refine_text\",\\n            \"refine_image\": \"refine_image\",\\n            \"terminate\": END,\\n        },\\n    )\\n    workflow.add_edge(\"refine_text\", \"classify_feedback\")\\n    workflow.add_edge(\"refine_image\", \"classify_feedback\")\\n    workflow.add_edge(\"send_email\", END)\\n    memory = MemorySaver()\\n    app = workflow.compile(\\n        checkpointer=memory,\\n        interrupt_after=[\"generate_image\", \"refine_text\", \"refine_image\"],\\n    )\\n    return app\\n\\n\\n# Convenience API mirroring notebook helpers\\napp = create_workflow()\\n\\n\\ndef start_workflow(user_input: str, thread_id: str = \"default\") -> WorkflowState:\\n    initial_state = WorkflowState(user_input=user_input)\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\n    result = app.invoke(initial_state, config)\\n    return WorkflowState(**result)\\n\\n\\ndef continue_workflow(user_feedback: str, thread_id: str = \"default\") -> WorkflowState:\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\n    app.update_state(config, {\"user_feedback\": user_feedback})\\n    result = app.invoke(None, config)\\n    return WorkflowState(**result)\\n\\n\\n'},\n",
       "  {'path': 'workflow/nodes.py',\n",
       "   'content': 'import os\\nfrom typing import List\\n\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain.schema import SystemMessage, HumanMessage\\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\\nfrom langgraph.errors import GraphRecursionError\\nfrom langgraph.prebuilt import create_react_agent\\n# Gmail imports will be done lazily in send_email function\\n\\nfrom workflow.state import WorkflowState\\nfrom workflow.tools import tavily_tool\\n\\n\\n# Initialize models\\ngemini_model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\\n\\ndef create_post_text(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Generate LinkedIn post text based on user input\"\"\"\\n    system_message = \"\"\"\\n    You are a professional LinkedIn content creator. Create engaging, professional LinkedIn posts.\\n\\n    Your tone should be:\\n    - Professional yet approachable\\n    - Concise and clear\\n    - Action-oriented with clear value\\n\\n    Format your post with:\\n    - A strong hook in the first sentence\\n    - Short, scannable paragraphs\\n    - 1-3 relevant emojis strategically placed\\n    - A clear call-to-action\\n    - 3-5 relevant hashtags at the end\\n\\n    Output ONLY the post text, nothing else.\"\"\"\\n    \\n    try:\\n        max_iterations = 3\\n        recursion_limit = 2 * max_iterations + 1\\n        \\n        agent = create_react_agent(\\n            model=\"openai:gpt-4o-mini\",\\n            tools=[tavily_tool],\\n            prompt=system_message,\\n            debug=True\\n        )\\n        \\n        try:\\n            response = agent.invoke(\\n                {\"messages\": [{\"role\": \"user\", \"content\": state.user_input}]},\\n                {\"recursion_limit\": recursion_limit},\\n            )\\n            \\n            state.post_text = response[\\'messages\\'][-1].content\\n            state.current_step = \"create_image\"\\n            \\n        except GraphRecursionError:\\n            print(\"Agent stopped due to max iterations.\")\\n            state.error_message = \"Agent stopped due to max iterations\"\\n            state.current_step = \"error\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error creating post text: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef generate_image(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Generate image directly from post text\"\"\"\\n    \\n    try:\\n        post_text = state.post_text\\n        \\n        # Create a proper prompt for image generation\\n        image_prompt_template = \"\"\"\\n        Create a modern, minimalistic professional LinkedIn illustration for the following post: {post_content}\\n        \\n        Style: 3D Object Generator with clean flat vector, LinkedIn color palette (blue, white, grey), simple, corporate-appropriate.\\n        Avoid clutter, make it inspiring and easy to understand.\\n        No text in the image, just visual elements.\\n        \\n        Generate only the image description prompt, no other text.\\n        \"\"\"\\n        \\n        # Create messages for the LLM\\n        messages = [\\n            SystemMessage(content=\"You are an expert LinkedIn Image Prompt Engineer. Generate concise AI image prompts.\"),\\n            HumanMessage(content=image_prompt_template.format(post_content=post_text))\\n        ]\\n        \\n        # Get the image prompt from Gemini\\n        response = gemini_model.invoke(messages)\\n        image_prompt = response.content.strip()\\n        \\n        print(f\"Image prompt: {image_prompt}\")\\n        \\n        # Generate image using DALL-E\\n        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\\n        image_url = dalle_wrapper.run(image_prompt)\\n        \\n        # Update state\\n        state.image_url = image_url\\n        state.image_prompt = image_prompt\\n        state.current_step = \"wait_feedback\" \\n        \\n    except Exception as e:\\n        state.error_message = f\"Error generating image: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef classify_feedback(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Classify user feedback into categories\"\"\"\\n    system_message = \"\"\"You are a text classifier. Classify the input text into one of these categories:\\n\\n        1. Approved - Approval message refers that its ok to proceed with the current status.\\n        2. Refine Text - Request to refine the post text\\n        3. Terminate - Termination request for the process.\\n        4. Refine Image - Request to refine the post image\\n\\n        Output only the category name, nothing else.\"\"\"\\n\\n    try:\\n        messages = [\\n            SystemMessage(content=system_message),\\n            HumanMessage(content=state.user_feedback)\\n        ]\\n        \\n        response = gemini_model.invoke(messages)\\n        classification = response.content.strip()\\n        \\n        # Map classification to next step\\n        if \"Approved\" in classification:\\n            state.classification = \"Approved\"\\n            state.current_step = \"post_content\"\\n            state.final_post_ready = True\\n        elif \"Refine Text\" in classification:\\n            state.classification = \"Refine Text\"\\n            state.current_step = \"refine_text\"\\n        elif \"Terminate\" in classification:\\n            state.classification = \"Terminate\"\\n            state.current_step = \"terminate\"\\n        elif \"Refine Image\" in classification:\\n            state.classification = \"Refine Image\"\\n            state.current_step = \"refine_image\"\\n        else:\\n            state.classification = \"Unknown\"\\n            state.current_step = \"wait_feedback\"\\n            state.error_message = f\"Unrecognized feedback classification: {classification}\"\\n        \\n        print(f\"DEBUG: Updated state - classification: {state.classification}, current_step: {state.current_step}\")\\n            \\n    except Exception as e:\\n        state.error_message = f\"Error classifying feedback: {str(e)}\"\\n        state.current_step = \"error\"\\n        print(f\"DEBUG: Exception in classify_feedback: {str(e)}\")\\n    \\n    return state\\n\\n\\ndef refine_text(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Refine the post text based on user feedback\"\"\"\\n    system_message = \"\"\"\\n        You are a helpful assistant who improve and refine LinkedIn post.\\n\\n        Output only the post text.\"\"\"\\n\\n    try:\\n        prompt = f\"Original post: {state.post_text}\\\\\\\\n\\\\\\\\nRequest: {state.user_feedback}\"\\n        \\n        messages = [\\n            SystemMessage(content=system_message),\\n            HumanMessage(content=prompt)\\n        ]\\n        \\n        response = gemini_model.invoke(messages)\\n        state.refined_text = response.content\\n        state.post_text = state.refined_text  # Update the main post text\\n        state.current_step = \"wait_feedback\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error refining text: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef refine_image(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Refine the image based on user feedback\"\"\"\\n    try:\\n        # Use the existing image prompt and add user feedback\\n        modified_prompt = f\"{state.image_prompt}. {state.user_feedback}\"\\n        \\n        print(f\"Refined image prompt: {modified_prompt}\")\\n        \\n        # Generate image using DALL-E with the modified prompt\\n        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\\n        image_url = dalle_wrapper.run(modified_prompt)\\n        \\n        # Update state with the refined image\\n        state.image_url = image_url\\n        state.image_prompt = modified_prompt  # Update the prompt with feedback\\n        state.current_step = \"wait_feedback\"  # Go back to wait for more feedback\\n        \\n        print(f\"Refined image URL: {image_url}\")\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error refining image: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef send_email(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Send email with the final post content\"\"\"\\n    try:\\n        # Lazy import Gmail functionality\\n        from langchain_google_community import GmailToolkit\\n        from langchain_google_community.gmail.utils import (\\n            build_resource_service,\\n            get_gmail_credentials,\\n        )\\n\\n        # Test imports without initializing toolkit\\n        print(\"Imports successful!\")\\n\\n            \\n        # Initialize Gmail toolkit\\n        credentials = get_gmail_credentials(\\n            token_file=\"token.json\",\\n            scopes=[\"https://mail.google.com/\"],\\n            client_secrets_file=\"credentials.json\",\\n        )\\n        \\n        api_resource = build_resource_service(credentials=credentials)\\n        gmail_toolkit = GmailToolkit(api_resource=api_resource)\\n        \\n        tools = gmail_toolkit.get_tools()\\n        print(f\\'🚩DEBUG: send mail tool: {tools[1]}\\')\\n        # Compose email content\\n        subject = \"LinkedIn Post Ready for Publishing\"\\n        body = f\"\"\"\\n            Your LinkedIn post is ready:\\n\\n            {state.post_text}\\n\\n            Image URL: {state.image_url}\\n\\n            Best regards,\\n            LinkedIn Post Creator\\n            \"\"\"\\n        \\n        # Send email using Gmail toolkit\\n        send_message_tool = gmail_toolkit.get_tools()[1]  # send_gmail_message tool\\n        \\n        email_result = send_message_tool.run({\\n            \"to\": \"mohammed-mowina@outlook.com\",\\n            \"subject\": subject,\\n            \"message\": body\\n        })\\n        \\n        state.current_step = \"completed\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"🚩Error sending email: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state'},\n",
       "  {'path': 'workflow/state.py',\n",
       "   'content': 'from dataclasses import dataclass\\nfrom typing import Optional\\n\\n\\n@dataclass\\nclass WorkflowState:\\n    \"\"\"State management for the LinkedIn post creation workflow.\"\"\"\\n    user_input: str = \"\"\\n    post_text: str = \"\"\\n    image_prompt: str = \"\"\\n    image_data: Optional[bytes] = None\\n    image_url: str = \"\"\\n    user_feedback: str = \"\"\\n    classification: str = \"\"\\n    refined_text: str = \"\"\\n    refined_image_data: Optional[bytes] = None\\n    current_step: str = \"start\"\\n    error_message: str = \"\"\\n    final_post_ready: bool = False\\n\\n\\n'},\n",
       "  {'path': 'workflow/tools.py',\n",
       "   'content': 'import os\\nimport requests\\nfrom typing import Any, Dict\\n\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\nfrom langchain.tools import Tool\\nfrom langchain_core.tools import tool\\n\\ndef tavily_search(query: str) -> str:\\n    \"\"\"Search the web using Tavily API\"\"\"\\n    try:\\n        url = \"https://api.tavily.com/search\"\\n        headers = {\"Authorization\": f\"Bearer {Config.TAVILY_API_KEY}\"}\\n        data = {\"query\": query}\\n        \\n        response = requests.post(url, headers=headers, json=data)\\n        response.raise_for_status()\\n        \\n        results = response.json()\\n        # Extract relevant information from search results\\n        search_results = []\\n        for result in results.get(\"results\", [])[:3]:  # Top 3 results\\n            search_results.append(f\"Title: {result.get(\\'title\\', \\'\\')}\\\\nContent: {result.get(\\'content\\', \\'\\')[:200]}...\")\\n        \\n        return \"\\\\n\\\\n\".join(search_results)\\n    except Exception as e:\\n        return f\"Search failed: {str(e)}\"\\n\\n\\n# Create Tavily search tool\\ntavily_tool = Tool(\\n    name=\"web_search\",\\n    description=\"Search the web for current information\",\\n    func=tavily_search\\n)\\n'}]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunked Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_from_extension(file_path: str) -> Optional[Language]:\n",
    "    \"\"\"\n",
    "    Map file extensions to LangChain Language enum values.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file\n",
    "        \n",
    "    Returns:\n",
    "        Optional[Language]: Corresponding Language enum or None if not supported\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    # Mapping of file extensions to LangChain Language enum\n",
    "    extension_map = {\n",
    "        '.py': Language.PYTHON,\n",
    "        '.js': Language.JS,\n",
    "        '.ts': Language.TS,\n",
    "        '.jsx': Language.JS,\n",
    "        '.tsx': Language.TS,\n",
    "        '.java': Language.JAVA,\n",
    "        '.kt': Language.KOTLIN,\n",
    "        '.cpp': Language.CPP,\n",
    "        '.cc': Language.CPP,\n",
    "        '.cxx': Language.CPP,\n",
    "        '.c': Language.C,\n",
    "        '.h': Language.C,\n",
    "        '.hpp': Language.CPP,\n",
    "        '.cs': Language.CSHARP,\n",
    "        '.php': Language.PHP,\n",
    "        '.rb': Language.RUBY,\n",
    "        '.go': Language.GO,\n",
    "        '.rs': Language.RUST,\n",
    "        '.swift': Language.SWIFT,\n",
    "        '.scala': Language.SCALA,\n",
    "        '.md': Language.MARKDOWN,\n",
    "        '.html': Language.HTML,\n",
    "        '.htm': Language.HTML,\n",
    "        '.sol': Language.SOL,\n",
    "        '.lua': Language.LUA,\n",
    "        '.pl': Language.PERL,\n",
    "        '.hs': Language.HASKELL,\n",
    "        '.ex': Language.ELIXIR,\n",
    "        '.exs': Language.ELIXIR,\n",
    "        '.ps1': Language.POWERSHELL,\n",
    "        '.vb': Language.VISUALBASIC6,\n",
    "        '.proto': Language.PROTO,\n",
    "        '.rst': Language.RST,\n",
    "        '.tex': Language.LATEX,\n",
    "        '.cob': Language.COBOL,\n",
    "        '.cbl': Language.COBOL,\n",
    "    }\n",
    "    \n",
    "    return extension_map.get(ext)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splitter_for_language(language: Language, chunk_size: int = 4000, chunk_overlap: int = 400) -> RecursiveCharacterTextSplitter:\n",
    "    \"\"\"\n",
    "    Create a language-specific text splitter.\n",
    "    \n",
    "    Args:\n",
    "        language (Language): The programming language\n",
    "        chunk_size (int): Maximum chunk size in characters\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        RecursiveCharacterTextSplitter: Configured splitter\n",
    "    \"\"\"\n",
    "    return RecursiveCharacterTextSplitter.from_language(\n",
    "        language=language,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_with_chunking(file_path: str, content: str, chunk_size: int, chunk_overlap: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process a single file with intelligent chunking based on language.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file\n",
    "        content (str): File content\n",
    "        chunk_size (int): Maximum chunk size\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of chunk information dictionaries\n",
    "    \"\"\"\n",
    "    # Determine if we should chunk this file\n",
    "    language = get_language_from_extension(file_path)\n",
    "    \n",
    "    # If file is small enough or no language-specific splitter, return as single chunk\n",
    "    if len(content) <= chunk_size or language is None:\n",
    "        return [{\n",
    "            'path': file_path,\n",
    "            'content': content,\n",
    "            'chunk_index': 0,\n",
    "            'total_chunks': 1,\n",
    "            'is_chunked': False,\n",
    "            'language': language.value if language else 'unknown'\n",
    "        }]\n",
    "    \n",
    "    # Use language-specific chunking\n",
    "    try:\n",
    "        splitter = create_splitter_for_language(language, chunk_size, chunk_overlap)\n",
    "        documents = splitter.create_documents([content])\n",
    "        \n",
    "        chunks = []\n",
    "        for i, doc in enumerate(documents):\n",
    "            chunk_info = {\n",
    "                'path': f\"{file_path}#chunk_{i}\",  # Add chunk identifier to path\n",
    "                'original_path': file_path,\n",
    "                'content': doc.page_content,\n",
    "                'chunk_index': i,\n",
    "                'total_chunks': len(documents),\n",
    "                'is_chunked': True,\n",
    "                'language': language.value,\n",
    "                'chunk_size_actual': len(doc.page_content)\n",
    "            }\n",
    "            chunks.append(chunk_info)\n",
    "        \n",
    "        return chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error chunking file {file_path}: {e}\")\n",
    "        # Fallback to single chunk\n",
    "        return [{\n",
    "            'path': file_path,\n",
    "            'content': content,\n",
    "            'chunk_index': 0,\n",
    "            'total_chunks': 1,\n",
    "            'is_chunked': False,\n",
    "            'language': language.value if language else 'unknown',\n",
    "            'chunking_error': str(e)\n",
    "        }]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_repo_info_chunked(repo_path: str, chunk_size: int = 4000, chunk_overlap: int = 400) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Extract information from a Git repository with intelligent chunking.\n",
    "    Returns a dictionary with repo metadata and chunked file contents.\n",
    "    Uses LangChain's language-aware text splitting for better context preservation.\n",
    "    \n",
    "    Args:\n",
    "        repo_path (str): Path to the Git repository\n",
    "        chunk_size (int): Maximum chunk size in characters (default: 4000)\n",
    "        chunk_overlap (int): Overlap between chunks (default: 400)\n",
    "    \n",
    "    Returns:\n",
    "        Optional[Dict]: Repository information with chunked files or None if error\n",
    "    \"\"\"\n",
    "    repo = None\n",
    "    try:\n",
    "        repo = git.Repo(repo_path)\n",
    "        repo_info = {\n",
    "            'repo_name': os.path.basename(repo_path),\n",
    "            'commit_count': len(list(repo.iter_commits())),\n",
    "            'branches': [branch.name for branch in repo.branches],\n",
    "            'files': [],\n",
    "            'chunking_info': {\n",
    "                'chunk_size': chunk_size,\n",
    "                'chunk_overlap': chunk_overlap,\n",
    "                'total_chunks': 0,\n",
    "                'files_chunked': 0,\n",
    "                'files_not_chunked': 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Define file extensions to INCLUDE (whitelist approach)\n",
    "        relevant_extensions = {\n",
    "            '.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.cpp', '.c', '.h',\n",
    "            '.cs', '.php', '.rb', '.go', '.rs', '.swift', '.kt', '.scala',\n",
    "            '.html', '.css', '.scss', '.sass', '.less',\n",
    "            '.md', '.txt', '.rst', '.json', '.yaml', '.yml', '.toml',\n",
    "            '.sql', '.sh', '.bat', '.ps1', '.proto', '.tex', '.lua', '.pl',\n",
    "            '.hs', '.ex', '.exs', '.vb', '.sol', '.cob', '.cbl',\n",
    "            '.dockerfile', '.gitignore', '.env.example'\n",
    "        }\n",
    "        \n",
    "        # Define files to INCLUDE by name (regardless of extension)\n",
    "        relevant_filenames = {\n",
    "            'README', 'LICENSE', 'CHANGELOG', 'CONTRIBUTING', 'INSTALL',\n",
    "            'Dockerfile', 'Makefile', 'requirements.txt', 'package.json',\n",
    "            'setup.py', 'pyproject.toml', 'Cargo.toml', 'pom.xml',\n",
    "            'build.gradle', 'composer.json', 'Gemfile'\n",
    "        }\n",
    "        \n",
    "        # Define directories to SKIP\n",
    "        skip_directories = {\n",
    "            'node_modules', '.git', '__pycache__', '.pytest_cache',\n",
    "            'venv', 'env', '.env', 'build', 'dist', 'target',\n",
    "            '.idea', '.vscode', 'logs', 'tmp', 'temp',\n",
    "            'images', 'assets', 'static/images', 'public/images'\n",
    "        }\n",
    "        \n",
    "        # Get file contents from the latest commit in the active branch\n",
    "        tree = repo.head.commit.tree\n",
    "        for item in tree.traverse():\n",
    "            if item.type == 'blob':  # Only process files, not directories\n",
    "                # Skip files in unwanted directories\n",
    "                if any(skip_dir in item.path for skip_dir in skip_directories):\n",
    "                    continue\n",
    "                \n",
    "                # Check if file should be included\n",
    "                file_ext = os.path.splitext(item.path)[1].lower()\n",
    "                filename = os.path.basename(item.path)\n",
    "                filename_no_ext = os.path.splitext(filename)[0].upper()\n",
    "                \n",
    "                # Include if extension or filename matches our criteria\n",
    "                should_include = (\n",
    "                    file_ext in relevant_extensions or\n",
    "                    filename_no_ext in relevant_filenames or\n",
    "                    filename in relevant_filenames\n",
    "                )\n",
    "                \n",
    "                if not should_include:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Try to decode as UTF-8, skip if it fails\n",
    "                    content = item.data_stream.read().decode('utf-8')\n",
    "                    # Skip files with null bytes (binary content)\n",
    "                    if '\\x00' in content:\n",
    "                        continue\n",
    "                    \n",
    "                    # Skip very large files (>500KB for chunking)\n",
    "                    if len(content) > 500000:\n",
    "                        continue\n",
    "                    \n",
    "                    # Process the file with chunking\n",
    "                    processed_chunks = process_file_with_chunking(\n",
    "                        item.path, content, chunk_size, chunk_overlap\n",
    "                    )\n",
    "                    \n",
    "                    # Add chunks to repo info\n",
    "                    for chunk_info in processed_chunks:\n",
    "                        repo_info['files'].append(chunk_info)\n",
    "                    \n",
    "                    # Update chunking statistics\n",
    "                    if len(processed_chunks) > 1:\n",
    "                        repo_info['chunking_info']['files_chunked'] += 1\n",
    "                        repo_info['chunking_info']['total_chunks'] += len(processed_chunks)\n",
    "                    else:\n",
    "                        repo_info['chunking_info']['files_not_chunked'] += 1\n",
    "                        repo_info['chunking_info']['total_chunks'] += 1\n",
    "                        \n",
    "                except UnicodeDecodeError:\n",
    "                    # Skip files that can't be decoded as text\n",
    "                    continue\n",
    "        \n",
    "        print(f\"Processed {len(repo_info['files'])} chunks from {repo_info['repo_name']}\")\n",
    "        print(f\"Chunking stats: {repo_info['chunking_info']['files_chunked']} files chunked, \"\n",
    "              f\"{repo_info['chunking_info']['files_not_chunked']} files kept whole\")\n",
    "        \n",
    "        return repo_info\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing Git repo: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Close the repository to release file handles\n",
    "        if repo is not None:\n",
    "            repo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_repo_info_chunked(github_url: str, branch: Optional[str] = None, \n",
    "                                chunk_size: int = 4000, chunk_overlap: int = 400) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Clone a GitHub repo temporarily and extract its information with chunking.\n",
    "    \n",
    "    Args:\n",
    "        github_url (str): GitHub repository URL\n",
    "        branch (Optional[str]): Specific branch to clone\n",
    "        chunk_size (int): Maximum chunk size in characters\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        Optional[Dict]: Repository information with chunked files or None if error\n",
    "    \"\"\"\n",
    "    repo_name = get_repo_name_from_url(github_url)\n",
    "    base_tmp = tempfile.mkdtemp()   # e.g. /tmp/tmpabcd1234\n",
    "    repo_dir = os.path.join(base_tmp, repo_name)\n",
    "    repo = None\n",
    "    \n",
    "    try:\n",
    "        repo = Repo.clone_from(github_url, repo_dir, branch=branch)\n",
    "        result = get_git_repo_info_chunked(repo_dir, chunk_size, chunk_overlap)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error cloning GitHub repo: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Close the repository to release file handles\n",
    "        if repo is not None:\n",
    "            repo.close()\n",
    "        \n",
    "        # Use onerror callback to handle readonly files on Windows\n",
    "        try:\n",
    "            shutil.rmtree(base_tmp, onerror=remove_readonly)\n",
    "        except Exception as cleanup_error:\n",
    "            print(f\"Warning: Could not clean up temp directory: {cleanup_error}\")\n",
    "            # Try alternative cleanup method\n",
    "            try:\n",
    "                import subprocess\n",
    "                subprocess.run(['rmdir', '/s', '/q', base_tmp], shell=True, check=False)\n",
    "            except:\n",
    "                pass  # If all cleanup methods fail, just continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20 chunks from LinkedIn-Booster\n",
      "Chunking stats: 3 files chunked, 11 files kept whole\n"
     ]
    }
   ],
   "source": [
    "repo_info = get_github_repo_info_chunked(github_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repo_name': 'LinkedIn-Booster',\n",
       " 'commit_count': 6,\n",
       " 'branches': ['main'],\n",
       " 'files': [{'path': 'README.md#chunk_0',\n",
       "   'original_path': 'README.md',\n",
       "   'content': '# LinkedIn-Booster 🚀\\n\\nAn intelligent AI-powered LinkedIn post creation workflow that combines the power of LangChain, LangGraph, and multiple AI services to create engaging LinkedIn content with matching visuals.\\n\\n![LangGraph Workflow](images/LangGraph-workflow-diagram.png)\\n\\n## ✨ Features\\n\\n- **🤖 AI-Powered Generation**: Google Gemini for high-quality LinkedIn post drafting\\n- **🔍 Web Research**: Tavily search integration for up-to-date context and insights\\n- **🔄 Interactive Feedback Loop**: Intelligent feedback classification and content refinement\\n- **🎨 Image Generation**: DALL·E 3 integration for compelling visual content\\n- **📧 Email Delivery**: Gmail API integration for seamless content distribution\\n- **🖥️ Multiple Interfaces**: CLI and web-based Chainlit UI options\\n- **📊 Workflow Visualization**: LangGraph Studio integration for workflow monitoring\\n\\n## 🏗️ Architecture\\n\\nThe project follows a modular architecture with clear separation of concerns:\\n\\n```\\nLinkedIn-Booster/\\n├── 📄 main.py                    # CLI interface\\n├── 🌐 app.py                     # Chainlit web interface  \\n├── 📁 workflow/\\n│   ├── __init__.py\\n│   ├── state.py                  # Workflow state management\\n│   ├── tools.py                  # External API integrations (Tavily)\\n│   ├── nodes.py                  # Core workflow nodes\\n│   └── graph.py                  # LangGraph workflow orchestration\\n├── 📁 utils/\\n│   ├── __init__.py\\n│   └── config.py                 # Configuration and environment setup\\n├── 📁 studio/                    # LangGraph Studio configuration\\n├── 📁 notebooks/                 # Development notebooks\\n├── 📁 images/                    # Documentation assets\\n├── 📄 requirements.txt\\n└── 📄 .env                       # Environment variables\\n```\\n\\n## 🚀 Quick Start\\n\\n### Prerequisites\\n- Python 3.8+\\n- API keys for OpenAI, Google Gemini, and Tavily\\n- Gmail API credentials (optional, for email functionality)\\n\\n### Installation\\n\\n1. **Clone and install dependencies**\\n```bash\\ngit clone <repository-url>\\ncd LinkedIn-Booster\\npip install -r requirements.txt\\n```\\n\\n2. **Configure environment variables**\\nCreate a `.env` file in the root directory:\\n```bash\\n# AI Service APIs\\nOPENAI_API_KEY=your_openai_api_key\\nGOOGLE_API_KEY=your_google_gemini_api_key\\nTAVILY_API_KEY=your_tavily_api_key\\n\\n# Gmail Integration (Optional)\\nGMAIL_CREDENTIALS_FILE=credentials.json\\nGMAIL_TOKEN_FILE=token.json\\nRECIPIENT_EMAIL=recipient@example.com\\n```\\n\\n3. **Gmail Setup (Optional)**\\nFor email functionality:\\n- Enable Gmail API in [Google Cloud Console](https://console.cloud.google.com/)\\n- Create OAuth2 credentials and download as `credentials.json`\\n- Place the file in the project root\\n- First run will generate `token.json` automatically\\n\\n## 💻 Usage\\n\\n### Option 1: Web Interface (Recommended)\\nLaunch the interactive Chainlit web interface:\\n\\n```bash\\nchainlit run app.py\\n```\\n\\n![Chainlit Interface](images/ChainLit-user%20Interface.png)\\n\\nThe web interface provides:\\n- 🎯 Guided workflow with clear instructions\\n- 📱 Real-time feedback and refinement\\n- 🖼️ Visual preview of generated content\\n- ✅ Easy approval and sending process\\n\\n### Option 2: Command Line Interface\\nFor quick automation or scripting:\\n\\n```bash\\npython main.py \"Create a post about the importance of drinking water\"\\n```\\n\\n### Option 3: LangGraph Studio\\nFor workflow development and debugging:\\n\\n![LangGraph Studio](images/LangGraphStudio-workflow-diagram.png)\\n\\n```bash\\ncd studio\\nlanggraph dev\\n```\\n\\n### Option 4: n8n Integration\\nFor no-code workflow automation:\\n\\n![n8n Workflow](images/n8n-workflow-diagram.png)\\n\\nImport the workflow from `n8n/main (NO img2img).json` into your n8n instance.',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 2,\n",
       "   'is_chunked': True,\n",
       "   'language': 'markdown',\n",
       "   'chunk_size_actual': 3627},\n",
       "  {'path': 'README.md#chunk_1',\n",
       "   'original_path': 'README.md',\n",
       "   'content': \"### Option 3: LangGraph Studio\\nFor workflow development and debugging:\\n\\n![LangGraph Studio](images/LangGraphStudio-workflow-diagram.png)\\n\\n```bash\\ncd studio\\nlanggraph dev\\n```\\n\\n### Option 4: n8n Integration\\nFor no-code workflow automation:\\n\\n![n8n Workflow](images/n8n-workflow-diagram.png)\\n\\nImport the workflow from `n8n/main (NO img2img).json` into your n8n instance.\\n\\n## 🔄 Workflow Process\\n\\nThe LinkedIn Booster follows an intelligent multi-step process:\\n\\n1. **📝 Content Creation**: AI generates LinkedIn post based on your topic\\n2. **🎨 Image Generation**: Creates matching visual content using DALL·E 3\\n3. **👀 Review & Feedback**: Present content for user review\\n4. **🤖 Smart Classification**: AI analyzes feedback and determines next action\\n5. **✨ Refinement**: Improves text or image based on feedback\\n6. **✅ Approval & Delivery**: Sends final content via email when approved\\n\\nThe workflow uses LangGraph's stateful execution with strategic interrupts, allowing for human-in-the-loop refinement at key decision points.\\n\\n## 🛠️ Development\\n\\n### Running Tests\\n```bash\\npython test_imports.py\\npython test_create_post.py\\npython send_mail_test.py\\n```\\n\\n### Jupyter Notebook\\nExplore the original development process:\\n```bash\\njupyter notebook notebooks/booster_workflow_v0.2.ipynb\\n```\\n\\n## 📋 Requirements\\n\\nKey dependencies include:\\n- `langchain` - LLM framework and integrations\\n- `langgraph` - Workflow orchestration\\n- `chainlit` - Web interface\\n- `google-api-python-client` - Gmail integration\\n- `openai` - DALL·E 3 image generation\\n- `tavily-python` - Web search capabilities\\n\\nSee `requirements.txt` for complete list.\\n\\n## 🤝 Contributing\\n\\n1. Fork the repository\\n2. Create a feature branch\\n3. Make your changes\\n4. Add tests if applicable\\n5. Submit a pull request\\n\\n## 📄 License\\n\\nMIT License - see LICENSE file for details.\",\n",
       "   'chunk_index': 1,\n",
       "   'total_chunks': 2,\n",
       "   'is_chunked': True,\n",
       "   'language': 'markdown',\n",
       "   'chunk_size_actual': 1815},\n",
       "  {'path': 'app.py#chunk_0',\n",
       "   'original_path': 'app.py',\n",
       "   'content': '\"\"\"Chainlit GUI for LinkedIn Booster workflow.\"\"\"\\nimport chainlit as cl\\nfrom typing import Dict, Any\\nimport asyncio\\nfrom utils import load_env\\nfrom workflow.graph import start_workflow, continue_workflow\\n\\n\\n# Load environment variables\\nload_env()\\n\\n# Store user sessions\\nuser_sessions: Dict[str, Dict[str, Any]] = {}\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    \"\"\"Initialize the chat session.\"\"\"\\n    session_id = cl.user_session.get(\"id\")\\n    user_sessions[session_id] = {\\n        \"thread_id\": f\"chainlit_{session_id}\",\\n        \"current_state\": None,\\n        \"step\": \"initial\"\\n    }\\n    \\n    await cl.Message(\\n        content=\"🚀 **Welcome to LinkedIn Booster!**\\\\n\\\\n\"\\n                \"I\\'ll help you create engaging LinkedIn posts with AI-generated images.\\\\n\\\\n\"\\n                \"**How it works:**\\\\n\"\\n                \"1. Tell me your post topic or idea\\\\n\"\\n                \"2. I\\'ll generate a professional post and matching image\\\\n\"\\n                \"3. You can review and refine both text and image\\\\n\"\\n                \"4. Once approved, I\\'ll send it via email\\\\n\\\\n\"\\n                \"**What would you like to post about today?**\"\\n    ).send()\\n\\n\\n@cl.on_message\\nasync def main(message: cl.Message):\\n    \"\"\"Handle user messages.\"\"\"\\n    session_id = cl.user_session.get(\"id\")\\n    session = user_sessions.get(session_id, {})\\n    \\n    user_input = message.content.strip()\\n    \\n    if session.get(\"step\") == \"initial\":\\n        # Start the workflow\\n        await handle_initial_request(user_input, session)\\n    else:\\n        # Continue with feedback\\n        await handle_feedback(user_input, session)\\n\\n\\nasync def handle_initial_request(topic: str, session: Dict[str, Any]):\\n    \"\"\"Handle the initial post creation request.\"\"\"\\n    session_id = cl.user_session.get(\"id\")\\n    thread_id = session[\"thread_id\"]\\n    \\n    # Show loading message\\n    loading_msg = await cl.Message(content=\"🔄 Creating your LinkedIn post...\").send()\\n    \\n    try:\\n        # Start the workflow\\n        state = await asyncio.to_thread(start_workflow, topic, thread_id)\\n        session[\"current_state\"] = state\\n        \\n        if state.error_message:\\n            await cl.Message(content=f\"❌ **Error:** {state.error_message}\").send()\\n            return\\n        \\n        # Show success message\\n        await cl.Message(content=\"✅ **Post Created Successfully!**\").send()\\n        \\n        # Display the generated post\\n        await cl.Message(\\n            content=f\"📝 **Generated LinkedIn Post:**\\\\n\\\\n{state.post_text}\"\\n        ).send()\\n        \\n        # Display the image if available\\n        if state.image_url:\\n            await cl.Message(\\n                content=f\"🖼️ **Generated Image:**\\\\n\\\\n![Generated Image]({state.image_url})\"\\n            ).send()\\n        \\n        # Ask for feedback\\n        session[\"step\"] = \"feedback\"\\n        await cl.Message(\\n            content=\"**What would you like to do next?**\\\\n\\\\n\"\\n                   \"• Type **\\'approve\\'** to send the post via email\\\\n\"\\n                   \"• Type **\\'refine text [your suggestions]\\'** to improve the text\\\\n\"\\n                   \"• Type **\\'refine image [your suggestions]\\'** to improve the image\\\\n\"\\n                   \"• Type **\\'quit\\'** to end the session\"\\n        ).send()\\n        \\n    except Exception as e:\\n        await cl.Message(content=f\"❌ **Error:** {str(e)}\").send()',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 4,\n",
       "   'is_chunked': True,\n",
       "   'language': 'python',\n",
       "   'chunk_size_actual': 3293},\n",
       "  {'path': 'app.py#chunk_1',\n",
       "   'original_path': 'app.py',\n",
       "   'content': 'async def handle_feedback(feedback: str, session: Dict[str, Any]):\\n    \"\"\"Handle user feedback on the generated content.\"\"\"\\n    session_id = cl.user_session.get(\"id\")\\n    thread_id = session[\"thread_id\"]\\n    \\n    if feedback.lower() in {\"quit\", \"exit\", \"q\"}:\\n        await cl.Message(content=\"👋 **Thanks for using LinkedIn Booster!** Have a great day!\").send()\\n        session[\"step\"] = \"completed\"\\n        return\\n    \\n    # Show processing message\\n    await cl.Message(content=\"🔄 Processing your feedback...\").send()\\n    \\n    try:\\n        # Continue the workflow with feedback\\n        state = await asyncio.to_thread(continue_workflow, feedback, thread_id)\\n        session[\"current_state\"] = state\\n        \\n        if state.error_message:\\n            await cl.Message(content=f\"❌ **Error:** {state.error_message}\").send()\\n            return\\n        \\n        # Handle different workflow states\\n        if state.current_step == \"completed\":\\n            await cl.Message(content=\"✅ **Email sent successfully!**\").send()\\n            await cl.Message(\\n                content=\"🎉 **Workflow Completed!**\\\\n\\\\n\"\\n                       \"Your LinkedIn post has been sent via email. \"\\n                       \"You can start a new post by refreshing the page.\"\\n            ).send()\\n            session[\"step\"] = \"completed\"\\n            \\n        elif state.current_step == \"post_content\" and state.classification == \"Approved\":\\n            # This means the post was approved, trigger email sending\\n            await cl.Message(content=\"✅ **Post approved! Sending email...**\").send()\\n            # The workflow should automatically proceed to send_email\\n            \\n        elif state.current_step == \"terminate\":\\n            await cl.Message(content=\"🛑 **Workflow terminated.**\").send()\\n            await cl.Message(\\n                content=\"The workflow has been terminated. \"\\n                       \"You can start a new post by refreshing the page.\"\\n            ).send()\\n            session[\"step\"] = \"completed\"\\n            \\n        elif state.current_step == \"wait_feedback\":\\n            await cl.Message(content=\"✅ **Content updated!**\").send()\\n            \\n            # Always show the current content after updates\\n            if state.post_text:\\n                await cl.Message(\\n                    content=f\"📝 **Updated LinkedIn Post:**\\\\n\\\\n{state.post_text}\"\\n                ).send()\\n            \\n            if state.image_url:\\n                await cl.Message(\\n                    content=f\"🖼️ **Updated Image:**\\\\n\\\\n![Updated Image]({state.image_url})\"\\n                ).send()\\n            \\n            # Ask for next action\\n            await cl.Message(\\n                content=\"**What would you like to do next?**\\\\n\\\\n\"\\n                       \"• Type **\\'approve\\'** to send the post via email\\\\n\"\\n                       \"• Type **\\'refine text [your suggestions]\\'** to improve the text further\\\\n\"\\n                       \"• Type **\\'refine image [your suggestions]\\'** to improve the image further\\\\n\"\\n                       \"• Type **\\'quit\\'** to end the session\"\\n            ).send()\\n        \\n        else:\\n            # Handle any other states or show current content\\n            await cl.Message(content=f\"**Current Status:** {state.current_step}\").send()\\n            if state.classification:\\n                await cl.Message(content=f\"**Classification:** {state.classification}\").send()\\n            \\n            # Show current content\\n            if state.post_text:\\n                await cl.Message(\\n                    content=f\"📝 **Current LinkedIn Post:**\\\\n\\\\n{state.post_text}\"\\n                ).send()\\n            \\n            if state.image_url:\\n                await cl.Message(\\n                    content=f\"🖼️ **Current Image:**\\\\n\\\\n![Current Image]({state.image_url})\"\\n                ).send()\\n            \\n            # Ask for next action\\n            await cl.Message(\\n                content=\"**What would you like to do next?**\\\\n\\\\n\"',\n",
       "   'chunk_index': 1,\n",
       "   'total_chunks': 4,\n",
       "   'is_chunked': True,\n",
       "   'language': 'python',\n",
       "   'chunk_size_actual': 3943},\n",
       "  {'path': 'app.py#chunk_2',\n",
       "   'original_path': 'app.py',\n",
       "   'content': ').send()\\n            \\n            if state.image_url:\\n                await cl.Message(\\n                    content=f\"🖼️ **Current Image:**\\\\n\\\\n![Current Image]({state.image_url})\"\\n                ).send()\\n            \\n            # Ask for next action\\n            await cl.Message(\\n                content=\"**What would you like to do next?**\\\\n\\\\n\"\\n                       \"• Type **\\'approve\\'** to send the post via email\\\\n\"\\n                       \"• Type **\\'refine text [your suggestions]\\'** to improve the text\\\\n\"\\n                       \"• Type **\\'refine image [your suggestions]\\'** to improve the image\\\\n\"\\n                       \"• Type **\\'quit\\'** to end the session\"\\n            ).send()\\n        \\n    except Exception as e:\\n        await cl.Message(content=f\"❌ **Error:** {str(e)}\").send()',\n",
       "   'chunk_index': 2,\n",
       "   'total_chunks': 4,\n",
       "   'is_chunked': True,\n",
       "   'language': 'python',\n",
       "   'chunk_size_actual': 791},\n",
       "  {'path': 'app.py#chunk_3',\n",
       "   'original_path': 'app.py',\n",
       "   'content': '@cl.on_chat_end\\nasync def end():\\n    \"\"\"Clean up when chat ends.\"\"\"\\n    session_id = cl.user_session.get(\"id\")\\n    if session_id in user_sessions:\\n        del user_sessions[session_id]',\n",
       "   'chunk_index': 3,\n",
       "   'total_chunks': 4,\n",
       "   'is_chunked': True,\n",
       "   'language': 'python',\n",
       "   'chunk_size_actual': 184},\n",
       "  {'path': 'main.py',\n",
       "   'content': 'import sys\\nfrom utils import load_env\\nfrom workflow.graph import start_workflow, continue_workflow\\n\\n\\ndef main() -> None:\\n    load_env()\\n    if len(sys.argv) < 2:\\n        print(\"Usage: python main.py \\\\\"Your post topic...\\\\\"\")\\n        sys.exit(1)\\n    topic = sys.argv[1]\\n    thread_id = \"cli\"\\n    state = start_workflow(topic, thread_id)\\n    if state.current_step != \"wait_feedback\":\\n        print(f\"Error: {state.error_message}\")\\n        sys.exit(1)\\n    print(\"\\\\nGenerated Post:\\\\n\")\\n    print(state.post_text)\\n    print(\"\\\\nImage URL:\\\\n\")\\n    print(state.image_url)\\n    while True:\\n        feedback = input(\"\\\\nFeedback (approve/refine text .../refine image .../quit): \").strip()\\n        if feedback.lower() in {\"quit\", \"exit\", \"q\"}:\\n            print(\"Goodbye!\")\\n            break\\n        state = continue_workflow(feedback, thread_id)\\n        if state.current_step == \"completed\":\\n            print(\"Email sent. Workflow completed.\")\\n            break\\n        if state.current_step == \"terminate\":\\n            print(\"Workflow terminated.\")\\n            break\\n        if state.current_step == \"wait_feedback\":\\n            if state.classification == \"Refine Text\":\\n                print(\"\\\\nUpdated Post:\\\\n\")\\n                print(state.post_text)\\n            elif state.classification == \"Refine Image\":\\n                print(\"\\\\nUpdated Image URL:\\\\n\")\\n                print(state.image_url)\\n\\n\\nif __name__ == \"__main__\":\\n    main()',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'python'},\n",
       "  {'path': 'requirements.txt',\n",
       "   'content': 'chainlit>=1.0.0\\nlangchain>=0.1.0\\nlangchain-community>=0.1.0\\nlangchain-google-genai>=1.0.0\\nlangchain-google-community>=0.1.0\\nlanggraph>=0.0.40\\nrequests>=2.31.0\\ntyping-extensions>=4.5.0\\npython-dotenv>=1.0.0\\ngoogle-auth>=2.0.0\\ngoogle-auth-oauthlib>=1.0.0\\ngoogle-auth-httplib2>=0.2.0\\ngoogle-api-python-client>=2.0.0\\nopenai>=1.0.0\\ntavily-python>=0.3.0',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'unknown'},\n",
       "  {'path': 'n8n/main (NO img2img).json',\n",
       "   'content': '{\\n  \"name\": \"main (NO img2img)\",\\n  \"nodes\": [\\n    {\\n      \"parameters\": {\\n        \"options\": {}\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatGoogleGemini\",\\n      \"typeVersion\": 1,\\n      \"position\": [\\n        832,\\n        352\\n      ],\\n      \"id\": \"575c62c0-07d4-4c73-9d70-4fdafe125e86\",\\n      \"name\": \"Google Gemini Chat Model\",\\n      \"credentials\": {\\n        \"googlePalmApi\": {\\n          \"id\": \"mP5d64x2rkeTRzbi\",\\n          \"name\": \"Google Gemini(PaLM) Api account 2\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"toolDescription\": \"Web search tool.\",\\n        \"method\": \"POST\",\\n        \"url\": \"https://api.tavily.com/search\",\\n        \"sendHeaders\": true,\\n        \"headerParameters\": {\\n          \"parameters\": [\\n            {\\n              \"name\": \"Authorization\",\\n              \"value\": \"Bearer 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\"\\n            }\\n          ]\\n        },\\n        \"sendBody\": true,\\n        \"bodyParameters\": {\\n          \"parameters\": [\\n            {\\n              \"name\": \"query\",\\n              \"value\": \"={{ /*n8n-auto-generated-fromAI-override*/ $fromAI(\\'parameters0_Value\\', ``, \\'string\\') }}\"\\n            }\\n          ]\\n        },\\n        \"options\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.httpRequestTool\",\\n      \"typeVersion\": 4.2,\\n      \"position\": [\\n        -496,\\n        -832\\n      ],\\n      \"id\": \"f6bfb373-1e88-448b-8dfd-da7c90016a8d\",\\n      \"name\": \"Tavily\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"inputText\": \"={{ $json.data.text }}\",\\n        \"categories\": {\\n          \"categories\": [\\n            {\\n              \"category\": \"Approved\",\\n              \"description\": \"Approval message refers that its ok to proceed with the current status.\"\\n            },\\n            {\\n              \"category\": \"Refine Text\",\\n              \"description\": \"Request to Refine the post text\"\\n            },\\n            {\\n              \"category\": \"Terminate\",\\n              \"description\": \"Termination request for the process.\"\\n            },\\n            {\\n              \"category\": \"Refine Image\",\\n              \"description\": \"Request to Refine the post Image\"\\n            }\\n          ]\\n        },\\n        \"options\": {}\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.textClassifier\",\\n      \"typeVersion\": 1.1,\\n      \"position\": [\\n        624,\\n        -464\\n      ],\\n      \"id\": \"abf5e791-c5f8-4b32-8a4b-58804e853434\",\\n      \"name\": \"Text Classifier\"\\n    },\\n    {\\n      \"parameters\": {},\\n      \"type\": \"n8n-nodes-base.noOp\",\\n      \"typeVersion\": 1,\\n      \"position\": [\\n        1296,\\n        64\\n      ],\\n      \"id\": \"04ec9776-b607-4c47-8140-bfeb62086c9c\",\\n      \"name\": \"No Operation, do nothing\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"authentication\": \"communityManagement\",\\n        \"person\": \"=Mohamed Mowina\",\\n        \"text\": \"={{ $(\\'Post writer\\').item.json.output }}\",\\n        \"shareMediaCategory\": \"IMAGE\",\\n        \"additionalFields\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.linkedIn\",\\n      \"typeVersion\": 1,\\n      \"position\": [\\n        1616,\\n        -1008\\n      ],\\n      \"id\": \"c56079bb-4e3e-471b-819c-3a6f73f2496b\",\\n      \"name\": \"Create a post\",\\n      \"disabled\": true\\n    },\\n    {\\n      \"parameters\": {\\n        \"updates\": [\\n          \"message\"\\n        ],\\n        \"additionalFields\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.telegramTrigger\",\\n      \"typeVersion\": 1.2,\\n      \"position\": [\\n        -1168,\\n        -1008\\n      ],\\n      \"id\": \"c6d21d0a-52b5-405d-b16d-e7568dd8e839\",\\n      \"name\": \"Telegram Trigger\",\\n      \"webhookId\": \"931413d8-41c9-43f0-85bc-1b1b1fb80fa4\",\\n      \"credentials\": {\\n        \"telegramApi\": {\\n          \"id\": \"EFoDusKQAfWdyYaO\",\\n          \"name\": \"Telegram account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"promptType\": \"define\",\\n        \"text\": \"={{ $json.message.text }}\",\\n        \"options\": {\\n          \"systemMessage\": \"=Your tone should be:\\\\n\\\\nProfessional yet approachable: Confident and authoritative, but also conversational and relatable.\\\\n\\\\nConcise and clear: Get straight to the point while using compelling language.\\\\n\\\\nAction-oriented: Focus on providing value and prompting engagement.\\\\n\\\\nCore Functionality\\\\nYou will perform the following tasks in a structured workflow:\\\\n\\\\nInput Analysis & Goal Clarification:\\\\n\\\\nUpon receiving a request, first identify the core topic and the user\\'s primary goal (e.g., generate leads, build thought leadership, share a company update, get job offers).\\\\n\\\\nIdentify the target audience and their pain points or interests.\\\\n\\\\nRecognize any specific constraints (e.g., character limit, inclusion of a specific link, mention of a name).\\\\n\\\\nStrategy & Brainstorming:\\\\n\\\\nBased on your analysis, brainstorm a minimum of three potential angles or hooks for the post. Each hook should be designed to capture attention and align with the user\\'s goal.\\\\n\\\\nConsider different post formats: a personal story, a listicle, a thought-provoking question, a concise tip, a problem/solution narrative.\\\\n\\\\nDetermine the most effective approach for the given topic and goal.\\\\n\\\\nDraft Generation:\\\\n\\\\nWrite a draft of the post using the chosen angle. Follow these rules for the highest quality:\\\\n\\\\nHook: Start with a strong, attention-grabbing first sentence.\\\\n\\\\nBody: Break up text into short, single-sentence paragraphs. Use line breaks frequently for scannability.\\\\n\\\\nElegance: Incorporate relevant emojis to break up text and add personality, but use them sparingly and strategically (1-3 emojis is usually sufficient).\\\\n\\\\nValue: Provide clear, actionable value, a unique perspective, or a compelling story.\\\\n\\\\nCall-to-Action (CTA): End with a clear, concise CTA that encourages engagement (e.g., \\\\\"What are your thoughts?\\\\\", \\\\\"Share your experience below!\\\\\", \\\\\"Comment \\'Yes\\' if you agree.\\\\\").\\\\n\\\\nHashtags: Suggest 3-5 relevant, specific, and popular hashtags at the end of the post.\\\\n\\\\nOutput:\\\\n\\\\nDon\\'t forget you Output only the post, Do not ask flowup questions and don\\'t provide other suggestions.\\\\n\"\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\\n      \"typeVersion\": 2.2,\\n      \"position\": [\\n        -848,\\n        -1008\\n      ],\\n      \"id\": \"2cac353e-25c0-4125-8b49-c13448885f92\",\\n      \"name\": \"Post writer\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"promptType\": \"define\",\\n        \"text\": \"=Original post:  {{ $(\\'Send a photo message\\').item.json.result.caption }}\\\\n\\\\nRequest: {{ $json.data.text }}\",\\n        \"options\": {\\n          \"systemMessage\": \"You are a helpful assistant who improve and refine Linkedin post.\\\\n\\\\nOutput only the post text.\"\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\\n      \"typeVersion\": 2.2,\\n      \"position\": [\\n        1552,\\n        -368\\n      ],\\n      \"id\": \"49ee2b7d-5cac-4a5d-9696-8d835045a536\",\\n      \"name\": \"Post refiner\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"operation\": \"sendAndWait\",\\n        \"chatId\": \"={{ $(\\'Telegram Trigger\\').item.json.message.chat.id }}\",\\n        \"message\": \"=Your LinkedIn post draft is ready (text + image).\\\\nYou can now decide what to do next. Simply type your instruction in plain language, and the system will handle it.\\\\nExamples:\\\\n\\\\n‘Looks good, post it.’ → Post\\\\n\\\\n‘Change the text, it feels too formal.’ → Edit text\\\\n\\\\n‘The image should focus more on teamwork.’ → Edit image\\\\n\\\\n‘Cancel this, I don’t want to continue.’ → Terminate\\\\n\\\\n👉 You are free to write anything — the system will classify your input and act accordingly.\",\\n        \"responseType\": \"freeText\",\\n        \"options\": {\\n          \"appendAttribution\": false\\n        }\\n      },\\n      \"type\": \"n8n-nodes-base.telegram\",\\n      \"typeVersion\": 1.2,\\n      \"position\": [\\n        320,\\n        112\\n      ],\\n      \"id\": \"07eb16f1-1579-41e4-8443-c2ef5445eb45\",\\n      \"name\": \"Wait for feedback\",\\n      \"webhookId\": \"a39f1d60-d759-43ca-b5b1-9e0b565f9e8a\",\\n      \"credentials\": {\\n        \"telegramApi\": {\\n          \"id\": \"EFoDusKQAfWdyYaO\",\\n          \"name\": \"Telegram account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Create post\\'s text\",\\n        \"height\": 448,\\n        \"width\": 640,\\n        \"color\": 7\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        -976,\\n        -1104\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"22b6a2cf-2715-479b-81d5-ce549dc5c223\",\\n      \"name\": \"Sticky Note\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Create post\\'s Image\",\\n        \"height\": 448,\\n        \"width\": 640,\\n        \"color\": 7\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        -752,\\n        -576\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"9ece431d-1cba-486c-abff-28e2d035c987\",\\n      \"name\": \"Sticky Note1\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Feedback\",\\n        \"height\": 448,\\n        \"width\": 816\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        -352,\\n        -48\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"3dd8f3c0-3ebf-45b4-ad4d-c2ce905863dd\",\\n      \"name\": \"Sticky Note2\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Terminate operation\",\\n        \"height\": 352,\\n        \"width\": 368,\\n        \"color\": 3\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1152,\\n        -96\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"f507679e-8023-4ee4-9a91-f067273627c9\",\\n      \"name\": \"Sticky Note3\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Post\",\\n        \"height\": 272,\\n        \"width\": 336,\\n        \"color\": 4\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1152,\\n        -1088\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"b9503555-ca57-49dc-8d12-d9ec43450bd4\",\\n      \"name\": \"Sticky Note4\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Refine post\",\\n        \"height\": 688,\\n        \"width\": 848,\\n        \"color\": 5\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1152,\\n        -800\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"4597f92e-43d9-4e6f-9aa9-10036656f7e5\",\\n      \"name\": \"Sticky Note5\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"## Refine Image\",\\n        \"height\": 272,\\n        \"width\": 576,\\n        \"color\": 6\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1376,\\n        -784\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"e0a0047b-199a-4590-9c25-155788058cb5\",\\n      \"name\": \"Sticky Note6\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"## Refine Text\",\\n        \"height\": 256,\\n        \"width\": 368,\\n        \"color\": 6\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1472,\\n        -432\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"c603d205-3fe1-409d-9ef4-aa4b4b44f86e\",\\n      \"name\": \"Sticky Note7\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"sendTo\": \"mohammed-mowina@outlook.com\",\\n        \"subject\": \"LinkedIn Booster\",\\n        \"emailType\": \"text\",\\n        \"message\": \"={{ $(\\'Merge\\').item.json.output }}\",\\n        \"options\": {\\n          \"appendAttribution\": false,\\n          \"attachmentsUi\": {\\n            \"attachmentsBinary\": [\\n              {\\n                \"property\": \"={{ $(\\'Merge\\').item.json.fileName }}\"\\n              }\\n            ]\\n          }\\n        }\\n      },\\n      \"type\": \"n8n-nodes-base.gmail\",\\n      \"typeVersion\": 2.1,\\n      \"position\": [\\n        1360,\\n        -992\\n      ],\\n      \"id\": \"b9b5dfed-369a-4e5d-86e5-8e8324b820a6\",\\n      \"name\": \"Send a message\",\\n      \"webhookId\": \"9e818c1d-3573-4592-8465-6953a6754cef\",\\n      \"credentials\": {\\n        \"gmailOAuth2\": {\\n          \"id\": \"Z3qIvY9PLNzWAis2\",\\n          \"name\": \"Gmail account 2\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"operation\": \"sendPhoto\",\\n        \"chatId\": \"={{ $(\\'Telegram Trigger\\').item.json.message.chat.id }}\",\\n        \"binaryData\": true,\\n        \"additionalFields\": {\\n          \"caption\": \"={{ $(\\'Post\\').item.json.output }}\"\\n        }\\n      },\\n      \"type\": \"n8n-nodes-base.telegram\",\\n      \"typeVersion\": 1.2,\\n      \"position\": [\\n        96,\\n        112\\n      ],\\n      \"id\": \"f6891562-43fb-4bb4-9d63-4494ef2e8b51\",\\n      \"name\": \"Send a photo message\",\\n      \"webhookId\": \"94294c72-0650-4b8b-8444-c6ec80b12d2a\",\\n      \"credentials\": {\\n        \"telegramApi\": {\\n          \"id\": \"EFoDusKQAfWdyYaO\",\\n          \"name\": \"Telegram account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"assignments\": {\\n          \"assignments\": [\\n            {\\n              \"id\": \"ef82052d-4309-4359-aa18-b6f2c8313036\",\\n              \"name\": \"output\",\\n              \"value\": \"={{ $json.output }}\",\\n              \"type\": \"string\"\\n            }\\n          ]\\n        },\\n        \"options\": {\\n          \"ignoreConversionErrors\": true,\\n          \"dotNotation\": true\\n        }\\n      },\\n      \"type\": \"n8n-nodes-base.set\",\\n      \"typeVersion\": 3.4,\\n      \"position\": [\\n        -304,\\n        112\\n      ],\\n      \"id\": \"f2baa1ec-2bcb-466e-881b-eb8f8b926d6e\",\\n      \"name\": \"Post\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"promptType\": \"define\",\\n        \"text\": \"=Old Prompt: {{ $(\\'AI Prompt Writer\\').item.json.output }}\\\\nUser comment: {{ $(\\'Wait for feedback\\').item.json.data.text }}\",\\n        \"options\": {\\n          \"systemMessage\": \"=**Overview**:\\\\nYou are an expert LinkedIn Image Prompt Engineer. For each request, generate AI image prompts following this structure. You work in a workflow and your responsability is to refine an already generated image by making a modified propmt to the Image Generator Model.\\\\n\\\\n*Input*:\\\\n- The old prompt for the generated Image.\\\\n- The user comment on the Image to change it.\\\\n\\\\n*Instructions*:\\\\nA modern, minimalistic professional LinkedIn illustration about the topic\\\\nAvoid text images just some\\\\n\\\\n*Style*:\\\\n3D Object Generator with clean flat vector, LinkedIn color palette (blue, white, grey), simple, corporate-appropriate. \\\\nAvoid clutter, make it inspiring and easy to understand.\\\\n\\\\n*Output*:\\\\nOnly output the Prompt no other suggestions or flowup questions.\"\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\\n      \"typeVersion\": 2.2,\\n      \"position\": [\\n        1456,\\n        -704\\n      ],\\n      \"id\": \"b464c76d-0cde-41f1-ab56-7e3f4690bdd7\",\\n      \"name\": \"AI Prompt Refiner\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"promptType\": \"define\",\\n        \"text\": \"={{ $json.output }}\",\\n        \"options\": {\\n          \"systemMessage\": \"=**Overview**:\\\\nYou are an expert LinkedIn Image Prompt Engineer. For each request, generate AI image prompts following this structure.\\\\n\\\\n*Input*:\\\\nYou will have the post\\'s text as the user Prompt.\\\\n\\\\n*Instructions*:\\\\nA modern, minimalistic professional LinkedIn illustration about the topic\\\\nAvoid text images just some\\\\n\\\\n*Style*:\\\\n3D Object Generator with clean flat vector, LinkedIn color palette (blue, white, grey), simple, corporate-appropriate. \\\\nAvoid clutter, make it inspiring and easy to understand.\\\\n\\\\n*Output*:\\\\nOnly output the Prompt no other suggestions or flowup questions.\"\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\\n      \"typeVersion\": 2.2,\\n      \"position\": [\\n        -656,\\n        -480\\n      ],\\n      \"id\": \"df8b4dfb-b2b9-4e65-bf2d-108a6fb99294\",\\n      \"name\": \"AI Prompt Writer\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"resource\": \"image\",\\n        \"prompt\": \"={{ $json.output }}\",\\n        \"options\": {\\n          \"returnImageUrls\": false\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.openAi\",\\n      \"typeVersion\": 1.8,\\n      \"position\": [\\n        -352,\\n        -480\\n      ],\\n      \"id\": \"85253bd1-a207-42be-8510-bec86925c7d9\",\\n      \"name\": \"OpenAI Post Image Generation\",\\n      \"credentials\": {\\n        \"openAiApi\": {\\n          \"id\": \"WSrQfOLqmZvD07lC\",\\n          \"name\": \"OpenAi account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"resource\": \"image\",\\n        \"prompt\": \"={{ $json.output }}\",\\n        \"options\": {\\n          \"returnImageUrls\": false\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.openAi\",\\n      \"typeVersion\": 1.8,\\n      \"position\": [\\n        1760,\\n        -704\\n      ],\\n      \"id\": \"a9bb6412-f5a5-4514-881f-20f2f19643e8\",\\n      \"name\": \"Regenerate an image\",\\n      \"credentials\": {\\n        \"openAiApi\": {\\n          \"id\": \"WSrQfOLqmZvD07lC\",\\n          \"name\": \"OpenAi account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"mode\": \"combine\",\\n        \"combineBy\": \"combineByPosition\",\\n        \"options\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.merge\",\\n      \"typeVersion\": 3.2,\\n      \"position\": [\\n        -96,\\n        112\\n      ],\\n      \"id\": \"33ab8e06-59b3-4428-9fb6-d376bd565ace\",\\n      \"name\": \"Merge\",\\n      \"retryOnFail\": false\\n    },\\n    {\\n      \"parameters\": {\\n        \"mode\": \"combine\",\\n        \"combineBy\": \"combineByPosition\",\\n        \"options\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.merge\",\\n      \"typeVersion\": 3.2,\\n      \"position\": [\\n        1200,\\n        -992\\n      ],\\n      \"id\": \"9e582664-2a82-469a-b642-20bdf2d52f01\",\\n      \"name\": \"Merge1\",\\n      \"retryOnFail\": false\\n    }\\n  ],\\n  \"pinData\": {},\\n  \"connections\": {\\n    \"Google Gemini Chat Model\": {\\n      \"ai_languageModel\": [\\n        [\\n          {\\n            \"node\": \"Post writer\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"Text Classifier\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"Post refiner\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"AI Prompt Writer\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"AI Prompt Refiner\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Tavily\": {\\n      \"ai_tool\": [\\n        [\\n          {\\n            \"node\": \"Post writer\",\\n            \"type\": \"ai_tool\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Text Classifier\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Merge1\",\\n            \"type\": \"main\",\\n            \"index\": 1\\n          }\\n        ],\\n        [\\n          {\\n            \"node\": \"Post refiner\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ],\\n        [\\n          {\\n            \"node\": \"No Operation, do nothing\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ],\\n        [\\n          {\\n            \"node\": \"AI Prompt Refiner\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Telegram Trigger\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Post writer\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Post writer\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"AI Prompt Writer\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"Post\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Post refiner\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Post\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Wait for feedback\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Text Classifier\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Create a post\": {\\n      \"main\": [\\n        []\\n      ]\\n    },\\n    \"Send a photo message\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Wait for feedback\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Post\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Merge\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"AI Prompt Refiner\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Regenerate an image\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"AI Prompt Writer\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"OpenAI Post Image Generation\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"OpenAI Post Image Generation\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Merge\",\\n            \"type\": \"main\",\\n            \"index\": 1\\n          }\\n        ]\\n      ]\\n    },\\n    \"Regenerate an image\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Merge\",\\n            \"type\": \"main\",\\n            \"index\": 1\\n          }\\n        ]\\n      ]\\n    },\\n    \"Merge\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Send a photo message\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"Merge1\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Merge1\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Send a message\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    }\\n  },\\n  \"active\": false,\\n  \"settings\": {\\n    \"executionOrder\": \"v1\"\\n  },\\n  \"versionId\": \"d01124c9-2c7a-40fc-ac9e-7edaa4535fc4\",\\n  \"meta\": {\\n    \"templateCredsSetupCompleted\": true,\\n    \"instanceId\": \"e42899c7dfda6042a4f9f2f2b7e15acf58d2ab8ee4dfbe7dec48ee2a3be4e756\"\\n  },\\n  \"id\": \"dlis4ObYbraVh6Bt\",\\n  \"tags\": []\\n}',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'unknown'},\n",
       "  {'path': 'studio/__init__.py',\n",
       "   'content': '# This makes studio a proper Python package',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'python'},\n",
       "  {'path': 'studio/graph.py',\n",
       "   'content': '\"\"\"Standalone graph file for LangGraph Studio.\"\"\"\\nimport sys\\nimport os\\nfrom typing import Optional\\nfrom dataclasses import dataclass\\n\\n# Add the parent directory to the path so we can import from workflow\\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\\n\\nfrom langgraph.graph import StateGraph, END\\n\\n# Define state directly to avoid import conflicts\\n@dataclass\\nclass WorkflowState:\\n    \"\"\"State management for the LinkedIn post creation workflow.\"\"\"\\n    user_input: str = \"\"\\n    post_text: str = \"\"\\n    image_prompt: str = \"\"\\n    image_data: Optional[bytes] = None\\n    image_url: str = \"\"\\n    user_feedback: str = \"\"\\n    classification: str = \"\"\\n    refined_text: str = \"\"\\n    refined_image_data: Optional[bytes] = None\\n    current_step: str = \"start\"\\n    error_message: str = \"\"\\n    final_post_ready: bool = False\\n\\n# Import nodes after defining state\\ntry:\\n    from workflow.nodes import (\\n        create_post_text,\\n        generate_image,\\n        classify_feedback,\\n        refine_text,\\n        refine_image,\\n        send_email,\\n    )\\nexcept ImportError as e:\\n    # Fallback dummy functions for studio\\n    def create_post_text(state: WorkflowState) -> WorkflowState:\\n        state.post_text = \"Sample LinkedIn post created\"\\n        return state\\n    \\n    def generate_image(state: WorkflowState) -> WorkflowState:\\n        state.image_url = \"https://example.com/image.jpg\"\\n        return state\\n    \\n    def classify_feedback(state: WorkflowState) -> WorkflowState:\\n        state.classification = \"approved\"\\n        return state\\n    \\n    def refine_text(state: WorkflowState) -> WorkflowState:\\n        state.refined_text = \"Refined post text\"\\n        return state\\n    \\n    def refine_image(state: WorkflowState) -> WorkflowState:\\n        state.refined_image_data = b\"refined_image_data\"\\n        return state\\n    \\n    def send_email(state: WorkflowState) -> WorkflowState:\\n        state.final_post_ready = True\\n        return state\\n\\n\\ndef route_feedback(state: WorkflowState) -> str:\\n    \"\"\"Route based on feedback classification.\"\"\"\\n    classification = state.classification.lower()\\n    if \"approved\" in classification:\\n        return \"approved\"\\n    if \"refine text\" in classification or \"refine_text\" in classification:\\n        return \"refine_text\"\\n    if \"refine image\" in classification or \"refine_image\" in classification:\\n        return \"refine_image\"\\n    if \"terminate\" in classification:\\n        return \"terminate\"\\n    return \"terminate\"\\n\\n\\ndef create_studio_workflow():\\n    \"\"\"Create the LangGraph workflow for Studio (without checkpointer).\"\"\"\\n    workflow = StateGraph(WorkflowState)\\n    workflow.add_node(\"create_post\", create_post_text)\\n    workflow.add_node(\"generate_image\", generate_image)\\n    workflow.add_node(\"classify_feedback\", classify_feedback)\\n    workflow.add_node(\"refine_text\", refine_text)\\n    workflow.add_node(\"refine_image\", refine_image)\\n    workflow.add_node(\"send_email\", send_email)\\n    workflow.set_entry_point(\"create_post\")\\n    workflow.add_edge(\"create_post\", \"generate_image\")\\n    workflow.add_edge(\"generate_image\", \"classify_feedback\")\\n    workflow.add_conditional_edges(\\n        \"classify_feedback\",\\n        route_feedback,\\n        {\\n            \"approved\": \"send_email\",\\n            \"refine_text\": \"refine_text\",\\n            \"refine_image\": \"refine_image\",\\n            \"terminate\": END,\\n        },\\n    )\\n    workflow.add_edge(\"refine_text\", \"classify_feedback\")\\n    workflow.add_edge(\"refine_image\", \"classify_feedback\")\\n    workflow.add_edge(\"send_email\", END)\\n    \\n    # Compile without checkpointer for LangGraph Studio\\n    app = workflow.compile(\\n        interrupt_after=[\"generate_image\", \"refine_text\", \"refine_image\"],\\n    )\\n    return app\\n\\n\\n# Create the app for LangGraph Studio\\napp = create_studio_workflow()\\n\\n# Export the app for LangGraph Studio\\n__all__ = [\"app\"]',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'python'},\n",
       "  {'path': 'studio/langgraph.json',\n",
       "   'content': '{\\n    \"dockerfile_lines\": [],\\n    \"graphs\": {\\n      \"chatbot\": \"./graph.py:app\"\\n    },\\n    \"env\": \"./.env\",\\n    \"python_version\": \"3.11\",\\n    \"dependencies\": [\\n      \".\"\\n    ]\\n  }',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'unknown'},\n",
       "  {'path': 'utils/config.py',\n",
       "   'content': 'import os\\nfrom dataclasses import dataclass\\nfrom dotenv import load_dotenv\\n\\n\\ndef load_env() -> None:\\n    \"\"\"Load environment variables from a .env file if present.\"\"\"\\n    load_dotenv()\\n\\n\\n@dataclass\\nclass Config:\\n    OPENAI_API_KEY: str | None = os.getenv(\"OPENAI_API_KEY\")\\n    GOOGLE_API_KEY: str | None = os.getenv(\"GOOGLE_API_KEY\")\\n    TAVILY_API_KEY: str | None = os.getenv(\"TAVILY_API_KEY\")\\n    GMAIL_CREDENTIALS_FILE: str = os.getenv(\"GMAIL_CREDENTIALS_FILE\", \"credentials.json\")\\n    GMAIL_TOKEN_FILE: str = os.getenv(\"GMAIL_TOKEN_FILE\", \"token.json\")\\n    RECIPIENT_EMAIL: str = os.getenv(\"RECIPIENT_EMAIL\", \"mohammed-mowina@outlook.com\")\\n\\n\\n',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'python'},\n",
       "  {'path': 'workflow/__init__.py',\n",
       "   'content': '\"\"\"LinkedIn Booster workflow package.\"\"\"\\n\\nfrom .graph import app, create_workflow, start_workflow, continue_workflow\\n\\n__all__ = [\"app\", \"create_workflow\", \"start_workflow\", \"continue_workflow\"]',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'python'},\n",
       "  {'path': 'workflow/graph.py',\n",
       "   'content': 'from langgraph.graph import StateGraph, END\\nfrom langgraph.checkpoint.memory import MemorySaver\\n\\nfrom workflow.state import WorkflowState\\nfrom workflow.nodes import (\\n    create_post_text,\\n    generate_image,\\n    classify_feedback,\\n    refine_text,\\n    refine_image,\\n    send_email,\\n)\\n\\n\\ndef route_feedback(state: WorkflowState) -> str:\\n    \"\"\"Route based on feedback classification.\"\"\"\\n    classification = state.classification.lower()\\n    if \"approved\" in classification:\\n        return \"approved\"\\n    if \"refine text\" in classification or \"refine_text\" in classification:\\n        return \"refine_text\"\\n    if \"refine image\" in classification or \"refine_image\" in classification:\\n        return \"refine_image\"\\n    if \"terminate\" in classification:\\n        return \"terminate\"\\n    return \"terminate\"\\n\\n\\ndef create_workflow():\\n    \"\"\"Create the LangGraph workflow with interrupts configured.\"\"\"\\n    workflow = StateGraph(WorkflowState)\\n    workflow.add_node(\"create_post\", create_post_text)\\n    workflow.add_node(\"generate_image\", generate_image)\\n    workflow.add_node(\"classify_feedback\", classify_feedback)\\n    workflow.add_node(\"refine_text\", refine_text)\\n    workflow.add_node(\"refine_image\", refine_image)\\n    workflow.add_node(\"send_email\", send_email)\\n    workflow.set_entry_point(\"create_post\")\\n    workflow.add_edge(\"create_post\", \"generate_image\")\\n    workflow.add_edge(\"generate_image\", \"classify_feedback\")\\n    workflow.add_conditional_edges(\\n        \"classify_feedback\",\\n        route_feedback,\\n        {\\n            \"approved\": \"send_email\",\\n            \"refine_text\": \"refine_text\",\\n            \"refine_image\": \"refine_image\",\\n            \"terminate\": END,\\n        },\\n    )\\n    workflow.add_edge(\"refine_text\", \"classify_feedback\")\\n    workflow.add_edge(\"refine_image\", \"classify_feedback\")\\n    workflow.add_edge(\"send_email\", END)\\n    memory = MemorySaver()\\n    app = workflow.compile(\\n        checkpointer=memory,\\n        interrupt_after=[\"generate_image\", \"refine_text\", \"refine_image\"],\\n    )\\n    return app\\n\\n\\n# Convenience API mirroring notebook helpers\\napp = create_workflow()\\n\\n\\ndef start_workflow(user_input: str, thread_id: str = \"default\") -> WorkflowState:\\n    initial_state = WorkflowState(user_input=user_input)\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\n    result = app.invoke(initial_state, config)\\n    return WorkflowState(**result)\\n\\n\\ndef continue_workflow(user_feedback: str, thread_id: str = \"default\") -> WorkflowState:\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\n    app.update_state(config, {\"user_feedback\": user_feedback})\\n    result = app.invoke(None, config)\\n    return WorkflowState(**result)\\n\\n\\n',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'python'},\n",
       "  {'path': 'workflow/nodes.py#chunk_0',\n",
       "   'original_path': 'workflow/nodes.py',\n",
       "   'content': 'import os\\nfrom typing import List\\n\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain.schema import SystemMessage, HumanMessage\\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\\nfrom langgraph.errors import GraphRecursionError\\nfrom langgraph.prebuilt import create_react_agent\\n# Gmail imports will be done lazily in send_email function\\n\\nfrom workflow.state import WorkflowState\\nfrom workflow.tools import tavily_tool\\n\\n\\n# Initialize models\\ngemini_model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\\n\\ndef create_post_text(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Generate LinkedIn post text based on user input\"\"\"\\n    system_message = \"\"\"\\n    You are a professional LinkedIn content creator. Create engaging, professional LinkedIn posts.\\n\\n    Your tone should be:\\n    - Professional yet approachable\\n    - Concise and clear\\n    - Action-oriented with clear value\\n\\n    Format your post with:\\n    - A strong hook in the first sentence\\n    - Short, scannable paragraphs\\n    - 1-3 relevant emojis strategically placed\\n    - A clear call-to-action\\n    - 3-5 relevant hashtags at the end\\n\\n    Output ONLY the post text, nothing else.\"\"\"\\n    \\n    try:\\n        max_iterations = 3\\n        recursion_limit = 2 * max_iterations + 1\\n        \\n        agent = create_react_agent(\\n            model=\"openai:gpt-4o-mini\",\\n            tools=[tavily_tool],\\n            prompt=system_message,\\n            debug=True\\n        )\\n        \\n        try:\\n            response = agent.invoke(\\n                {\"messages\": [{\"role\": \"user\", \"content\": state.user_input}]},\\n                {\"recursion_limit\": recursion_limit},\\n            )\\n            \\n            state.post_text = response[\\'messages\\'][-1].content\\n            state.current_step = \"create_image\"\\n            \\n        except GraphRecursionError:\\n            print(\"Agent stopped due to max iterations.\")\\n            state.error_message = \"Agent stopped due to max iterations\"\\n            state.current_step = \"error\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error creating post text: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef generate_image(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Generate image directly from post text\"\"\"\\n    \\n    try:\\n        post_text = state.post_text\\n        \\n        # Create a proper prompt for image generation\\n        image_prompt_template = \"\"\"\\n        Create a modern, minimalistic professional LinkedIn illustration for the following post: {post_content}\\n        \\n        Style: 3D Object Generator with clean flat vector, LinkedIn color palette (blue, white, grey), simple, corporate-appropriate.\\n        Avoid clutter, make it inspiring and easy to understand.\\n        No text in the image, just visual elements.\\n        \\n        Generate only the image description prompt, no other text.\\n        \"\"\"\\n        \\n        # Create messages for the LLM\\n        messages = [\\n            SystemMessage(content=\"You are an expert LinkedIn Image Prompt Engineer. Generate concise AI image prompts.\"),\\n            HumanMessage(content=image_prompt_template.format(post_content=post_text))\\n        ]\\n        \\n        # Get the image prompt from Gemini\\n        response = gemini_model.invoke(messages)\\n        image_prompt = response.content.strip()\\n        \\n        print(f\"Image prompt: {image_prompt}\")\\n        \\n        # Generate image using DALL-E\\n        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\\n        image_url = dalle_wrapper.run(image_prompt)\\n        \\n        # Update state\\n        state.image_url = image_url\\n        state.image_prompt = image_prompt\\n        state.current_step = \"wait_feedback\" \\n        \\n    except Exception as e:\\n        state.error_message = f\"Error generating image: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 3,\n",
       "   'is_chunked': True,\n",
       "   'language': 'python',\n",
       "   'chunk_size_actual': 3911},\n",
       "  {'path': 'workflow/nodes.py#chunk_1',\n",
       "   'original_path': 'workflow/nodes.py',\n",
       "   'content': 'def classify_feedback(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Classify user feedback into categories\"\"\"\\n    system_message = \"\"\"You are a text classifier. Classify the input text into one of these categories:\\n\\n        1. Approved - Approval message refers that its ok to proceed with the current status.\\n        2. Refine Text - Request to refine the post text\\n        3. Terminate - Termination request for the process.\\n        4. Refine Image - Request to refine the post image\\n\\n        Output only the category name, nothing else.\"\"\"\\n\\n    try:\\n        messages = [\\n            SystemMessage(content=system_message),\\n            HumanMessage(content=state.user_feedback)\\n        ]\\n        \\n        response = gemini_model.invoke(messages)\\n        classification = response.content.strip()\\n        \\n        # Map classification to next step\\n        if \"Approved\" in classification:\\n            state.classification = \"Approved\"\\n            state.current_step = \"post_content\"\\n            state.final_post_ready = True\\n        elif \"Refine Text\" in classification:\\n            state.classification = \"Refine Text\"\\n            state.current_step = \"refine_text\"\\n        elif \"Terminate\" in classification:\\n            state.classification = \"Terminate\"\\n            state.current_step = \"terminate\"\\n        elif \"Refine Image\" in classification:\\n            state.classification = \"Refine Image\"\\n            state.current_step = \"refine_image\"\\n        else:\\n            state.classification = \"Unknown\"\\n            state.current_step = \"wait_feedback\"\\n            state.error_message = f\"Unrecognized feedback classification: {classification}\"\\n        \\n        print(f\"DEBUG: Updated state - classification: {state.classification}, current_step: {state.current_step}\")\\n            \\n    except Exception as e:\\n        state.error_message = f\"Error classifying feedback: {str(e)}\"\\n        state.current_step = \"error\"\\n        print(f\"DEBUG: Exception in classify_feedback: {str(e)}\")\\n    \\n    return state\\n\\n\\ndef refine_text(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Refine the post text based on user feedback\"\"\"\\n    system_message = \"\"\"\\n        You are a helpful assistant who improve and refine LinkedIn post.\\n\\n        Output only the post text.\"\"\"\\n\\n    try:\\n        prompt = f\"Original post: {state.post_text}\\\\\\\\n\\\\\\\\nRequest: {state.user_feedback}\"\\n        \\n        messages = [\\n            SystemMessage(content=system_message),\\n            HumanMessage(content=prompt)\\n        ]\\n        \\n        response = gemini_model.invoke(messages)\\n        state.refined_text = response.content\\n        state.post_text = state.refined_text  # Update the main post text\\n        state.current_step = \"wait_feedback\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error refining text: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef refine_image(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Refine the image based on user feedback\"\"\"\\n    try:\\n        # Use the existing image prompt and add user feedback\\n        modified_prompt = f\"{state.image_prompt}. {state.user_feedback}\"\\n        \\n        print(f\"Refined image prompt: {modified_prompt}\")\\n        \\n        # Generate image using DALL-E with the modified prompt\\n        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\\n        image_url = dalle_wrapper.run(modified_prompt)\\n        \\n        # Update state with the refined image\\n        state.image_url = image_url\\n        state.image_prompt = modified_prompt  # Update the prompt with feedback\\n        state.current_step = \"wait_feedback\"  # Go back to wait for more feedback\\n        \\n        print(f\"Refined image URL: {image_url}\")\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error refining image: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state',\n",
       "   'chunk_index': 1,\n",
       "   'total_chunks': 3,\n",
       "   'is_chunked': True,\n",
       "   'language': 'python',\n",
       "   'chunk_size_actual': 3845},\n",
       "  {'path': 'workflow/nodes.py#chunk_2',\n",
       "   'original_path': 'workflow/nodes.py',\n",
       "   'content': 'def send_email(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Send email with the final post content\"\"\"\\n    try:\\n        # Lazy import Gmail functionality\\n        from langchain_google_community import GmailToolkit\\n        from langchain_google_community.gmail.utils import (\\n            build_resource_service,\\n            get_gmail_credentials,\\n        )\\n\\n        # Test imports without initializing toolkit\\n        print(\"Imports successful!\")\\n\\n            \\n        # Initialize Gmail toolkit\\n        credentials = get_gmail_credentials(\\n            token_file=\"token.json\",\\n            scopes=[\"https://mail.google.com/\"],\\n            client_secrets_file=\"credentials.json\",\\n        )\\n        \\n        api_resource = build_resource_service(credentials=credentials)\\n        gmail_toolkit = GmailToolkit(api_resource=api_resource)\\n        \\n        tools = gmail_toolkit.get_tools()\\n        print(f\\'🚩DEBUG: send mail tool: {tools[1]}\\')\\n        # Compose email content\\n        subject = \"LinkedIn Post Ready for Publishing\"\\n        body = f\"\"\"\\n            Your LinkedIn post is ready:\\n\\n            {state.post_text}\\n\\n            Image URL: {state.image_url}\\n\\n            Best regards,\\n            LinkedIn Post Creator\\n            \"\"\"\\n        \\n        # Send email using Gmail toolkit\\n        send_message_tool = gmail_toolkit.get_tools()[1]  # send_gmail_message tool\\n        \\n        email_result = send_message_tool.run({\\n            \"to\": \"mohammed-mowina@outlook.com\",\\n            \"subject\": subject,\\n            \"message\": body\\n        })\\n        \\n        state.current_step = \"completed\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"🚩Error sending email: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state',\n",
       "   'chunk_index': 2,\n",
       "   'total_chunks': 3,\n",
       "   'is_chunked': True,\n",
       "   'language': 'python',\n",
       "   'chunk_size_actual': 1753},\n",
       "  {'path': 'workflow/state.py',\n",
       "   'content': 'from dataclasses import dataclass\\nfrom typing import Optional\\n\\n\\n@dataclass\\nclass WorkflowState:\\n    \"\"\"State management for the LinkedIn post creation workflow.\"\"\"\\n    user_input: str = \"\"\\n    post_text: str = \"\"\\n    image_prompt: str = \"\"\\n    image_data: Optional[bytes] = None\\n    image_url: str = \"\"\\n    user_feedback: str = \"\"\\n    classification: str = \"\"\\n    refined_text: str = \"\"\\n    refined_image_data: Optional[bytes] = None\\n    current_step: str = \"start\"\\n    error_message: str = \"\"\\n    final_post_ready: bool = False\\n\\n\\n',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'python'},\n",
       "  {'path': 'workflow/tools.py',\n",
       "   'content': 'import os\\nimport requests\\nfrom typing import Any, Dict\\n\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\nfrom langchain.tools import Tool\\nfrom langchain_core.tools import tool\\n\\ndef tavily_search(query: str) -> str:\\n    \"\"\"Search the web using Tavily API\"\"\"\\n    try:\\n        url = \"https://api.tavily.com/search\"\\n        headers = {\"Authorization\": f\"Bearer {Config.TAVILY_API_KEY}\"}\\n        data = {\"query\": query}\\n        \\n        response = requests.post(url, headers=headers, json=data)\\n        response.raise_for_status()\\n        \\n        results = response.json()\\n        # Extract relevant information from search results\\n        search_results = []\\n        for result in results.get(\"results\", [])[:3]:  # Top 3 results\\n            search_results.append(f\"Title: {result.get(\\'title\\', \\'\\')}\\\\nContent: {result.get(\\'content\\', \\'\\')[:200]}...\")\\n        \\n        return \"\\\\n\\\\n\".join(search_results)\\n    except Exception as e:\\n        return f\"Search failed: {str(e)}\"\\n\\n\\n# Create Tavily search tool\\ntavily_tool = Tool(\\n    name=\"web_search\",\\n    description=\"Search the web for current information\",\\n    func=tavily_search\\n)\\n',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'python'}],\n",
       " 'chunking_info': {'chunk_size': 4000,\n",
       "  'chunk_overlap': 400,\n",
       "  'total_chunks': 20,\n",
       "  'files_chunked': 3,\n",
       "  'files_not_chunked': 11}}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AG94NddmMCL59NC4j2kmSQ29vT4HN9lGdqXEiQrUwO9hiX4CUsBnJQQJ99BIACAAAAAHdXvcAAASAZDOXgON'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.getenv(\"AZURE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fill these in ---\n",
    "organization = \"areebgroup\"\n",
    "project      = \"Internship-Playground\"\n",
    "repository   = \"Internship-ai\"\n",
    "branch       = \"Linked-Booster-LangGraph-Task\" # Optional\n",
    "pat          = os.getenv(\"AZURE_API_KEY\")  # must have Code -> Read\n",
    "# ----------------------\n",
    "\n",
    "# Embed the PAT directly in the clone URL\n",
    "# NOTE: Keep this token private! Don't commit it.\n",
    "clone_url = f\"https://{organization}:{pat}@dev.azure.com/{organization}/{project}/_git/{repository}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 14 relevant files from Internship-ai\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'repo_name': 'Internship-ai',\n",
       " 'commit_count': 19,\n",
       " 'branches': ['Linked-Booster-LangGraph-Task'],\n",
       " 'files': [{'path': 'README.md',\n",
       "   'content': '# LinkedIn-Booster 🚀\\n\\nAn intelligent AI-powered LinkedIn post creation workflow that combines the power of LangChain, LangGraph, and multiple AI services to create engaging LinkedIn content with matching visuals.\\n\\n![LangGraph Workflow](images/LangGraph-workflow-diagram.png)\\n\\n## ✨ Features\\n\\n- **🤖 AI-Powered Generation**: Google Gemini for high-quality LinkedIn post drafting\\n- **🔍 Web Research**: Tavily search integration for up-to-date context and insights\\n- **🔄 Interactive Feedback Loop**: Intelligent feedback classification and content refinement\\n- **🎨 Image Generation**: DALL·E 3 integration for compelling visual content\\n- **📧 Email Delivery**: Gmail API integration for seamless content distribution\\n- **🖥️ Multiple Interfaces**: CLI and web-based Chainlit UI options\\n- **📊 Workflow Visualization**: LangGraph Studio integration for workflow monitoring\\n\\n## 🏗️ Architecture\\n\\nThe project follows a modular architecture with clear separation of concerns:\\n\\n```\\nLinkedIn-Booster/\\n├── 📄 main.py                    # CLI interface\\n├── 🌐 app.py                     # Chainlit web interface  \\n├── 📁 workflow/\\n│   ├── __init__.py\\n│   ├── state.py                  # Workflow state management\\n│   ├── tools.py                  # External API integrations (Tavily)\\n│   ├── nodes.py                  # Core workflow nodes\\n│   └── graph.py                  # LangGraph workflow orchestration\\n├── 📁 utils/\\n│   ├── __init__.py\\n│   └── config.py                 # Configuration and environment setup\\n├── 📁 studio/                    # LangGraph Studio configuration\\n├── 📁 notebooks/                 # Development notebooks\\n├── 📁 images/                    # Documentation assets\\n├── 📄 requirements.txt\\n└── 📄 .env                       # Environment variables\\n```\\n\\n## 🚀 Quick Start\\n\\n### Prerequisites\\n- Python 3.8+\\n- API keys for OpenAI, Google Gemini, and Tavily\\n- Gmail API credentials (optional, for email functionality)\\n\\n### Installation\\n\\n1. **Clone and install dependencies**\\n```bash\\ngit clone <repository-url>\\ncd LinkedIn-Booster\\npip install -r requirements.txt\\n```\\n\\n2. **Configure environment variables**\\nCreate a `.env` file in the root directory:\\n```bash\\n# AI Service APIs\\nOPENAI_API_KEY=your_openai_api_key\\nGOOGLE_API_KEY=your_google_gemini_api_key\\nTAVILY_API_KEY=your_tavily_api_key\\n\\n# Gmail Integration (Optional)\\nGMAIL_CREDENTIALS_FILE=credentials.json\\nGMAIL_TOKEN_FILE=token.json\\nRECIPIENT_EMAIL=recipient@example.com\\n```\\n\\n3. **Gmail Setup (Optional)**\\nFor email functionality:\\n- Enable Gmail API in [Google Cloud Console](https://console.cloud.google.com/)\\n- Create OAuth2 credentials and download as `credentials.json`\\n- Place the file in the project root\\n- First run will generate `token.json` automatically\\n\\n## 💻 Usage\\n\\n### Option 1: Web Interface (Recommended)\\nLaunch the interactive Chainlit web interface:\\n\\n```bash\\nchainlit run app.py\\n```\\n\\n![Chainlit Interface](images/ChainLit-user%20Interface.png)\\n\\nThe web interface provides:\\n- 🎯 Guided workflow with clear instructions\\n- 📱 Real-time feedback and refinement\\n- 🖼️ Visual preview of generated content\\n- ✅ Easy approval and sending process\\n\\n### Option 2: Command Line Interface\\nFor quick automation or scripting:\\n\\n```bash\\npython main.py \"Create a post about the importance of drinking water\"\\n```\\n\\n### Option 3: LangGraph Studio\\nFor workflow development and debugging:\\n\\n![LangGraph Studio](images/LangGraphStudio-workflow-diagram.png)\\n\\n```bash\\ncd studio\\nlanggraph dev\\n```\\n\\n### Option 4: n8n Integration\\nFor no-code workflow automation:\\n\\n![n8n Workflow](images/n8n-workflow-diagram.png)\\n\\nImport the workflow from `n8n/main (NO img2img).json` into your n8n instance.\\n\\n## 🔄 Workflow Process\\n\\nThe LinkedIn Booster follows an intelligent multi-step process:\\n\\n1. **📝 Content Creation**: AI generates LinkedIn post based on your topic\\n2. **🎨 Image Generation**: Creates matching visual content using DALL·E 3\\n3. **👀 Review & Feedback**: Present content for user review\\n4. **🤖 Smart Classification**: AI analyzes feedback and determines next action\\n5. **✨ Refinement**: Improves text or image based on feedback\\n6. **✅ Approval & Delivery**: Sends final content via email when approved\\n\\nThe workflow uses LangGraph\\'s stateful execution with strategic interrupts, allowing for human-in-the-loop refinement at key decision points.\\n\\n## 🛠️ Development\\n\\n### Running Tests\\n```bash\\npython test_imports.py\\npython test_create_post.py\\npython send_mail_test.py\\n```\\n\\n### Jupyter Notebook\\nExplore the original development process:\\n```bash\\njupyter notebook notebooks/booster_workflow_v0.2.ipynb\\n```\\n\\n## 📋 Requirements\\n\\nKey dependencies include:\\n- `langchain` - LLM framework and integrations\\n- `langgraph` - Workflow orchestration\\n- `chainlit` - Web interface\\n- `google-api-python-client` - Gmail integration\\n- `openai` - DALL·E 3 image generation\\n- `tavily-python` - Web search capabilities\\n\\nSee `requirements.txt` for complete list.\\n\\n## 🤝 Contributing\\n\\n1. Fork the repository\\n2. Create a feature branch\\n3. Make your changes\\n4. Add tests if applicable\\n5. Submit a pull request\\n\\n## 📄 License\\n\\nMIT License - see LICENSE file for details.'},\n",
       "  {'path': 'app.py',\n",
       "   'content': '\"\"\"Chainlit GUI for LinkedIn Booster workflow.\"\"\"\\r\\nimport chainlit as cl\\r\\nfrom typing import Dict, Any\\r\\nimport asyncio\\r\\nfrom utils import load_env\\r\\nfrom workflow.graph import start_workflow, continue_workflow\\r\\n\\r\\n\\r\\n# Load environment variables\\r\\nload_env()\\r\\n\\r\\n# Store user sessions\\r\\nuser_sessions: Dict[str, Dict[str, Any]] = {}\\r\\n\\r\\n\\r\\n@cl.on_chat_start\\r\\nasync def start():\\r\\n    \"\"\"Initialize the chat session.\"\"\"\\r\\n    session_id = cl.user_session.get(\"id\")\\r\\n    user_sessions[session_id] = {\\r\\n        \"thread_id\": f\"chainlit_{session_id}\",\\r\\n        \"current_state\": None,\\r\\n        \"step\": \"initial\"\\r\\n    }\\r\\n    \\r\\n    await cl.Message(\\r\\n        content=\"🚀 **Welcome to LinkedIn Booster!**\\\\n\\\\n\"\\r\\n                \"I\\'ll help you create engaging LinkedIn posts with AI-generated images.\\\\n\\\\n\"\\r\\n                \"**How it works:**\\\\n\"\\r\\n                \"1. Tell me your post topic or idea\\\\n\"\\r\\n                \"2. I\\'ll generate a professional post and matching image\\\\n\"\\r\\n                \"3. You can review and refine both text and image\\\\n\"\\r\\n                \"4. Once approved, I\\'ll send it via email\\\\n\\\\n\"\\r\\n                \"**What would you like to post about today?**\"\\r\\n    ).send()\\r\\n\\r\\n\\r\\n@cl.on_message\\r\\nasync def main(message: cl.Message):\\r\\n    \"\"\"Handle user messages.\"\"\"\\r\\n    session_id = cl.user_session.get(\"id\")\\r\\n    session = user_sessions.get(session_id, {})\\r\\n    \\r\\n    user_input = message.content.strip()\\r\\n    \\r\\n    if session.get(\"step\") == \"initial\":\\r\\n        # Start the workflow\\r\\n        await handle_initial_request(user_input, session)\\r\\n    else:\\r\\n        # Continue with feedback\\r\\n        await handle_feedback(user_input, session)\\r\\n\\r\\n\\r\\nasync def handle_initial_request(topic: str, session: Dict[str, Any]):\\r\\n    \"\"\"Handle the initial post creation request.\"\"\"\\r\\n    session_id = cl.user_session.get(\"id\")\\r\\n    thread_id = session[\"thread_id\"]\\r\\n    \\r\\n    # Show loading message\\r\\n    loading_msg = await cl.Message(content=\"🔄 Creating your LinkedIn post...\").send()\\r\\n    \\r\\n    try:\\r\\n        # Start the workflow\\r\\n        state = await asyncio.to_thread(start_workflow, topic, thread_id)\\r\\n        session[\"current_state\"] = state\\r\\n        \\r\\n        if state.error_message:\\r\\n            await cl.Message(content=f\"❌ **Error:** {state.error_message}\").send()\\r\\n            return\\r\\n        \\r\\n        # Show success message\\r\\n        await cl.Message(content=\"✅ **Post Created Successfully!**\").send()\\r\\n        \\r\\n        # Display the generated post\\r\\n        await cl.Message(\\r\\n            content=f\"📝 **Generated LinkedIn Post:**\\\\n\\\\n{state.post_text}\"\\r\\n        ).send()\\r\\n        \\r\\n        # Display the image if available\\r\\n        if state.image_url:\\r\\n            await cl.Message(\\r\\n                content=f\"🖼️ **Generated Image:**\\\\n\\\\n![Generated Image]({state.image_url})\"\\r\\n            ).send()\\r\\n        \\r\\n        # Ask for feedback\\r\\n        session[\"step\"] = \"feedback\"\\r\\n        await cl.Message(\\r\\n            content=\"**What would you like to do next?**\\\\n\\\\n\"\\r\\n                   \"• Type **\\'approve\\'** to send the post via email\\\\n\"\\r\\n                   \"• Type **\\'refine text [your suggestions]\\'** to improve the text\\\\n\"\\r\\n                   \"• Type **\\'refine image [your suggestions]\\'** to improve the image\\\\n\"\\r\\n                   \"• Type **\\'quit\\'** to end the session\"\\r\\n        ).send()\\r\\n        \\r\\n    except Exception as e:\\r\\n        await cl.Message(content=f\"❌ **Error:** {str(e)}\").send()\\r\\n\\r\\n\\r\\nasync def handle_feedback(feedback: str, session: Dict[str, Any]):\\r\\n    \"\"\"Handle user feedback on the generated content.\"\"\"\\r\\n    session_id = cl.user_session.get(\"id\")\\r\\n    thread_id = session[\"thread_id\"]\\r\\n    \\r\\n    if feedback.lower() in {\"quit\", \"exit\", \"q\"}:\\r\\n        await cl.Message(content=\"👋 **Thanks for using LinkedIn Booster!** Have a great day!\").send()\\r\\n        session[\"step\"] = \"completed\"\\r\\n        return\\r\\n    \\r\\n    # Show processing message\\r\\n    await cl.Message(content=\"🔄 Processing your feedback...\").send()\\r\\n    \\r\\n    try:\\r\\n        # Continue the workflow with feedback\\r\\n        state = await asyncio.to_thread(continue_workflow, feedback, thread_id)\\r\\n        session[\"current_state\"] = state\\r\\n        \\r\\n        if state.error_message:\\r\\n            await cl.Message(content=f\"❌ **Error:** {state.error_message}\").send()\\r\\n            return\\r\\n        \\r\\n        # Handle different workflow states\\r\\n        if state.current_step == \"completed\":\\r\\n            await cl.Message(content=\"✅ **Email sent successfully!**\").send()\\r\\n            await cl.Message(\\r\\n                content=\"🎉 **Workflow Completed!**\\\\n\\\\n\"\\r\\n                       \"Your LinkedIn post has been sent via email. \"\\r\\n                       \"You can start a new post by refreshing the page.\"\\r\\n            ).send()\\r\\n            session[\"step\"] = \"completed\"\\r\\n            \\r\\n        elif state.current_step == \"terminate\":\\r\\n            await cl.Message(content=\"🛑 **Workflow terminated.**\").send()\\r\\n            await cl.Message(\\r\\n                content=\"The workflow has been terminated. \"\\r\\n                       \"You can start a new post by refreshing the page.\"\\r\\n            ).send()\\r\\n            session[\"step\"] = \"completed\"\\r\\n            \\r\\n        elif state.current_step == \"wait_feedback\":\\r\\n            await cl.Message(content=\"✅ **Content updated!**\").send()\\r\\n            \\r\\n            # Show updated content based on what was refined\\r\\n            if \"refine text\" in feedback.lower() or \"refine_text\" in state.classification.lower():\\r\\n                await cl.Message(\\r\\n                    content=f\"📝 **Updated LinkedIn Post:**\\\\n\\\\n{state.post_text}\"\\r\\n                ).send()\\r\\n                \\r\\n            elif \"refine image\" in feedback.lower() or \"refine_image\" in state.classification.lower():\\r\\n                if state.image_url:\\r\\n                    await cl.Message(\\r\\n                        content=f\"🖼️ **Updated Image:**\\\\n\\\\n![Updated Image]({state.image_url})\"\\r\\n                    ).send()\\r\\n            \\r\\n            # Ask for next action\\r\\n            await cl.Message(\\r\\n                content=\"**What would you like to do next?**\\\\n\\\\n\"\\r\\n                       \"• Type **\\'approve\\'** to send the post via email\\\\n\"\\r\\n                       \"• Type **\\'refine text [your suggestions]\\'** to improve the text further\\\\n\"\\r\\n                       \"• Type **\\'refine image [your suggestions]\\'** to improve the image further\\\\n\"\\r\\n                       \"• Type **\\'quit\\'** to end the session\"\\r\\n            ).send()\\r\\n        \\r\\n    except Exception as e:\\r\\n        await cl.Message(content=f\"❌ **Error:** {str(e)}\").send()\\r\\n\\r\\n\\r\\n@cl.on_chat_end\\r\\nasync def end():\\r\\n    \"\"\"Clean up when chat ends.\"\"\"\\r\\n    session_id = cl.user_session.get(\"id\")\\r\\n    if session_id in user_sessions:\\r\\n        del user_sessions[session_id]'},\n",
       "  {'path': 'main.py',\n",
       "   'content': 'import sys\\r\\nfrom utils import load_env\\r\\nfrom workflow.graph import start_workflow, continue_workflow\\r\\n\\r\\n\\r\\ndef main() -> None:\\r\\n    load_env()\\r\\n    if len(sys.argv) < 2:\\r\\n        print(\"Usage: python main.py \\\\\"Your post topic...\\\\\"\")\\r\\n        sys.exit(1)\\r\\n    topic = sys.argv[1]\\r\\n    thread_id = \"cli\"\\r\\n    state = start_workflow(topic, thread_id)\\r\\n    if state.current_step != \"wait_feedback\":\\r\\n        print(f\"Error: {state.error_message}\")\\r\\n        sys.exit(1)\\r\\n    print(\"\\\\nGenerated Post:\\\\n\")\\r\\n    print(state.post_text)\\r\\n    print(\"\\\\nImage URL:\\\\n\")\\r\\n    print(state.image_url)\\r\\n    while True:\\r\\n        feedback = input(\"\\\\nFeedback (approve/refine text .../refine image .../quit): \").strip()\\r\\n        if feedback.lower() in {\"quit\", \"exit\", \"q\"}:\\r\\n            print(\"Goodbye!\")\\r\\n            break\\r\\n        state = continue_workflow(feedback, thread_id)\\r\\n        if state.current_step == \"completed\":\\r\\n            print(\"Email sent. Workflow completed.\")\\r\\n            break\\r\\n        if state.current_step == \"terminate\":\\r\\n            print(\"Workflow terminated.\")\\r\\n            break\\r\\n        if state.current_step == \"wait_feedback\":\\r\\n            if state.classification == \"Refine Text\":\\r\\n                print(\"\\\\nUpdated Post:\\\\n\")\\r\\n                print(state.post_text)\\r\\n            elif state.classification == \"Refine Image\":\\r\\n                print(\"\\\\nUpdated Image URL:\\\\n\")\\r\\n                print(state.image_url)\\r\\n\\r\\n\\r\\nif __name__ == \"__main__\":\\r\\n    main()\\r\\n\\r\\n\\r\\n'},\n",
       "  {'path': 'requirements.txt',\n",
       "   'content': 'chainlit>=1.0.0\\r\\nlangchain>=0.1.0\\r\\nlangchain-community>=0.1.0\\r\\nlangchain-google-genai>=1.0.0\\r\\nlangchain-google-community>=0.1.0\\r\\nlanggraph>=0.0.40\\r\\nrequests>=2.31.0\\r\\ntyping-extensions>=4.5.0\\r\\npython-dotenv>=1.0.0\\r\\ngoogle-auth>=2.0.0\\r\\ngoogle-auth-oauthlib>=1.0.0\\r\\ngoogle-auth-httplib2>=0.2.0\\r\\ngoogle-api-python-client>=2.0.0\\r\\nopenai>=1.0.0\\r\\ntavily-python>=0.3.0'},\n",
       "  {'path': 'notebooks/notebooks.md', 'content': ''},\n",
       "  {'path': 'studio/graph.py',\n",
       "   'content': '\"\"\"Standalone graph file for LangGraph Studio.\"\"\"\\r\\nimport sys\\r\\nimport os\\r\\nfrom typing import Optional\\r\\nfrom dataclasses import dataclass\\r\\n\\r\\n# Add the parent directory to the path so we can import from workflow\\r\\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\\r\\n\\r\\nfrom langgraph.graph import StateGraph, END\\r\\n\\r\\n# Define state directly to avoid import conflicts\\r\\n@dataclass\\r\\nclass WorkflowState:\\r\\n    \"\"\"State management for the LinkedIn post creation workflow.\"\"\"\\r\\n    user_input: str = \"\"\\r\\n    post_text: str = \"\"\\r\\n    image_prompt: str = \"\"\\r\\n    image_data: Optional[bytes] = None\\r\\n    image_url: str = \"\"\\r\\n    user_feedback: str = \"\"\\r\\n    classification: str = \"\"\\r\\n    refined_text: str = \"\"\\r\\n    refined_image_data: Optional[bytes] = None\\r\\n    current_step: str = \"start\"\\r\\n    error_message: str = \"\"\\r\\n    final_post_ready: bool = False\\r\\n\\r\\n# Import nodes after defining state\\r\\ntry:\\r\\n    from workflow.nodes import (\\r\\n        create_post_text,\\r\\n        generate_image,\\r\\n        classify_feedback,\\r\\n        refine_text,\\r\\n        refine_image,\\r\\n        send_email,\\r\\n    )\\r\\nexcept ImportError as e:\\r\\n    # Fallback dummy functions for studio\\r\\n    def create_post_text(state: WorkflowState) -> WorkflowState:\\r\\n        state.post_text = \"Sample LinkedIn post created\"\\r\\n        return state\\r\\n    \\r\\n    def generate_image(state: WorkflowState) -> WorkflowState:\\r\\n        state.image_url = \"https://example.com/image.jpg\"\\r\\n        return state\\r\\n    \\r\\n    def classify_feedback(state: WorkflowState) -> WorkflowState:\\r\\n        state.classification = \"approved\"\\r\\n        return state\\r\\n    \\r\\n    def refine_text(state: WorkflowState) -> WorkflowState:\\r\\n        state.refined_text = \"Refined post text\"\\r\\n        return state\\r\\n    \\r\\n    def refine_image(state: WorkflowState) -> WorkflowState:\\r\\n        state.refined_image_data = b\"refined_image_data\"\\r\\n        return state\\r\\n    \\r\\n    def send_email(state: WorkflowState) -> WorkflowState:\\r\\n        state.final_post_ready = True\\r\\n        return state\\r\\n\\r\\n\\r\\ndef route_feedback(state: WorkflowState) -> str:\\r\\n    \"\"\"Route based on feedback classification.\"\"\"\\r\\n    classification = state.classification.lower()\\r\\n    if \"approved\" in classification:\\r\\n        return \"approved\"\\r\\n    if \"refine text\" in classification or \"refine_text\" in classification:\\r\\n        return \"refine_text\"\\r\\n    if \"refine image\" in classification or \"refine_image\" in classification:\\r\\n        return \"refine_image\"\\r\\n    if \"terminate\" in classification:\\r\\n        return \"terminate\"\\r\\n    return \"terminate\"\\r\\n\\r\\n\\r\\ndef create_studio_workflow():\\r\\n    \"\"\"Create the LangGraph workflow for Studio (without checkpointer).\"\"\"\\r\\n    workflow = StateGraph(WorkflowState)\\r\\n    workflow.add_node(\"create_post\", create_post_text)\\r\\n    workflow.add_node(\"generate_image\", generate_image)\\r\\n    workflow.add_node(\"classify_feedback\", classify_feedback)\\r\\n    workflow.add_node(\"refine_text\", refine_text)\\r\\n    workflow.add_node(\"refine_image\", refine_image)\\r\\n    workflow.add_node(\"send_email\", send_email)\\r\\n    workflow.set_entry_point(\"create_post\")\\r\\n    workflow.add_edge(\"create_post\", \"generate_image\")\\r\\n    workflow.add_edge(\"generate_image\", \"classify_feedback\")\\r\\n    workflow.add_conditional_edges(\\r\\n        \"classify_feedback\",\\r\\n        route_feedback,\\r\\n        {\\r\\n            \"approved\": \"send_email\",\\r\\n            \"refine_text\": \"refine_text\",\\r\\n            \"refine_image\": \"refine_image\",\\r\\n            \"terminate\": END,\\r\\n        },\\r\\n    )\\r\\n    workflow.add_edge(\"refine_text\", \"classify_feedback\")\\r\\n    workflow.add_edge(\"refine_image\", \"classify_feedback\")\\r\\n    workflow.add_edge(\"send_email\", END)\\r\\n    \\r\\n    # Compile without checkpointer for LangGraph Studio\\r\\n    app = workflow.compile(\\r\\n        interrupt_after=[\"generate_image\", \"refine_text\", \"refine_image\"],\\r\\n    )\\r\\n    return app\\r\\n\\r\\n\\r\\n# Create the app for LangGraph Studio\\r\\napp = create_studio_workflow()\\r\\n\\r\\n# Export the app for LangGraph Studio\\r\\n__all__ = [\"app\"]'},\n",
       "  {'path': 'studio/langgraph.json',\n",
       "   'content': '{\\r\\n    \"dockerfile_lines\": [],\\r\\n    \"graphs\": {\\r\\n      \"chatbot\": \"./graph.py:app\"\\r\\n    },\\r\\n    \"env\": \"./.env\",\\r\\n    \"python_version\": \"3.11\",\\r\\n    \"dependencies\": [\\r\\n      \".\"\\r\\n    ]\\r\\n  }'},\n",
       "  {'path': 'utils/__init__.py',\n",
       "   'content': 'from .config import load_env\\r\\n\\r\\n__all__ = [\"load_env\"]\\r\\n\\r\\n\\r\\n'},\n",
       "  {'path': 'utils/config.py',\n",
       "   'content': 'import os\\r\\nfrom dataclasses import dataclass\\r\\nfrom dotenv import load_dotenv\\r\\n\\r\\n\\r\\ndef load_env() -> None:\\r\\n    \"\"\"Load environment variables from a .env file if present.\"\"\"\\r\\n    load_dotenv()\\r\\n\\r\\n\\r\\n@dataclass\\r\\nclass Config:\\r\\n    OPENAI_API_KEY: str | None = os.getenv(\"OPENAI_API_KEY\")\\r\\n    GOOGLE_API_KEY: str | None = os.getenv(\"GOOGLE_API_KEY\")\\r\\n    TAVILY_API_KEY: str | None = os.getenv(\"TAVILY_API_KEY\")\\r\\n    GMAIL_CREDENTIALS_FILE: str = os.getenv(\"GMAIL_CREDENTIALS_FILE\", \"credentials.json\")\\r\\n    GMAIL_TOKEN_FILE: str = os.getenv(\"GMAIL_TOKEN_FILE\", \"token.json\")\\r\\n    RECIPIENT_EMAIL: str = os.getenv(\"RECIPIENT_EMAIL\", \"mohammed-mowina@outlook.com\")\\r\\n\\r\\n\\r\\n'},\n",
       "  {'path': 'workflow/__init__.py',\n",
       "   'content': '\"\"\"LinkedIn Booster workflow package.\"\"\"\\r\\n\\r\\nfrom .graph import app, create_workflow, start_workflow, continue_workflow\\r\\n\\r\\n__all__ = [\"app\", \"create_workflow\", \"start_workflow\", \"continue_workflow\"]'},\n",
       "  {'path': 'workflow/graph.py',\n",
       "   'content': 'from langgraph.graph import StateGraph, END\\r\\nfrom langgraph.checkpoint.memory import MemorySaver\\r\\n\\r\\nfrom workflow.state import WorkflowState\\r\\nfrom workflow.nodes import (\\r\\n    create_post_text,\\r\\n    generate_image,\\r\\n    classify_feedback,\\r\\n    refine_text,\\r\\n    refine_image,\\r\\n    send_email,\\r\\n)\\r\\n\\r\\n\\r\\ndef route_feedback(state: WorkflowState) -> str:\\r\\n    \"\"\"Route based on feedback classification.\"\"\"\\r\\n    classification = state.classification.lower()\\r\\n    if \"approved\" in classification:\\r\\n        return \"approved\"\\r\\n    if \"refine text\" in classification or \"refine_text\" in classification:\\r\\n        return \"refine_text\"\\r\\n    if \"refine image\" in classification or \"refine_image\" in classification:\\r\\n        return \"refine_image\"\\r\\n    if \"terminate\" in classification:\\r\\n        return \"terminate\"\\r\\n    return \"terminate\"\\r\\n\\r\\n\\r\\ndef create_workflow():\\r\\n    \"\"\"Create the LangGraph workflow with interrupts configured.\"\"\"\\r\\n    workflow = StateGraph(WorkflowState)\\r\\n    workflow.add_node(\"create_post\", create_post_text)\\r\\n    workflow.add_node(\"generate_image\", generate_image)\\r\\n    workflow.add_node(\"classify_feedback\", classify_feedback)\\r\\n    workflow.add_node(\"refine_text\", refine_text)\\r\\n    workflow.add_node(\"refine_image\", refine_image)\\r\\n    workflow.add_node(\"send_email\", send_email)\\r\\n    workflow.set_entry_point(\"create_post\")\\r\\n    workflow.add_edge(\"create_post\", \"generate_image\")\\r\\n    workflow.add_edge(\"generate_image\", \"classify_feedback\")\\r\\n    workflow.add_conditional_edges(\\r\\n        \"classify_feedback\",\\r\\n        route_feedback,\\r\\n        {\\r\\n            \"approved\": \"send_email\",\\r\\n            \"refine_text\": \"refine_text\",\\r\\n            \"refine_image\": \"refine_image\",\\r\\n            \"terminate\": END,\\r\\n        },\\r\\n    )\\r\\n    workflow.add_edge(\"refine_text\", \"classify_feedback\")\\r\\n    workflow.add_edge(\"refine_image\", \"classify_feedback\")\\r\\n    workflow.add_edge(\"send_email\", END)\\r\\n    memory = MemorySaver()\\r\\n    app = workflow.compile(\\r\\n        checkpointer=memory,\\r\\n        interrupt_after=[\"generate_image\", \"refine_text\", \"refine_image\"],\\r\\n    )\\r\\n    return app\\r\\n\\r\\n\\r\\n# Convenience API mirroring notebook helpers\\r\\napp = create_workflow()\\r\\n\\r\\n\\r\\ndef start_workflow(user_input: str, thread_id: str = \"default\") -> WorkflowState:\\r\\n    initial_state = WorkflowState(user_input=user_input)\\r\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\r\\n    result = app.invoke(initial_state, config)\\r\\n    return WorkflowState(**result)\\r\\n\\r\\n\\r\\ndef continue_workflow(user_feedback: str, thread_id: str = \"default\") -> WorkflowState:\\r\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\r\\n    app.update_state(config, {\"user_feedback\": user_feedback})\\r\\n    result = app.invoke(None, config)\\r\\n    return WorkflowState(**result)\\r\\n\\r\\n\\r\\n'},\n",
       "  {'path': 'workflow/nodes.py',\n",
       "   'content': 'import os\\r\\nfrom typing import List\\r\\n\\r\\nfrom dotenv import load_dotenv\\r\\nload_dotenv()\\r\\n\\r\\nfrom langchain.chat_models import init_chat_model\\r\\nfrom langchain.schema import SystemMessage, HumanMessage\\r\\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\\r\\nfrom langgraph.errors import GraphRecursionError\\r\\nfrom langgraph.prebuilt import create_react_agent\\r\\n# Gmail imports will be done lazily in send_email function\\r\\n\\r\\nfrom workflow.state import WorkflowState\\r\\nfrom workflow.tools import tavily_tool\\r\\n\\r\\n\\r\\n# Initialize models\\r\\ngemini_model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\\r\\n\\r\\ndef create_post_text(state: WorkflowState) -> WorkflowState:\\r\\n    \"\"\"Generate LinkedIn post text based on user input\"\"\"\\r\\n    system_message = \"\"\"\\r\\n    You are a professional LinkedIn content creator. Create engaging, professional LinkedIn posts.\\r\\n\\r\\n    Your tone should be:\\r\\n    - Professional yet approachable\\r\\n    - Concise and clear\\r\\n    - Action-oriented with clear value\\r\\n\\r\\n    Format your post with:\\r\\n    - A strong hook in the first sentence\\r\\n    - Short, scannable paragraphs\\r\\n    - 1-3 relevant emojis strategically placed\\r\\n    - A clear call-to-action\\r\\n    - 3-5 relevant hashtags at the end\\r\\n\\r\\n    Output ONLY the post text, nothing else.\"\"\"\\r\\n    \\r\\n    try:\\r\\n        max_iterations = 3\\r\\n        recursion_limit = 2 * max_iterations + 1\\r\\n        \\r\\n        agent = create_react_agent(\\r\\n            model=\"openai:gpt-4o-mini\",\\r\\n            tools=[tavily_tool],\\r\\n            prompt=system_message,\\r\\n            debug=True\\r\\n        )\\r\\n        \\r\\n        try:\\r\\n            response = agent.invoke(\\r\\n                {\"messages\": [{\"role\": \"user\", \"content\": state.user_input}]},\\r\\n                {\"recursion_limit\": recursion_limit},\\r\\n            )\\r\\n            \\r\\n            state.post_text = response[\\'messages\\'][-1].content\\r\\n            state.current_step = \"create_image\"\\r\\n            \\r\\n        except GraphRecursionError:\\r\\n            print(\"Agent stopped due to max iterations.\")\\r\\n            state.error_message = \"Agent stopped due to max iterations\"\\r\\n            state.current_step = \"error\"\\r\\n        \\r\\n    except Exception as e:\\r\\n        state.error_message = f\"Error creating post text: {str(e)}\"\\r\\n        state.current_step = \"error\"\\r\\n    \\r\\n    return state\\r\\n\\r\\n\\r\\ndef generate_image(state: WorkflowState) -> WorkflowState:\\r\\n    \"\"\"Generate image directly from post text\"\"\"\\r\\n    \\r\\n    try:\\r\\n        post_text = state.post_text\\r\\n        \\r\\n        # Create a proper prompt for image generation\\r\\n        image_prompt_template = \"\"\"\\r\\n        Create a modern, minimalistic professional LinkedIn illustration for the following post: {post_content}\\r\\n        \\r\\n        Style: 3D Object Generator with clean flat vector, LinkedIn color palette (blue, white, grey), simple, corporate-appropriate.\\r\\n        Avoid clutter, make it inspiring and easy to understand.\\r\\n        No text in the image, just visual elements.\\r\\n        \\r\\n        Generate only the image description prompt, no other text.\\r\\n        \"\"\"\\r\\n        \\r\\n        # Create messages for the LLM\\r\\n        messages = [\\r\\n            SystemMessage(content=\"You are an expert LinkedIn Image Prompt Engineer. Generate concise AI image prompts.\"),\\r\\n            HumanMessage(content=image_prompt_template.format(post_content=post_text))\\r\\n        ]\\r\\n        \\r\\n        # Get the image prompt from Gemini\\r\\n        response = gemini_model.invoke(messages)\\r\\n        image_prompt = response.content.strip()\\r\\n        \\r\\n        print(f\"Image prompt: {image_prompt}\")\\r\\n        \\r\\n        # Generate image using DALL-E\\r\\n        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\\r\\n        image_url = dalle_wrapper.run(image_prompt)\\r\\n        \\r\\n        # Update state\\r\\n        state.image_url = image_url\\r\\n        state.image_prompt = image_prompt\\r\\n        state.current_step = \"wait_feedback\" \\r\\n        \\r\\n    except Exception as e:\\r\\n        state.error_message = f\"Error generating image: {str(e)}\"\\r\\n        state.current_step = \"error\"\\r\\n    \\r\\n    return state\\r\\n\\r\\n\\r\\ndef classify_feedback(state: WorkflowState) -> WorkflowState:\\r\\n    \"\"\"Classify user feedback into categories\"\"\"\\r\\n    system_message = \"\"\"You are a text classifier. Classify the input text into one of these categories:\\r\\n\\r\\n        1. Approved - Approval message refers that its ok to proceed with the current status.\\r\\n        2. Refine Text - Request to refine the post text\\r\\n        3. Terminate - Termination request for the process.\\r\\n        4. Refine Image - Request to refine the post image\\r\\n\\r\\n        Output only the category name, nothing else.\"\"\"\\r\\n\\r\\n    try:\\r\\n        messages = [\\r\\n            SystemMessage(content=system_message),\\r\\n            HumanMessage(content=state.user_feedback)\\r\\n        ]\\r\\n        \\r\\n        response = gemini_model.invoke(messages)\\r\\n        classification = response.content.strip()\\r\\n        \\r\\n        # Map classification to next step\\r\\n        if \"Approved\" in classification:\\r\\n            state.classification = \"Approved\"\\r\\n            state.current_step = \"post_content\"\\r\\n            state.final_post_ready = True\\r\\n        elif \"Refine Text\" in classification:\\r\\n            state.classification = \"Refine Text\"\\r\\n            state.current_step = \"refine_text\"\\r\\n        elif \"Terminate\" in classification:\\r\\n            state.classification = \"Terminate\"\\r\\n            state.current_step = \"terminate\"\\r\\n        elif \"Refine Image\" in classification:\\r\\n            state.classification = \"Refine Image\"\\r\\n            state.current_step = \"refine_image\"\\r\\n        else:\\r\\n            state.classification = \"Unknown\"\\r\\n            state.current_step = \"wait_feedback\"\\r\\n            state.error_message = f\"Unrecognized feedback classification: {classification}\"\\r\\n        \\r\\n        print(f\"DEBUG: Updated state - classification: {state.classification}, current_step: {state.current_step}\")\\r\\n            \\r\\n    except Exception as e:\\r\\n        state.error_message = f\"Error classifying feedback: {str(e)}\"\\r\\n        state.current_step = \"error\"\\r\\n        print(f\"DEBUG: Exception in classify_feedback: {str(e)}\")\\r\\n    \\r\\n    return state\\r\\n\\r\\n\\r\\ndef refine_text(state: WorkflowState) -> WorkflowState:\\r\\n    \"\"\"Refine the post text based on user feedback\"\"\"\\r\\n    system_message = \"\"\"\\r\\n        You are a helpful assistant who improve and refine LinkedIn post.\\r\\n\\r\\n        Output only the post text.\"\"\"\\r\\n\\r\\n    try:\\r\\n        prompt = f\"Original post: {state.post_text}\\\\\\\\n\\\\\\\\nRequest: {state.user_feedback}\"\\r\\n        \\r\\n        messages = [\\r\\n            SystemMessage(content=system_message),\\r\\n            HumanMessage(content=prompt)\\r\\n        ]\\r\\n        \\r\\n        response = gemini_model.invoke(messages)\\r\\n        state.refined_text = response.content\\r\\n        state.post_text = state.refined_text  # Update the main post text\\r\\n        state.current_step = \"wait_feedback\"\\r\\n        \\r\\n    except Exception as e:\\r\\n        state.error_message = f\"Error refining text: {str(e)}\"\\r\\n        state.current_step = \"error\"\\r\\n    \\r\\n    return state\\r\\n\\r\\n\\r\\ndef refine_image(state: WorkflowState) -> WorkflowState:\\r\\n    \"\"\"Refine the image based on user feedback\"\"\"\\r\\n    try:\\r\\n        # Use the existing image prompt and add user feedback\\r\\n        modified_prompt = f\"{state.image_prompt}. {state.user_feedback}\"\\r\\n        \\r\\n        print(f\"Refined image prompt: {modified_prompt}\")\\r\\n        \\r\\n        # Generate image using DALL-E with the modified prompt\\r\\n        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\\r\\n        image_url = dalle_wrapper.run(modified_prompt)\\r\\n        \\r\\n        # Update state with the refined image\\r\\n        state.image_url = image_url\\r\\n        state.image_prompt = modified_prompt  # Update the prompt with feedback\\r\\n        state.current_step = \"wait_feedback\"  # Go back to wait for more feedback\\r\\n        \\r\\n        print(f\"Refined image URL: {image_url}\")\\r\\n        \\r\\n    except Exception as e:\\r\\n        state.error_message = f\"Error refining image: {str(e)}\"\\r\\n        state.current_step = \"error\"\\r\\n    \\r\\n    return state\\r\\n\\r\\n\\r\\ndef send_email(state: WorkflowState) -> WorkflowState:\\r\\n    \"\"\"Send email with the final post content\"\"\"\\r\\n    try:\\r\\n        # Lazy import Gmail functionality\\r\\n        from langchain_google_community import GmailToolkit\\r\\n        from langchain_google_community.gmail.utils import (\\r\\n            build_resource_service,\\r\\n            get_gmail_credentials,\\r\\n        )\\r\\n\\r\\n        # Test imports without initializing toolkit\\r\\n        print(\"Imports successful!\")\\r\\n\\r\\n            \\r\\n        # Initialize Gmail toolkit\\r\\n        credentials = get_gmail_credentials(\\r\\n            token_file=\"token.json\",\\r\\n            scopes=[\"https://mail.google.com/\"],\\r\\n            client_secrets_file=\"credentials.json\",\\r\\n        )\\r\\n        \\r\\n        api_resource = build_resource_service(credentials=credentials)\\r\\n        gmail_toolkit = GmailToolkit(api_resource=api_resource)\\r\\n        \\r\\n        tools = gmail_toolkit.get_tools()\\r\\n        print(f\\'🚩DEBUG: send mail tool: {tools[1]}\\')\\r\\n        # Compose email content\\r\\n        subject = \"LinkedIn Post Ready for Publishing\"\\r\\n        body = f\"\"\"\\r\\n            Your LinkedIn post is ready:\\r\\n\\r\\n            {state.post_text}\\r\\n\\r\\n            Image URL: {state.image_url}\\r\\n\\r\\n            Best regards,\\r\\n            LinkedIn Post Creator\\r\\n            \"\"\"\\r\\n        \\r\\n        # Send email using Gmail toolkit\\r\\n        send_message_tool = gmail_toolkit.get_tools()[1]  # send_gmail_message tool\\r\\n        \\r\\n        email_result = send_message_tool.run({\\r\\n            \"to\": \"mohammed-mowina@outlook.com\",\\r\\n            \"subject\": subject,\\r\\n            \"message\": body\\r\\n        })\\r\\n        \\r\\n        state.current_step = \"completed\"\\r\\n        \\r\\n    except Exception as e:\\r\\n        state.error_message = f\"🚩Error sending email: {str(e)}\"\\r\\n        state.current_step = \"error\"\\r\\n    \\r\\n    return state'},\n",
       "  {'path': 'workflow/state.py',\n",
       "   'content': 'from dataclasses import dataclass\\r\\nfrom typing import Optional\\r\\n\\r\\n\\r\\n@dataclass\\r\\nclass WorkflowState:\\r\\n    \"\"\"State management for the LinkedIn post creation workflow.\"\"\"\\r\\n    user_input: str = \"\"\\r\\n    post_text: str = \"\"\\r\\n    image_prompt: str = \"\"\\r\\n    image_data: Optional[bytes] = None\\r\\n    image_url: str = \"\"\\r\\n    user_feedback: str = \"\"\\r\\n    classification: str = \"\"\\r\\n    refined_text: str = \"\"\\r\\n    refined_image_data: Optional[bytes] = None\\r\\n    current_step: str = \"start\"\\r\\n    error_message: str = \"\"\\r\\n    final_post_ready: bool = False\\r\\n\\r\\n\\r\\n'},\n",
       "  {'path': 'workflow/tools.py',\n",
       "   'content': 'import os\\r\\nimport requests\\r\\nfrom typing import Any, Dict\\r\\n\\r\\nfrom dotenv import load_dotenv\\r\\nload_dotenv()\\r\\n\\r\\nfrom langchain.tools import Tool\\r\\nfrom langchain_core.tools import tool\\r\\n\\r\\ndef tavily_search(query: str) -> str:\\r\\n    \"\"\"Search the web using Tavily API\"\"\"\\r\\n    try:\\r\\n        url = \"https://api.tavily.com/search\"\\r\\n        headers = {\"Authorization\": f\"Bearer {Config.TAVILY_API_KEY}\"}\\r\\n        data = {\"query\": query}\\r\\n        \\r\\n        response = requests.post(url, headers=headers, json=data)\\r\\n        response.raise_for_status()\\r\\n        \\r\\n        results = response.json()\\r\\n        # Extract relevant information from search results\\r\\n        search_results = []\\r\\n        for result in results.get(\"results\", [])[:3]:  # Top 3 results\\r\\n            search_results.append(f\"Title: {result.get(\\'title\\', \\'\\')}\\\\nContent: {result.get(\\'content\\', \\'\\')[:200]}...\")\\r\\n        \\r\\n        return \"\\\\n\\\\n\".join(search_results)\\r\\n    except Exception as e:\\r\\n        return f\"Search failed: {str(e)}\"\\r\\n\\r\\n\\r\\n# Create Tavily search tool\\r\\ntavily_tool = Tool(\\r\\n    name=\"web_search\",\\r\\n    description=\"Search the web for current information\",\\r\\n    func=tavily_search\\r\\n)\\r\\n'}]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure_repo_info = get_github_repo_info(clone_url, branch)\n",
    "azure_repo_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': 'README.md',\n",
       "  'content': '# Introduction \\r\\nTODO: Give a short introduction of your project. Let this section explain the objectives or the motivation behind this project. \\r\\n\\r\\n# Getting Started\\r\\nTODO: Guide users through getting your code up and running on their own system. In this section you can talk about:\\r\\n1.\\tInstallation process\\r\\n2.\\tSoftware dependencies\\r\\n3.\\tLatest releases\\r\\n4.\\tAPI references\\r\\n\\r\\n# Build and Test\\r\\nTODO: Describe and show how to build your code and run the tests. \\r\\n\\r\\n# Contribute\\r\\nTODO: Explain how other users and developers can contribute to make your code better. \\r\\n\\r\\nIf you want to learn more about creating good readme files then refer the following [guidelines](https://docs.microsoft.com/en-us/azure/devops/repos/git/create-a-readme?view=azure-devops). You can also seek inspiration from the below readme files:\\r\\n- [ASP.NET Core](https://github.com/aspnet/Home)\\r\\n- [Visual Studio Code](https://github.com/Microsoft/vscode)\\r\\n- [Chakra Core](https://github.com/Microsoft/ChakraCore)'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure_repo_info['files']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_azure_repo_info_chunked(organization: str, project: str, repository: str, \n",
    "                               pat: str, branch: Optional[str] = None,\n",
    "                               chunk_size: int = 4000, chunk_overlap: int = 400) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Clone an Azure DevOps repo temporarily and extract its information with chunking.\n",
    "    \n",
    "    Args:\n",
    "        organization (str): Azure DevOps organization name\n",
    "        project (str): Azure DevOps project name\n",
    "        repository (str): Repository name\n",
    "        pat (str): Personal Access Token with Code -> Read permissions\n",
    "        branch (Optional[str]): Specific branch to clone\n",
    "        chunk_size (int): Maximum chunk size in characters\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        Optional[Dict]: Repository information with chunked files or None if error\n",
    "    \"\"\"\n",
    "    # Construct Azure DevOps clone URL with embedded PAT\n",
    "    clone_url = f\"https://{organization}:{pat}@dev.azure.com/{organization}/{project}/_git/{repository}\"\n",
    "    \n",
    "    base_tmp = tempfile.mkdtemp()\n",
    "    repo_dir = os.path.join(base_tmp, repository)\n",
    "    repo = None\n",
    "    \n",
    "    try:\n",
    "        print(f\"Cloning Azure DevOps repo: {organization}/{project}/{repository}\")\n",
    "        if branch:\n",
    "            print(f\"Branch: {branch}\")\n",
    "        \n",
    "        repo = Repo.clone_from(clone_url, repo_dir, branch=branch)\n",
    "        result = get_git_repo_info_chunked(repo_dir, chunk_size, chunk_overlap)\n",
    "        \n",
    "        # Add Azure-specific metadata\n",
    "        if result:\n",
    "            result['source_type'] = 'azure_devops'\n",
    "            result['organization'] = organization\n",
    "            result['project'] = project\n",
    "            result['azure_repo_name'] = repository\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error cloning Azure DevOps repo: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Close the repository to release file handles\n",
    "        if repo is not None:\n",
    "            repo.close()\n",
    "        \n",
    "        # Use onerror callback to handle readonly files on Windows\n",
    "        try:\n",
    "            shutil.rmtree(base_tmp, onerror=remove_readonly)\n",
    "        except Exception as cleanup_error:\n",
    "            print(f\"Warning: Could not clean up temp directory: {cleanup_error}\")\n",
    "            # Try alternative cleanup method\n",
    "            try:\n",
    "                import subprocess\n",
    "                subprocess.run(['rmdir', '/s', '/q', base_tmp], shell=True, check=False)\n",
    "            except:\n",
    "                pass  # If all cleanup methods fail, just continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning Azure DevOps repo: areebgroup/Internship-Playground/Internship-ai\n",
      "Branch: Linked-Booster-LangGraph-Task\n",
      "Processed 18 chunks from Internship-ai\n",
      "Chunking stats: 3 files chunked, 11 files kept whole\n"
     ]
    }
   ],
   "source": [
    "organization = \"areebgroup\"\n",
    "project      = \"Internship-Playground\"\n",
    "repository   = \"Internship-ai\"\n",
    "branch       = \"Linked-Booster-LangGraph-Task\" # Optional\n",
    "pat          = os.getenv(\"AZURE_API_KEY\")  # must have Code -> Read\n",
    "\n",
    "azure_repo_info = get_azure_repo_info_chunked(organization, project, repository, pat, branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repo_name': 'Internship-ai',\n",
       " 'commit_count': 19,\n",
       " 'branches': ['Linked-Booster-LangGraph-Task'],\n",
       " 'files': [{'path': 'README.md#chunk_0',\n",
       "   'original_path': 'README.md',\n",
       "   'content': '# LinkedIn-Booster 🚀\\n\\nAn intelligent AI-powered LinkedIn post creation workflow that combines the power of LangChain, LangGraph, and multiple AI services to create engaging LinkedIn content with matching visuals.\\n\\n![LangGraph Workflow](images/LangGraph-workflow-diagram.png)\\n\\n## ✨ Features\\n\\n- **🤖 AI-Powered Generation**: Google Gemini for high-quality LinkedIn post drafting\\n- **🔍 Web Research**: Tavily search integration for up-to-date context and insights\\n- **🔄 Interactive Feedback Loop**: Intelligent feedback classification and content refinement\\n- **🎨 Image Generation**: DALL·E 3 integration for compelling visual content\\n- **📧 Email Delivery**: Gmail API integration for seamless content distribution\\n- **🖥️ Multiple Interfaces**: CLI and web-based Chainlit UI options\\n- **📊 Workflow Visualization**: LangGraph Studio integration for workflow monitoring\\n\\n## 🏗️ Architecture\\n\\nThe project follows a modular architecture with clear separation of concerns:\\n\\n```\\nLinkedIn-Booster/\\n├── 📄 main.py                    # CLI interface\\n├── 🌐 app.py                     # Chainlit web interface  \\n├── 📁 workflow/\\n│   ├── __init__.py\\n│   ├── state.py                  # Workflow state management\\n│   ├── tools.py                  # External API integrations (Tavily)\\n│   ├── nodes.py                  # Core workflow nodes\\n│   └── graph.py                  # LangGraph workflow orchestration\\n├── 📁 utils/\\n│   ├── __init__.py\\n│   └── config.py                 # Configuration and environment setup\\n├── 📁 studio/                    # LangGraph Studio configuration\\n├── 📁 notebooks/                 # Development notebooks\\n├── 📁 images/                    # Documentation assets\\n├── 📄 requirements.txt\\n└── 📄 .env                       # Environment variables\\n```\\n\\n## 🚀 Quick Start\\n\\n### Prerequisites\\n- Python 3.8+\\n- API keys for OpenAI, Google Gemini, and Tavily\\n- Gmail API credentials (optional, for email functionality)\\n\\n### Installation\\n\\n1. **Clone and install dependencies**\\n```bash\\ngit clone <repository-url>\\ncd LinkedIn-Booster\\npip install -r requirements.txt\\n```\\n\\n2. **Configure environment variables**\\nCreate a `.env` file in the root directory:\\n```bash\\n# AI Service APIs\\nOPENAI_API_KEY=your_openai_api_key\\nGOOGLE_API_KEY=your_google_gemini_api_key\\nTAVILY_API_KEY=your_tavily_api_key\\n\\n# Gmail Integration (Optional)\\nGMAIL_CREDENTIALS_FILE=credentials.json\\nGMAIL_TOKEN_FILE=token.json\\nRECIPIENT_EMAIL=recipient@example.com\\n```\\n\\n3. **Gmail Setup (Optional)**\\nFor email functionality:\\n- Enable Gmail API in [Google Cloud Console](https://console.cloud.google.com/)\\n- Create OAuth2 credentials and download as `credentials.json`\\n- Place the file in the project root\\n- First run will generate `token.json` automatically\\n\\n## 💻 Usage\\n\\n### Option 1: Web Interface (Recommended)\\nLaunch the interactive Chainlit web interface:\\n\\n```bash\\nchainlit run app.py\\n```\\n\\n![Chainlit Interface](images/ChainLit-user%20Interface.png)\\n\\nThe web interface provides:\\n- 🎯 Guided workflow with clear instructions\\n- 📱 Real-time feedback and refinement\\n- 🖼️ Visual preview of generated content\\n- ✅ Easy approval and sending process\\n\\n### Option 2: Command Line Interface\\nFor quick automation or scripting:\\n\\n```bash\\npython main.py \"Create a post about the importance of drinking water\"\\n```\\n\\n### Option 3: LangGraph Studio\\nFor workflow development and debugging:\\n\\n![LangGraph Studio](images/LangGraphStudio-workflow-diagram.png)\\n\\n```bash\\ncd studio\\nlanggraph dev\\n```\\n\\n### Option 4: n8n Integration\\nFor no-code workflow automation:\\n\\n![n8n Workflow](images/n8n-workflow-diagram.png)\\n\\nImport the workflow from `n8n/main (NO img2img).json` into your n8n instance.',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 2,\n",
       "   'is_chunked': True,\n",
       "   'language': 'markdown',\n",
       "   'chunk_size_actual': 3627},\n",
       "  {'path': 'README.md#chunk_1',\n",
       "   'original_path': 'README.md',\n",
       "   'content': \"### Option 3: LangGraph Studio\\nFor workflow development and debugging:\\n\\n![LangGraph Studio](images/LangGraphStudio-workflow-diagram.png)\\n\\n```bash\\ncd studio\\nlanggraph dev\\n```\\n\\n### Option 4: n8n Integration\\nFor no-code workflow automation:\\n\\n![n8n Workflow](images/n8n-workflow-diagram.png)\\n\\nImport the workflow from `n8n/main (NO img2img).json` into your n8n instance.\\n\\n## 🔄 Workflow Process\\n\\nThe LinkedIn Booster follows an intelligent multi-step process:\\n\\n1. **📝 Content Creation**: AI generates LinkedIn post based on your topic\\n2. **🎨 Image Generation**: Creates matching visual content using DALL·E 3\\n3. **👀 Review & Feedback**: Present content for user review\\n4. **🤖 Smart Classification**: AI analyzes feedback and determines next action\\n5. **✨ Refinement**: Improves text or image based on feedback\\n6. **✅ Approval & Delivery**: Sends final content via email when approved\\n\\nThe workflow uses LangGraph's stateful execution with strategic interrupts, allowing for human-in-the-loop refinement at key decision points.\\n\\n## 🛠️ Development\\n\\n### Running Tests\\n```bash\\npython test_imports.py\\npython test_create_post.py\\npython send_mail_test.py\\n```\\n\\n### Jupyter Notebook\\nExplore the original development process:\\n```bash\\njupyter notebook notebooks/booster_workflow_v0.2.ipynb\\n```\\n\\n## 📋 Requirements\\n\\nKey dependencies include:\\n- `langchain` - LLM framework and integrations\\n- `langgraph` - Workflow orchestration\\n- `chainlit` - Web interface\\n- `google-api-python-client` - Gmail integration\\n- `openai` - DALL·E 3 image generation\\n- `tavily-python` - Web search capabilities\\n\\nSee `requirements.txt` for complete list.\\n\\n## 🤝 Contributing\\n\\n1. Fork the repository\\n2. Create a feature branch\\n3. Make your changes\\n4. Add tests if applicable\\n5. Submit a pull request\\n\\n## 📄 License\\n\\nMIT License - see LICENSE file for details.\",\n",
       "   'chunk_index': 1,\n",
       "   'total_chunks': 2,\n",
       "   'is_chunked': True,\n",
       "   'language': 'markdown',\n",
       "   'chunk_size_actual': 1815},\n",
       "  {'path': 'app.py#chunk_0',\n",
       "   'original_path': 'app.py',\n",
       "   'content': '\"\"\"Chainlit GUI for LinkedIn Booster workflow.\"\"\"\\r\\nimport chainlit as cl\\r\\nfrom typing import Dict, Any\\r\\nimport asyncio\\r\\nfrom utils import load_env\\r\\nfrom workflow.graph import start_workflow, continue_workflow\\r\\n\\r\\n\\r\\n# Load environment variables\\r\\nload_env()\\r\\n\\r\\n# Store user sessions\\r\\nuser_sessions: Dict[str, Dict[str, Any]] = {}\\r\\n\\r\\n\\r\\n@cl.on_chat_start\\r\\nasync def start():\\r\\n    \"\"\"Initialize the chat session.\"\"\"\\r\\n    session_id = cl.user_session.get(\"id\")\\r\\n    user_sessions[session_id] = {\\r\\n        \"thread_id\": f\"chainlit_{session_id}\",\\r\\n        \"current_state\": None,\\r\\n        \"step\": \"initial\"\\r\\n    }\\r\\n    \\r\\n    await cl.Message(\\r\\n        content=\"🚀 **Welcome to LinkedIn Booster!**\\\\n\\\\n\"\\r\\n                \"I\\'ll help you create engaging LinkedIn posts with AI-generated images.\\\\n\\\\n\"\\r\\n                \"**How it works:**\\\\n\"\\r\\n                \"1. Tell me your post topic or idea\\\\n\"\\r\\n                \"2. I\\'ll generate a professional post and matching image\\\\n\"\\r\\n                \"3. You can review and refine both text and image\\\\n\"\\r\\n                \"4. Once approved, I\\'ll send it via email\\\\n\\\\n\"\\r\\n                \"**What would you like to post about today?**\"\\r\\n    ).send()\\r\\n\\r\\n\\r\\n@cl.on_message\\r\\nasync def main(message: cl.Message):\\r\\n    \"\"\"Handle user messages.\"\"\"\\r\\n    session_id = cl.user_session.get(\"id\")\\r\\n    session = user_sessions.get(session_id, {})\\r\\n    \\r\\n    user_input = message.content.strip()\\r\\n    \\r\\n    if session.get(\"step\") == \"initial\":\\r\\n        # Start the workflow\\r\\n        await handle_initial_request(user_input, session)\\r\\n    else:\\r\\n        # Continue with feedback\\r\\n        await handle_feedback(user_input, session)\\r\\n\\r\\n\\r\\nasync def handle_initial_request(topic: str, session: Dict[str, Any]):\\r\\n    \"\"\"Handle the initial post creation request.\"\"\"\\r\\n    session_id = cl.user_session.get(\"id\")\\r\\n    thread_id = session[\"thread_id\"]\\r\\n    \\r\\n    # Show loading message\\r\\n    loading_msg = await cl.Message(content=\"🔄 Creating your LinkedIn post...\").send()\\r\\n    \\r\\n    try:\\r\\n        # Start the workflow\\r\\n        state = await asyncio.to_thread(start_workflow, topic, thread_id)\\r\\n        session[\"current_state\"] = state\\r\\n        \\r\\n        if state.error_message:\\r\\n            await cl.Message(content=f\"❌ **Error:** {state.error_message}\").send()\\r\\n            return\\r\\n        \\r\\n        # Show success message\\r\\n        await cl.Message(content=\"✅ **Post Created Successfully!**\").send()\\r\\n        \\r\\n        # Display the generated post\\r\\n        await cl.Message(\\r\\n            content=f\"📝 **Generated LinkedIn Post:**\\\\n\\\\n{state.post_text}\"\\r\\n        ).send()\\r\\n        \\r\\n        # Display the image if available\\r\\n        if state.image_url:\\r\\n            await cl.Message(\\r\\n                content=f\"🖼️ **Generated Image:**\\\\n\\\\n![Generated Image]({state.image_url})\"\\r\\n            ).send()\\r\\n        \\r\\n        # Ask for feedback\\r\\n        session[\"step\"] = \"feedback\"\\r\\n        await cl.Message(\\r\\n            content=\"**What would you like to do next?**\\\\n\\\\n\"\\r\\n                   \"• Type **\\'approve\\'** to send the post via email\\\\n\"\\r\\n                   \"• Type **\\'refine text [your suggestions]\\'** to improve the text\\\\n\"\\r\\n                   \"• Type **\\'refine image [your suggestions]\\'** to improve the image\\\\n\"\\r\\n                   \"• Type **\\'quit\\'** to end the session\"\\r\\n        ).send()\\r\\n        \\r\\n    except Exception as e:\\r\\n        await cl.Message(content=f\"❌ **Error:** {str(e)}\").send()\\r\\n\\r\\n\\r\\nasync def handle_feedback(feedback: str, session: Dict[str, Any]):\\r\\n    \"\"\"Handle user feedback on the generated content.\"\"\"\\r\\n    session_id = cl.user_session.get(\"id\")\\r\\n    thread_id = session[\"thread_id\"]\\r\\n    \\r\\n    if feedback.lower() in {\"quit\", \"exit\", \"q\"}:\\r\\n        await cl.Message(content=\"👋 **Thanks for using LinkedIn Booster!** Have a great day!\").send()\\r\\n        session[\"step\"] = \"completed\"\\r\\n        return\\r\\n    \\r\\n    # Show processing message\\r\\n    await cl.Message(content=\"🔄 Processing your feedback...\").send()\\r\\n    \\r\\n    try:\\r\\n        # Continue the workflow with feedback',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 2,\n",
       "   'is_chunked': True,\n",
       "   'language': 'python',\n",
       "   'chunk_size_actual': 3985},\n",
       "  {'path': 'app.py#chunk_1',\n",
       "   'original_path': 'app.py',\n",
       "   'content': 'if feedback.lower() in {\"quit\", \"exit\", \"q\"}:\\r\\n        await cl.Message(content=\"👋 **Thanks for using LinkedIn Booster!** Have a great day!\").send()\\r\\n        session[\"step\"] = \"completed\"\\r\\n        return\\r\\n    \\r\\n    # Show processing message\\r\\n    await cl.Message(content=\"🔄 Processing your feedback...\").send()\\r\\n    \\r\\n    try:\\r\\n        # Continue the workflow with feedback\\r\\n        state = await asyncio.to_thread(continue_workflow, feedback, thread_id)\\r\\n        session[\"current_state\"] = state\\r\\n        \\r\\n        if state.error_message:\\r\\n            await cl.Message(content=f\"❌ **Error:** {state.error_message}\").send()\\r\\n            return\\r\\n        \\r\\n        # Handle different workflow states\\r\\n        if state.current_step == \"completed\":\\r\\n            await cl.Message(content=\"✅ **Email sent successfully!**\").send()\\r\\n            await cl.Message(\\r\\n                content=\"🎉 **Workflow Completed!**\\\\n\\\\n\"\\r\\n                       \"Your LinkedIn post has been sent via email. \"\\r\\n                       \"You can start a new post by refreshing the page.\"\\r\\n            ).send()\\r\\n            session[\"step\"] = \"completed\"\\r\\n            \\r\\n        elif state.current_step == \"terminate\":\\r\\n            await cl.Message(content=\"🛑 **Workflow terminated.**\").send()\\r\\n            await cl.Message(\\r\\n                content=\"The workflow has been terminated. \"\\r\\n                       \"You can start a new post by refreshing the page.\"\\r\\n            ).send()\\r\\n            session[\"step\"] = \"completed\"\\r\\n            \\r\\n        elif state.current_step == \"wait_feedback\":\\r\\n            await cl.Message(content=\"✅ **Content updated!**\").send()\\r\\n            \\r\\n            # Show updated content based on what was refined\\r\\n            if \"refine text\" in feedback.lower() or \"refine_text\" in state.classification.lower():\\r\\n                await cl.Message(\\r\\n                    content=f\"📝 **Updated LinkedIn Post:**\\\\n\\\\n{state.post_text}\"\\r\\n                ).send()\\r\\n                \\r\\n            elif \"refine image\" in feedback.lower() or \"refine_image\" in state.classification.lower():\\r\\n                if state.image_url:\\r\\n                    await cl.Message(\\r\\n                        content=f\"🖼️ **Updated Image:**\\\\n\\\\n![Updated Image]({state.image_url})\"\\r\\n                    ).send()\\r\\n            \\r\\n            # Ask for next action\\r\\n            await cl.Message(\\r\\n                content=\"**What would you like to do next?**\\\\n\\\\n\"\\r\\n                       \"• Type **\\'approve\\'** to send the post via email\\\\n\"\\r\\n                       \"• Type **\\'refine text [your suggestions]\\'** to improve the text further\\\\n\"\\r\\n                       \"• Type **\\'refine image [your suggestions]\\'** to improve the image further\\\\n\"\\r\\n                       \"• Type **\\'quit\\'** to end the session\"\\r\\n            ).send()\\r\\n        \\r\\n    except Exception as e:\\r\\n        await cl.Message(content=f\"❌ **Error:** {str(e)}\").send()\\r\\n\\r\\n\\r\\n@cl.on_chat_end\\r\\nasync def end():\\r\\n    \"\"\"Clean up when chat ends.\"\"\"\\r\\n    session_id = cl.user_session.get(\"id\")\\r\\n    if session_id in user_sessions:\\r\\n        del user_sessions[session_id]',\n",
       "   'chunk_index': 1,\n",
       "   'total_chunks': 2,\n",
       "   'is_chunked': True,\n",
       "   'language': 'python',\n",
       "   'chunk_size_actual': 3085},\n",
       "  {'path': 'main.py',\n",
       "   'content': 'import sys\\r\\nfrom utils import load_env\\r\\nfrom workflow.graph import start_workflow, continue_workflow\\r\\n\\r\\n\\r\\ndef main() -> None:\\r\\n    load_env()\\r\\n    if len(sys.argv) < 2:\\r\\n        print(\"Usage: python main.py \\\\\"Your post topic...\\\\\"\")\\r\\n        sys.exit(1)\\r\\n    topic = sys.argv[1]\\r\\n    thread_id = \"cli\"\\r\\n    state = start_workflow(topic, thread_id)\\r\\n    if state.current_step != \"wait_feedback\":\\r\\n        print(f\"Error: {state.error_message}\")\\r\\n        sys.exit(1)\\r\\n    print(\"\\\\nGenerated Post:\\\\n\")\\r\\n    print(state.post_text)\\r\\n    print(\"\\\\nImage URL:\\\\n\")\\r\\n    print(state.image_url)\\r\\n    while True:\\r\\n        feedback = input(\"\\\\nFeedback (approve/refine text .../refine image .../quit): \").strip()\\r\\n        if feedback.lower() in {\"quit\", \"exit\", \"q\"}:\\r\\n            print(\"Goodbye!\")\\r\\n            break\\r\\n        state = continue_workflow(feedback, thread_id)\\r\\n        if state.current_step == \"completed\":\\r\\n            print(\"Email sent. Workflow completed.\")\\r\\n            break\\r\\n        if state.current_step == \"terminate\":\\r\\n            print(\"Workflow terminated.\")\\r\\n            break\\r\\n        if state.current_step == \"wait_feedback\":\\r\\n            if state.classification == \"Refine Text\":\\r\\n                print(\"\\\\nUpdated Post:\\\\n\")\\r\\n                print(state.post_text)\\r\\n            elif state.classification == \"Refine Image\":\\r\\n                print(\"\\\\nUpdated Image URL:\\\\n\")\\r\\n                print(state.image_url)\\r\\n\\r\\n\\r\\nif __name__ == \"__main__\":\\r\\n    main()\\r\\n\\r\\n\\r\\n',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'python'},\n",
       "  {'path': 'requirements.txt',\n",
       "   'content': 'chainlit>=1.0.0\\r\\nlangchain>=0.1.0\\r\\nlangchain-community>=0.1.0\\r\\nlangchain-google-genai>=1.0.0\\r\\nlangchain-google-community>=0.1.0\\r\\nlanggraph>=0.0.40\\r\\nrequests>=2.31.0\\r\\ntyping-extensions>=4.5.0\\r\\npython-dotenv>=1.0.0\\r\\ngoogle-auth>=2.0.0\\r\\ngoogle-auth-oauthlib>=1.0.0\\r\\ngoogle-auth-httplib2>=0.2.0\\r\\ngoogle-api-python-client>=2.0.0\\r\\nopenai>=1.0.0\\r\\ntavily-python>=0.3.0',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'unknown'},\n",
       "  {'path': 'notebooks/notebooks.md',\n",
       "   'content': '',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'markdown'},\n",
       "  {'path': 'studio/graph.py',\n",
       "   'content': '\"\"\"Standalone graph file for LangGraph Studio.\"\"\"\\r\\nimport sys\\r\\nimport os\\r\\nfrom typing import Optional\\r\\nfrom dataclasses import dataclass\\r\\n\\r\\n# Add the parent directory to the path so we can import from workflow\\r\\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\\r\\n\\r\\nfrom langgraph.graph import StateGraph, END\\r\\n\\r\\n# Define state directly to avoid import conflicts\\r\\n@dataclass\\r\\nclass WorkflowState:\\r\\n    \"\"\"State management for the LinkedIn post creation workflow.\"\"\"\\r\\n    user_input: str = \"\"\\r\\n    post_text: str = \"\"\\r\\n    image_prompt: str = \"\"\\r\\n    image_data: Optional[bytes] = None\\r\\n    image_url: str = \"\"\\r\\n    user_feedback: str = \"\"\\r\\n    classification: str = \"\"\\r\\n    refined_text: str = \"\"\\r\\n    refined_image_data: Optional[bytes] = None\\r\\n    current_step: str = \"start\"\\r\\n    error_message: str = \"\"\\r\\n    final_post_ready: bool = False\\r\\n\\r\\n# Import nodes after defining state\\r\\ntry:\\r\\n    from workflow.nodes import (\\r\\n        create_post_text,\\r\\n        generate_image,\\r\\n        classify_feedback,\\r\\n        refine_text,\\r\\n        refine_image,\\r\\n        send_email,\\r\\n    )\\r\\nexcept ImportError as e:\\r\\n    # Fallback dummy functions for studio\\r\\n    def create_post_text(state: WorkflowState) -> WorkflowState:\\r\\n        state.post_text = \"Sample LinkedIn post created\"\\r\\n        return state\\r\\n    \\r\\n    def generate_image(state: WorkflowState) -> WorkflowState:\\r\\n        state.image_url = \"https://example.com/image.jpg\"\\r\\n        return state\\r\\n    \\r\\n    def classify_feedback(state: WorkflowState) -> WorkflowState:\\r\\n        state.classification = \"approved\"\\r\\n        return state\\r\\n    \\r\\n    def refine_text(state: WorkflowState) -> WorkflowState:\\r\\n        state.refined_text = \"Refined post text\"\\r\\n        return state\\r\\n    \\r\\n    def refine_image(state: WorkflowState) -> WorkflowState:\\r\\n        state.refined_image_data = b\"refined_image_data\"\\r\\n        return state\\r\\n    \\r\\n    def send_email(state: WorkflowState) -> WorkflowState:\\r\\n        state.final_post_ready = True\\r\\n        return state\\r\\n\\r\\n\\r\\ndef route_feedback(state: WorkflowState) -> str:\\r\\n    \"\"\"Route based on feedback classification.\"\"\"\\r\\n    classification = state.classification.lower()\\r\\n    if \"approved\" in classification:\\r\\n        return \"approved\"\\r\\n    if \"refine text\" in classification or \"refine_text\" in classification:\\r\\n        return \"refine_text\"\\r\\n    if \"refine image\" in classification or \"refine_image\" in classification:\\r\\n        return \"refine_image\"\\r\\n    if \"terminate\" in classification:\\r\\n        return \"terminate\"\\r\\n    return \"terminate\"\\r\\n\\r\\n\\r\\ndef create_studio_workflow():\\r\\n    \"\"\"Create the LangGraph workflow for Studio (without checkpointer).\"\"\"\\r\\n    workflow = StateGraph(WorkflowState)\\r\\n    workflow.add_node(\"create_post\", create_post_text)\\r\\n    workflow.add_node(\"generate_image\", generate_image)\\r\\n    workflow.add_node(\"classify_feedback\", classify_feedback)\\r\\n    workflow.add_node(\"refine_text\", refine_text)\\r\\n    workflow.add_node(\"refine_image\", refine_image)\\r\\n    workflow.add_node(\"send_email\", send_email)\\r\\n    workflow.set_entry_point(\"create_post\")\\r\\n    workflow.add_edge(\"create_post\", \"generate_image\")\\r\\n    workflow.add_edge(\"generate_image\", \"classify_feedback\")\\r\\n    workflow.add_conditional_edges(\\r\\n        \"classify_feedback\",\\r\\n        route_feedback,\\r\\n        {\\r\\n            \"approved\": \"send_email\",\\r\\n            \"refine_text\": \"refine_text\",\\r\\n            \"refine_image\": \"refine_image\",\\r\\n            \"terminate\": END,\\r\\n        },\\r\\n    )\\r\\n    workflow.add_edge(\"refine_text\", \"classify_feedback\")\\r\\n    workflow.add_edge(\"refine_image\", \"classify_feedback\")\\r\\n    workflow.add_edge(\"send_email\", END)\\r\\n    \\r\\n    # Compile without checkpointer for LangGraph Studio\\r\\n    app = workflow.compile(\\r\\n        interrupt_after=[\"generate_image\", \"refine_text\", \"refine_image\"],\\r\\n    )\\r\\n    return app\\r\\n\\r\\n\\r\\n# Create the app for LangGraph Studio\\r\\napp = create_studio_workflow()\\r\\n\\r\\n# Export the app for LangGraph Studio\\r\\n__all__ = [\"app\"]',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'python'},\n",
       "  {'path': 'studio/langgraph.json',\n",
       "   'content': '{\\r\\n    \"dockerfile_lines\": [],\\r\\n    \"graphs\": {\\r\\n      \"chatbot\": \"./graph.py:app\"\\r\\n    },\\r\\n    \"env\": \"./.env\",\\r\\n    \"python_version\": \"3.11\",\\r\\n    \"dependencies\": [\\r\\n      \".\"\\r\\n    ]\\r\\n  }',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'unknown'},\n",
       "  {'path': 'utils/__init__.py',\n",
       "   'content': 'from .config import load_env\\r\\n\\r\\n__all__ = [\"load_env\"]\\r\\n\\r\\n\\r\\n',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'python'},\n",
       "  {'path': 'utils/config.py',\n",
       "   'content': 'import os\\r\\nfrom dataclasses import dataclass\\r\\nfrom dotenv import load_dotenv\\r\\n\\r\\n\\r\\ndef load_env() -> None:\\r\\n    \"\"\"Load environment variables from a .env file if present.\"\"\"\\r\\n    load_dotenv()\\r\\n\\r\\n\\r\\n@dataclass\\r\\nclass Config:\\r\\n    OPENAI_API_KEY: str | None = os.getenv(\"OPENAI_API_KEY\")\\r\\n    GOOGLE_API_KEY: str | None = os.getenv(\"GOOGLE_API_KEY\")\\r\\n    TAVILY_API_KEY: str | None = os.getenv(\"TAVILY_API_KEY\")\\r\\n    GMAIL_CREDENTIALS_FILE: str = os.getenv(\"GMAIL_CREDENTIALS_FILE\", \"credentials.json\")\\r\\n    GMAIL_TOKEN_FILE: str = os.getenv(\"GMAIL_TOKEN_FILE\", \"token.json\")\\r\\n    RECIPIENT_EMAIL: str = os.getenv(\"RECIPIENT_EMAIL\", \"mohammed-mowina@outlook.com\")\\r\\n\\r\\n\\r\\n',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'python'},\n",
       "  {'path': 'workflow/__init__.py',\n",
       "   'content': '\"\"\"LinkedIn Booster workflow package.\"\"\"\\r\\n\\r\\nfrom .graph import app, create_workflow, start_workflow, continue_workflow\\r\\n\\r\\n__all__ = [\"app\", \"create_workflow\", \"start_workflow\", \"continue_workflow\"]',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'python'},\n",
       "  {'path': 'workflow/graph.py',\n",
       "   'content': 'from langgraph.graph import StateGraph, END\\r\\nfrom langgraph.checkpoint.memory import MemorySaver\\r\\n\\r\\nfrom workflow.state import WorkflowState\\r\\nfrom workflow.nodes import (\\r\\n    create_post_text,\\r\\n    generate_image,\\r\\n    classify_feedback,\\r\\n    refine_text,\\r\\n    refine_image,\\r\\n    send_email,\\r\\n)\\r\\n\\r\\n\\r\\ndef route_feedback(state: WorkflowState) -> str:\\r\\n    \"\"\"Route based on feedback classification.\"\"\"\\r\\n    classification = state.classification.lower()\\r\\n    if \"approved\" in classification:\\r\\n        return \"approved\"\\r\\n    if \"refine text\" in classification or \"refine_text\" in classification:\\r\\n        return \"refine_text\"\\r\\n    if \"refine image\" in classification or \"refine_image\" in classification:\\r\\n        return \"refine_image\"\\r\\n    if \"terminate\" in classification:\\r\\n        return \"terminate\"\\r\\n    return \"terminate\"\\r\\n\\r\\n\\r\\ndef create_workflow():\\r\\n    \"\"\"Create the LangGraph workflow with interrupts configured.\"\"\"\\r\\n    workflow = StateGraph(WorkflowState)\\r\\n    workflow.add_node(\"create_post\", create_post_text)\\r\\n    workflow.add_node(\"generate_image\", generate_image)\\r\\n    workflow.add_node(\"classify_feedback\", classify_feedback)\\r\\n    workflow.add_node(\"refine_text\", refine_text)\\r\\n    workflow.add_node(\"refine_image\", refine_image)\\r\\n    workflow.add_node(\"send_email\", send_email)\\r\\n    workflow.set_entry_point(\"create_post\")\\r\\n    workflow.add_edge(\"create_post\", \"generate_image\")\\r\\n    workflow.add_edge(\"generate_image\", \"classify_feedback\")\\r\\n    workflow.add_conditional_edges(\\r\\n        \"classify_feedback\",\\r\\n        route_feedback,\\r\\n        {\\r\\n            \"approved\": \"send_email\",\\r\\n            \"refine_text\": \"refine_text\",\\r\\n            \"refine_image\": \"refine_image\",\\r\\n            \"terminate\": END,\\r\\n        },\\r\\n    )\\r\\n    workflow.add_edge(\"refine_text\", \"classify_feedback\")\\r\\n    workflow.add_edge(\"refine_image\", \"classify_feedback\")\\r\\n    workflow.add_edge(\"send_email\", END)\\r\\n    memory = MemorySaver()\\r\\n    app = workflow.compile(\\r\\n        checkpointer=memory,\\r\\n        interrupt_after=[\"generate_image\", \"refine_text\", \"refine_image\"],\\r\\n    )\\r\\n    return app\\r\\n\\r\\n\\r\\n# Convenience API mirroring notebook helpers\\r\\napp = create_workflow()\\r\\n\\r\\n\\r\\ndef start_workflow(user_input: str, thread_id: str = \"default\") -> WorkflowState:\\r\\n    initial_state = WorkflowState(user_input=user_input)\\r\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\r\\n    result = app.invoke(initial_state, config)\\r\\n    return WorkflowState(**result)\\r\\n\\r\\n\\r\\ndef continue_workflow(user_feedback: str, thread_id: str = \"default\") -> WorkflowState:\\r\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\r\\n    app.update_state(config, {\"user_feedback\": user_feedback})\\r\\n    result = app.invoke(None, config)\\r\\n    return WorkflowState(**result)\\r\\n\\r\\n\\r\\n',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'python'},\n",
       "  {'path': 'workflow/nodes.py#chunk_0',\n",
       "   'original_path': 'workflow/nodes.py',\n",
       "   'content': 'import os\\r\\nfrom typing import List\\r\\n\\r\\nfrom dotenv import load_dotenv\\r\\nload_dotenv()\\r\\n\\r\\nfrom langchain.chat_models import init_chat_model\\r\\nfrom langchain.schema import SystemMessage, HumanMessage\\r\\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\\r\\nfrom langgraph.errors import GraphRecursionError\\r\\nfrom langgraph.prebuilt import create_react_agent\\r\\n# Gmail imports will be done lazily in send_email function\\r\\n\\r\\nfrom workflow.state import WorkflowState\\r\\nfrom workflow.tools import tavily_tool\\r\\n\\r\\n\\r\\n# Initialize models\\r\\ngemini_model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\\r\\n\\r\\ndef create_post_text(state: WorkflowState) -> WorkflowState:\\r\\n    \"\"\"Generate LinkedIn post text based on user input\"\"\"\\r\\n    system_message = \"\"\"\\r\\n    You are a professional LinkedIn content creator. Create engaging, professional LinkedIn posts.\\r\\n\\r\\n    Your tone should be:\\r\\n    - Professional yet approachable\\r\\n    - Concise and clear\\r\\n    - Action-oriented with clear value\\r\\n\\r\\n    Format your post with:\\r\\n    - A strong hook in the first sentence\\r\\n    - Short, scannable paragraphs\\r\\n    - 1-3 relevant emojis strategically placed\\r\\n    - A clear call-to-action\\r\\n    - 3-5 relevant hashtags at the end\\r\\n\\r\\n    Output ONLY the post text, nothing else.\"\"\"\\r\\n    \\r\\n    try:\\r\\n        max_iterations = 3\\r\\n        recursion_limit = 2 * max_iterations + 1\\r\\n        \\r\\n        agent = create_react_agent(\\r\\n            model=\"openai:gpt-4o-mini\",\\r\\n            tools=[tavily_tool],\\r\\n            prompt=system_message,\\r\\n            debug=True\\r\\n        )\\r\\n        \\r\\n        try:\\r\\n            response = agent.invoke(\\r\\n                {\"messages\": [{\"role\": \"user\", \"content\": state.user_input}]},\\r\\n                {\"recursion_limit\": recursion_limit},\\r\\n            )\\r\\n            \\r\\n            state.post_text = response[\\'messages\\'][-1].content\\r\\n            state.current_step = \"create_image\"\\r\\n            \\r\\n        except GraphRecursionError:\\r\\n            print(\"Agent stopped due to max iterations.\")\\r\\n            state.error_message = \"Agent stopped due to max iterations\"\\r\\n            state.current_step = \"error\"\\r\\n        \\r\\n    except Exception as e:\\r\\n        state.error_message = f\"Error creating post text: {str(e)}\"\\r\\n        state.current_step = \"error\"\\r\\n    \\r\\n    return state',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 3,\n",
       "   'is_chunked': True,\n",
       "   'language': 'python',\n",
       "   'chunk_size_actual': 2293},\n",
       "  {'path': 'workflow/nodes.py#chunk_1',\n",
       "   'original_path': 'workflow/nodes.py',\n",
       "   'content': 'def generate_image(state: WorkflowState) -> WorkflowState:\\r\\n    \"\"\"Generate image directly from post text\"\"\"\\r\\n    \\r\\n    try:\\r\\n        post_text = state.post_text\\r\\n        \\r\\n        # Create a proper prompt for image generation\\r\\n        image_prompt_template = \"\"\"\\r\\n        Create a modern, minimalistic professional LinkedIn illustration for the following post: {post_content}\\r\\n        \\r\\n        Style: 3D Object Generator with clean flat vector, LinkedIn color palette (blue, white, grey), simple, corporate-appropriate.\\r\\n        Avoid clutter, make it inspiring and easy to understand.\\r\\n        No text in the image, just visual elements.\\r\\n        \\r\\n        Generate only the image description prompt, no other text.\\r\\n        \"\"\"\\r\\n        \\r\\n        # Create messages for the LLM\\r\\n        messages = [\\r\\n            SystemMessage(content=\"You are an expert LinkedIn Image Prompt Engineer. Generate concise AI image prompts.\"),\\r\\n            HumanMessage(content=image_prompt_template.format(post_content=post_text))\\r\\n        ]\\r\\n        \\r\\n        # Get the image prompt from Gemini\\r\\n        response = gemini_model.invoke(messages)\\r\\n        image_prompt = response.content.strip()\\r\\n        \\r\\n        print(f\"Image prompt: {image_prompt}\")\\r\\n        \\r\\n        # Generate image using DALL-E\\r\\n        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\\r\\n        image_url = dalle_wrapper.run(image_prompt)\\r\\n        \\r\\n        # Update state\\r\\n        state.image_url = image_url\\r\\n        state.image_prompt = image_prompt\\r\\n        state.current_step = \"wait_feedback\" \\r\\n        \\r\\n    except Exception as e:\\r\\n        state.error_message = f\"Error generating image: {str(e)}\"\\r\\n        state.current_step = \"error\"\\r\\n    \\r\\n    return state\\r\\n\\r\\n\\r\\ndef classify_feedback(state: WorkflowState) -> WorkflowState:\\r\\n    \"\"\"Classify user feedback into categories\"\"\"\\r\\n    system_message = \"\"\"You are a text classifier. Classify the input text into one of these categories:\\r\\n\\r\\n        1. Approved - Approval message refers that its ok to proceed with the current status.\\r\\n        2. Refine Text - Request to refine the post text\\r\\n        3. Terminate - Termination request for the process.\\r\\n        4. Refine Image - Request to refine the post image\\r\\n\\r\\n        Output only the category name, nothing else.\"\"\"\\r\\n\\r\\n    try:\\r\\n        messages = [\\r\\n            SystemMessage(content=system_message),\\r\\n            HumanMessage(content=state.user_feedback)\\r\\n        ]\\r\\n        \\r\\n        response = gemini_model.invoke(messages)\\r\\n        classification = response.content.strip()\\r\\n        \\r\\n        # Map classification to next step\\r\\n        if \"Approved\" in classification:\\r\\n            state.classification = \"Approved\"\\r\\n            state.current_step = \"post_content\"\\r\\n            state.final_post_ready = True\\r\\n        elif \"Refine Text\" in classification:\\r\\n            state.classification = \"Refine Text\"\\r\\n            state.current_step = \"refine_text\"\\r\\n        elif \"Terminate\" in classification:\\r\\n            state.classification = \"Terminate\"\\r\\n            state.current_step = \"terminate\"\\r\\n        elif \"Refine Image\" in classification:\\r\\n            state.classification = \"Refine Image\"\\r\\n            state.current_step = \"refine_image\"\\r\\n        else:\\r\\n            state.classification = \"Unknown\"\\r\\n            state.current_step = \"wait_feedback\"\\r\\n            state.error_message = f\"Unrecognized feedback classification: {classification}\"\\r\\n        \\r\\n        print(f\"DEBUG: Updated state - classification: {state.classification}, current_step: {state.current_step}\")\\r\\n            \\r\\n    except Exception as e:\\r\\n        state.error_message = f\"Error classifying feedback: {str(e)}\"\\r\\n        state.current_step = \"error\"\\r\\n        print(f\"DEBUG: Exception in classify_feedback: {str(e)}\")\\r\\n    \\r\\n    return state',\n",
       "   'chunk_index': 1,\n",
       "   'total_chunks': 3,\n",
       "   'is_chunked': True,\n",
       "   'language': 'python',\n",
       "   'chunk_size_actual': 3785},\n",
       "  {'path': 'workflow/nodes.py#chunk_2',\n",
       "   'original_path': 'workflow/nodes.py',\n",
       "   'content': 'def refine_text(state: WorkflowState) -> WorkflowState:\\r\\n    \"\"\"Refine the post text based on user feedback\"\"\"\\r\\n    system_message = \"\"\"\\r\\n        You are a helpful assistant who improve and refine LinkedIn post.\\r\\n\\r\\n        Output only the post text.\"\"\"\\r\\n\\r\\n    try:\\r\\n        prompt = f\"Original post: {state.post_text}\\\\\\\\n\\\\\\\\nRequest: {state.user_feedback}\"\\r\\n        \\r\\n        messages = [\\r\\n            SystemMessage(content=system_message),\\r\\n            HumanMessage(content=prompt)\\r\\n        ]\\r\\n        \\r\\n        response = gemini_model.invoke(messages)\\r\\n        state.refined_text = response.content\\r\\n        state.post_text = state.refined_text  # Update the main post text\\r\\n        state.current_step = \"wait_feedback\"\\r\\n        \\r\\n    except Exception as e:\\r\\n        state.error_message = f\"Error refining text: {str(e)}\"\\r\\n        state.current_step = \"error\"\\r\\n    \\r\\n    return state\\r\\n\\r\\n\\r\\ndef refine_image(state: WorkflowState) -> WorkflowState:\\r\\n    \"\"\"Refine the image based on user feedback\"\"\"\\r\\n    try:\\r\\n        # Use the existing image prompt and add user feedback\\r\\n        modified_prompt = f\"{state.image_prompt}. {state.user_feedback}\"\\r\\n        \\r\\n        print(f\"Refined image prompt: {modified_prompt}\")\\r\\n        \\r\\n        # Generate image using DALL-E with the modified prompt\\r\\n        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\\r\\n        image_url = dalle_wrapper.run(modified_prompt)\\r\\n        \\r\\n        # Update state with the refined image\\r\\n        state.image_url = image_url\\r\\n        state.image_prompt = modified_prompt  # Update the prompt with feedback\\r\\n        state.current_step = \"wait_feedback\"  # Go back to wait for more feedback\\r\\n        \\r\\n        print(f\"Refined image URL: {image_url}\")\\r\\n        \\r\\n    except Exception as e:\\r\\n        state.error_message = f\"Error refining image: {str(e)}\"\\r\\n        state.current_step = \"error\"\\r\\n    \\r\\n    return state\\r\\n\\r\\n\\r\\ndef send_email(state: WorkflowState) -> WorkflowState:\\r\\n    \"\"\"Send email with the final post content\"\"\"\\r\\n    try:\\r\\n        # Lazy import Gmail functionality\\r\\n        from langchain_google_community import GmailToolkit\\r\\n        from langchain_google_community.gmail.utils import (\\r\\n            build_resource_service,\\r\\n            get_gmail_credentials,\\r\\n        )\\r\\n\\r\\n        # Test imports without initializing toolkit\\r\\n        print(\"Imports successful!\")\\r\\n\\r\\n            \\r\\n        # Initialize Gmail toolkit\\r\\n        credentials = get_gmail_credentials(\\r\\n            token_file=\"token.json\",\\r\\n            scopes=[\"https://mail.google.com/\"],\\r\\n            client_secrets_file=\"credentials.json\",\\r\\n        )\\r\\n        \\r\\n        api_resource = build_resource_service(credentials=credentials)\\r\\n        gmail_toolkit = GmailToolkit(api_resource=api_resource)\\r\\n        \\r\\n        tools = gmail_toolkit.get_tools()\\r\\n        print(f\\'🚩DEBUG: send mail tool: {tools[1]}\\')\\r\\n        # Compose email content\\r\\n        subject = \"LinkedIn Post Ready for Publishing\"\\r\\n        body = f\"\"\"\\r\\n            Your LinkedIn post is ready:\\r\\n\\r\\n            {state.post_text}\\r\\n\\r\\n            Image URL: {state.image_url}\\r\\n\\r\\n            Best regards,\\r\\n            LinkedIn Post Creator\\r\\n            \"\"\"\\r\\n        \\r\\n        # Send email using Gmail toolkit\\r\\n        send_message_tool = gmail_toolkit.get_tools()[1]  # send_gmail_message tool\\r\\n        \\r\\n        email_result = send_message_tool.run({\\r\\n            \"to\": \"mohammed-mowina@outlook.com\",\\r\\n            \"subject\": subject,\\r\\n            \"message\": body\\r\\n        })\\r\\n        \\r\\n        state.current_step = \"completed\"\\r\\n        \\r\\n    except Exception as e:\\r\\n        state.error_message = f\"🚩Error sending email: {str(e)}\"\\r\\n        state.current_step = \"error\"\\r\\n    \\r\\n    return state',\n",
       "   'chunk_index': 2,\n",
       "   'total_chunks': 3,\n",
       "   'is_chunked': True,\n",
       "   'language': 'python',\n",
       "   'chunk_size_actual': 3697},\n",
       "  {'path': 'workflow/state.py',\n",
       "   'content': 'from dataclasses import dataclass\\r\\nfrom typing import Optional\\r\\n\\r\\n\\r\\n@dataclass\\r\\nclass WorkflowState:\\r\\n    \"\"\"State management for the LinkedIn post creation workflow.\"\"\"\\r\\n    user_input: str = \"\"\\r\\n    post_text: str = \"\"\\r\\n    image_prompt: str = \"\"\\r\\n    image_data: Optional[bytes] = None\\r\\n    image_url: str = \"\"\\r\\n    user_feedback: str = \"\"\\r\\n    classification: str = \"\"\\r\\n    refined_text: str = \"\"\\r\\n    refined_image_data: Optional[bytes] = None\\r\\n    current_step: str = \"start\"\\r\\n    error_message: str = \"\"\\r\\n    final_post_ready: bool = False\\r\\n\\r\\n\\r\\n',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'python'},\n",
       "  {'path': 'workflow/tools.py',\n",
       "   'content': 'import os\\r\\nimport requests\\r\\nfrom typing import Any, Dict\\r\\n\\r\\nfrom dotenv import load_dotenv\\r\\nload_dotenv()\\r\\n\\r\\nfrom langchain.tools import Tool\\r\\nfrom langchain_core.tools import tool\\r\\n\\r\\ndef tavily_search(query: str) -> str:\\r\\n    \"\"\"Search the web using Tavily API\"\"\"\\r\\n    try:\\r\\n        url = \"https://api.tavily.com/search\"\\r\\n        headers = {\"Authorization\": f\"Bearer {Config.TAVILY_API_KEY}\"}\\r\\n        data = {\"query\": query}\\r\\n        \\r\\n        response = requests.post(url, headers=headers, json=data)\\r\\n        response.raise_for_status()\\r\\n        \\r\\n        results = response.json()\\r\\n        # Extract relevant information from search results\\r\\n        search_results = []\\r\\n        for result in results.get(\"results\", [])[:3]:  # Top 3 results\\r\\n            search_results.append(f\"Title: {result.get(\\'title\\', \\'\\')}\\\\nContent: {result.get(\\'content\\', \\'\\')[:200]}...\")\\r\\n        \\r\\n        return \"\\\\n\\\\n\".join(search_results)\\r\\n    except Exception as e:\\r\\n        return f\"Search failed: {str(e)}\"\\r\\n\\r\\n\\r\\n# Create Tavily search tool\\r\\ntavily_tool = Tool(\\r\\n    name=\"web_search\",\\r\\n    description=\"Search the web for current information\",\\r\\n    func=tavily_search\\r\\n)\\r\\n',\n",
       "   'chunk_index': 0,\n",
       "   'total_chunks': 1,\n",
       "   'is_chunked': False,\n",
       "   'language': 'python'}],\n",
       " 'chunking_info': {'chunk_size': 4000,\n",
       "  'chunk_overlap': 400,\n",
       "  'total_chunks': 18,\n",
       "  'files_chunked': 3,\n",
       "  'files_not_chunked': 11},\n",
       " 'source_type': 'azure_devops',\n",
       " 'organization': 'areebgroup',\n",
       " 'project': 'Internship-Playground',\n",
       " 'azure_repo_name': 'Internship-ai'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure_repo_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup SupaBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vector = embeddings.embed_query(\"Hello, world!\")\n",
    "len(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'postgresql://postgres.xrnjnxbdriboxwfoyrlx:areebrepoassistant@aws-1-eu-north-1.pooler.supabase.com:6543/postgres'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "os.getenv(\"SUPABASE_DB_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vecs\n",
    "import json  # If needed for serializing branches\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Load the embedding model (only do this once)\n",
    "model = embeddings\n",
    "\n",
    "# Connect to Supabase (replace with your actual connection string)\n",
    "SUPABASE_DB_URL = os.getenv(\"SUPABASE_DB_URL\")\n",
    "vx = vecs.create_client(SUPABASE_DB_URL)\n",
    "\n",
    "# Create or get the vector collection (name it whatever you like, e.g., \"repo_files\")\n",
    "# Dimension must match the model's output (384 for 'supabase/gte-small')\n",
    "collection = vx.get_or_create_collection(name=\"repo_files\", dimension=1536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[vecs.Collection(name=\"LinkedIn-Booster\", dimension=1536),\n",
       " vecs.Collection(name=\"TalentTalk---AI-powered-interview-system\", dimension=1536),\n",
       " vecs.Collection(name=\"repo_files\", dimension=1536)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vx.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_repo_in_vector_db(repo_info):\n",
    "    vectors_to_upsert = []\n",
    "    \n",
    "    for file in repo_info['files']:\n",
    "        # Generate embedding for the file content\n",
    "        content = file['content']\n",
    "        embedding = model.embed_query(content)\n",
    "        \n",
    "        # Create a unique ID (e.g., \"repo_name/file_path\")\n",
    "        unique_id = f\"{repo_info['repo_name']}/{file['path']}\"\n",
    "        \n",
    "        # Metadata: Store repo-level info here for each file\n",
    "        metadata = {\n",
    "            'repo_name': repo_info['repo_name'],\n",
    "            'path': file['path'],\n",
    "            'content': content,  # Store the full content for retrieval\n",
    "            'commit_count': repo_info['commit_count'],\n",
    "            'branches': repo_info['branches']  # List is JSON-serializable\n",
    "        }\n",
    "        \n",
    "        # Add to batch\n",
    "        vectors_to_upsert.append((unique_id, embedding, metadata))\n",
    "    \n",
    "    # Fix: Remove the 'vectors=' keyword argument\n",
    "    collection.upsert(vectors_to_upsert)\n",
    "    print(f\"Stored {len(vectors_to_upsert)} files from repo '{repo_info['repo_name']}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 14 files from repo 'tmpqv23rcta'\n"
     ]
    }
   ],
   "source": [
    "# Example usage with your repo_info\n",
    "store_repo_in_vector_db(repo_info)\n",
    "\n",
    "# Don't forget to disconnect when done\n",
    "vx.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection name: repo_files\n",
      "Collection dimension: 1536\n",
      "Available methods:\n",
      "['adapter', 'client', 'create_index', 'delete', 'dimension', 'fetch', 'index', 'is_indexed_for_measure', 'name', 'query', 'table', 'upsert']\n"
     ]
    }
   ],
   "source": [
    "# intialize connection with the Database\n",
    "vx = vecs.create_client(SUPABASE_DB_URL)\n",
    "\n",
    "# View the scheme of the collection\n",
    "collection = vx.get_or_create_collection(name=\"repo_files\", dimension=1536)\n",
    "\n",
    "# Get collection info\n",
    "print(f\"Collection name: {collection.name}\")\n",
    "print(f\"Collection dimension: {collection.dimension}\")\n",
    "\n",
    "# List all available methods and attributes\n",
    "print(\"Available methods:\")\n",
    "print([method for method in dir(collection) if not method.startswith('_')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vx.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supabase utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import vecs\n",
    "import json  # If needed for serializing branches\n",
    "load_dotenv()\n",
    "\n",
    "# Connect to Supabase (replace with your actual connection string)\n",
    "SUPABASE_DB_URL = os.getenv(\"SUPABASE_DB_URL\")\n",
    "vx = vecs.create_client(SUPABASE_DB_URL)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Load the embedding model (only do this once)\n",
    "model = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_repo_in_own_collection(repo_info, refresh=False):\n",
    "    repo_name = repo_info['repo_name']\n",
    "\n",
    "    if refresh:\n",
    "        # Drop existing collection if it exists\n",
    "        try:\n",
    "            vx.delete_collection(repo_name)\n",
    "            print(f\"🗑️ Old collection '{repo_name}' deleted.\")\n",
    "        except Exception:\n",
    "            pass  # collection may not exist yet\n",
    "\n",
    "    # Create/get a dedicated collection for this repo\n",
    "    collection = vx.get_or_create_collection(\n",
    "        name=repo_name,\n",
    "        dimension=1536\n",
    "    )\n",
    "\n",
    "    vectors_to_upsert = []\n",
    "    for file in repo_info['files']:\n",
    "        content = file['content']\n",
    "        embedding = model.embed_query(content)\n",
    "\n",
    "        unique_id = f\"{repo_name}/{file['path']}\"\n",
    "        metadata = {\n",
    "            'repo_name': repo_name,\n",
    "            'path': file['path'],\n",
    "            'commit_count': repo_info['commit_count'],\n",
    "            'branches': repo_info['branches']\n",
    "        }\n",
    "\n",
    "        vectors_to_upsert.append((unique_id, embedding, metadata))\n",
    "\n",
    "    collection.upsert(vectors_to_upsert)\n",
    "    print(f\"✅ Stored {len(vectors_to_upsert)} files into collection '{repo_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stored 14 files into collection 'LinkedIn-Booster'\n"
     ]
    }
   ],
   "source": [
    "store_repo_in_own_collection(repo_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[vecs.Collection(name=\"LinkedIn-Booster\", dimension=1536),\n",
       " vecs.Collection(name=\"TalentTalk---AI-powered-interview-system\", dimension=1536),\n",
       " vecs.Collection(name=\"repo_files\", dimension=1536)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vx.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohamed mowina\\Desktop\\Areeb\\DevOps-Analysis\\.venv\\lib\\site-packages\\vecs\\collection.py:506: UserWarning: Query does not have a covering index for cosine_distance. See Collection.create_index\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('LinkedIn-Booster/workflow/__init__.py', 0.604595542638825, {'path': 'workflow/__init__.py', 'branches': ['main'], 'repo_name': 'LinkedIn-Booster', 'commit_count': 6}),\n",
       " ('LinkedIn-Booster/workflow/graph.py', 0.617298291051589, {'path': 'workflow/graph.py', 'branches': ['main'], 'repo_name': 'LinkedIn-Booster', 'commit_count': 6}),\n",
       " ('LinkedIn-Booster/studio/graph.py', 0.649370674582624, {'path': 'studio/graph.py', 'branches': ['main'], 'repo_name': 'LinkedIn-Booster', 'commit_count': 6}),\n",
       " ('LinkedIn-Booster/main.py', 0.651415278059699, {'path': 'main.py', 'branches': ['main'], 'repo_name': 'LinkedIn-Booster', 'commit_count': 6}),\n",
       " ('LinkedIn-Booster/workflow/nodes.py', 0.658244322747666, {'path': 'workflow/nodes.py', 'branches': ['main'], 'repo_name': 'LinkedIn-Booster', 'commit_count': 6})]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = vx.get_or_create_collection(name=\"LinkedIn-Booster\", dimension=1536)\n",
    "\n",
    "query = model.embed_query(\"Workflow\")\n",
    "\n",
    "results = collection.query(\n",
    "    data=query,\n",
    "    limit=5,\n",
    "    include_value=True,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'workflow/__init__.py',\n",
       " 'branches': ['main'],\n",
       " 'repo_name': 'LinkedIn-Booster',\n",
       " 'commit_count': 6}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Field       | Description                                                 |\n",
    "| ----------- | ----------------------------------------------------------- |\n",
    "| `id`        | Unique identifier for the chunk/file                        |\n",
    "| `repo_id`   | Which repo/project this belongs to                          |\n",
    "| `file_path` | Path to the source file                                     |\n",
    "| `chunk_id`  | If the file is split, ID for the chunk                      |\n",
    "| `content`   | Raw code text (or chunk of it)                              |\n",
    "| `embedding` | Vector representation of the content                        |\n",
    "| `symbols`   | Extracted functions, classes, variables (for faster lookup) |\n",
    "| `imports`   | Which files/functions this chunk depends on                 |\n",
    "| `metadata`  | Language, size, timestamps, etc.                            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbols extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree_sitter_python as tspython\n",
    "from tree_sitter import Language, Parser\n",
    "\n",
    "def extract_python_symbols(code):\n",
    "    PY_LANGUAGE = Language(tspython.language())\n",
    "    parser = Parser(PY_LANGUAGE)\n",
    "    tree = parser.parse(bytes(code, \"utf8\"))\n",
    "    \n",
    "    symbols = {\n",
    "        'imports': [],\n",
    "        'functions': [],\n",
    "        'classes': [],\n",
    "        'variables': []\n",
    "    }\n",
    "    \n",
    "    def traverse_tree(node):\n",
    "        if node.type == 'import_statement':\n",
    "            symbols['imports'].append(node.text.decode('utf8'))\n",
    "        elif node.type == 'function_definition':\n",
    "            name_node = node.child_by_field_name('name')\n",
    "            if name_node:\n",
    "                symbols['functions'].append(name_node.text.decode('utf8'))\n",
    "        elif node.type == 'class_definition':\n",
    "            name_node = node.child_by_field_name('name')\n",
    "            if name_node:\n",
    "                symbols['classes'].append(name_node.text.decode('utf8'))\n",
    "        \n",
    "        for child in node.children:\n",
    "            traverse_tree(child)\n",
    "    \n",
    "    traverse_tree(tree.root_node)\n",
    "    return symbols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imports': ['import os', 'import vecs', 'import json', 'import traceback'],\n",
       " 'functions': ['get_embedding_model',\n",
       "  'get_vector_client',\n",
       "  'store_repo_in_own_collection',\n",
       "  'search_repo_collection',\n",
       "  'list_repo_collections',\n",
       "  'delete_repo_collection',\n",
       "  'debug_search_results',\n",
       "  'get_file_content',\n",
       "  'search_with_content'],\n",
       " 'classes': [],\n",
       " 'variables': []}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_python_symbols(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG intiations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search repo with similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_repo_code(query, top_k=5, similarity_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Search the repository code using RAG (Retrieval-Augmented Generation).\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query\n",
    "        top_k (int): Number of top results to return\n",
    "        similarity_threshold (float): Minimum similarity score (0-1)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of relevant code snippets with metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        vx = vecs.create_client(SUPABASE_DB_URL)\n",
    "        collection = vx.get_or_create_collection(name=\"repo_files\", dimension=1536)\n",
    "        \n",
    "        # Generate embedding for the query\n",
    "        query_embedding = model.embed_query(query)\n",
    "        \n",
    "        # Search for similar vectors\n",
    "        results = collection.query(\n",
    "            data=query_embedding,\n",
    "            limit=top_k,\n",
    "            include_value=True,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        \n",
    "        # Process and filter results\n",
    "        relevant_results = []\n",
    "        for result in results:\n",
    "            similarity_score = result[1]  # The similarity score\n",
    "            metadata = result[2]  # The metadata\n",
    "            \n",
    "            # Filter by similarity threshold\n",
    "            if similarity_score >= similarity_threshold:\n",
    "                relevant_results.append({\n",
    "                    'similarity_score': similarity_score,\n",
    "                    'repo_name': metadata.get('repo_name', 'Unknown'),\n",
    "                    'file_path': metadata.get('path', 'Unknown'),\n",
    "                    'content': metadata.get('content', ''),\n",
    "                    'commit_count': metadata.get('commit_count', 0),\n",
    "                    'branches': metadata.get('branches', [])\n",
    "                })\n",
    "        \n",
    "        # Sort by similarity score (highest first)\n",
    "        relevant_results.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "        \n",
    "        vx.disconnect()\n",
    "        return relevant_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error searching repository: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohamed mowina\\Desktop\\Areeb\\DevOps-Analysis\\.venv\\lib\\site-packages\\vecs\\collection.py:506: UserWarning: Query does not have a covering index for cosine_distance. See Collection.create_index\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = search_repo_code('send mail node', 5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'similarity_score': 0.783879111756007,\n",
       "  'repo_name': 'tmpqv23rcta',\n",
       "  'file_path': 'main.py',\n",
       "  'content': 'import sys\\nfrom utils import load_env\\nfrom workflow.graph import start_workflow, continue_workflow\\n\\n\\ndef main() -> None:\\n    load_env()\\n    if len(sys.argv) < 2:\\n        print(\"Usage: python main.py \\\\\"Your post topic...\\\\\"\")\\n        sys.exit(1)\\n    topic = sys.argv[1]\\n    thread_id = \"cli\"\\n    state = start_workflow(topic, thread_id)\\n    if state.current_step != \"wait_feedback\":\\n        print(f\"Error: {state.error_message}\")\\n        sys.exit(1)\\n    print(\"\\\\nGenerated Post:\\\\n\")\\n    print(state.post_text)\\n    print(\"\\\\nImage URL:\\\\n\")\\n    print(state.image_url)\\n    while True:\\n        feedback = input(\"\\\\nFeedback (approve/refine text .../refine image .../quit): \").strip()\\n        if feedback.lower() in {\"quit\", \"exit\", \"q\"}:\\n            print(\"Goodbye!\")\\n            break\\n        state = continue_workflow(feedback, thread_id)\\n        if state.current_step == \"completed\":\\n            print(\"Email sent. Workflow completed.\")\\n            break\\n        if state.current_step == \"terminate\":\\n            print(\"Workflow terminated.\")\\n            break\\n        if state.current_step == \"wait_feedback\":\\n            if state.classification == \"Refine Text\":\\n                print(\"\\\\nUpdated Post:\\\\n\")\\n                print(state.post_text)\\n            elif state.classification == \"Refine Image\":\\n                print(\"\\\\nUpdated Image URL:\\\\n\")\\n                print(state.image_url)\\n\\n\\nif __name__ == \"__main__\":\\n    main()',\n",
       "  'commit_count': 6,\n",
       "  'branches': ['main']},\n",
       " {'similarity_score': 0.780522244007721,\n",
       "  'repo_name': 'tmpqv23rcta',\n",
       "  'file_path': 'workflow/graph.py',\n",
       "  'content': 'from langgraph.graph import StateGraph, END\\nfrom langgraph.checkpoint.memory import MemorySaver\\n\\nfrom workflow.state import WorkflowState\\nfrom workflow.nodes import (\\n    create_post_text,\\n    generate_image,\\n    classify_feedback,\\n    refine_text,\\n    refine_image,\\n    send_email,\\n)\\n\\n\\ndef route_feedback(state: WorkflowState) -> str:\\n    \"\"\"Route based on feedback classification.\"\"\"\\n    classification = state.classification.lower()\\n    if \"approved\" in classification:\\n        return \"approved\"\\n    if \"refine text\" in classification or \"refine_text\" in classification:\\n        return \"refine_text\"\\n    if \"refine image\" in classification or \"refine_image\" in classification:\\n        return \"refine_image\"\\n    if \"terminate\" in classification:\\n        return \"terminate\"\\n    return \"terminate\"\\n\\n\\ndef create_workflow():\\n    \"\"\"Create the LangGraph workflow with interrupts configured.\"\"\"\\n    workflow = StateGraph(WorkflowState)\\n    workflow.add_node(\"create_post\", create_post_text)\\n    workflow.add_node(\"generate_image\", generate_image)\\n    workflow.add_node(\"classify_feedback\", classify_feedback)\\n    workflow.add_node(\"refine_text\", refine_text)\\n    workflow.add_node(\"refine_image\", refine_image)\\n    workflow.add_node(\"send_email\", send_email)\\n    workflow.set_entry_point(\"create_post\")\\n    workflow.add_edge(\"create_post\", \"generate_image\")\\n    workflow.add_edge(\"generate_image\", \"classify_feedback\")\\n    workflow.add_conditional_edges(\\n        \"classify_feedback\",\\n        route_feedback,\\n        {\\n            \"approved\": \"send_email\",\\n            \"refine_text\": \"refine_text\",\\n            \"refine_image\": \"refine_image\",\\n            \"terminate\": END,\\n        },\\n    )\\n    workflow.add_edge(\"refine_text\", \"classify_feedback\")\\n    workflow.add_edge(\"refine_image\", \"classify_feedback\")\\n    workflow.add_edge(\"send_email\", END)\\n    memory = MemorySaver()\\n    app = workflow.compile(\\n        checkpointer=memory,\\n        interrupt_after=[\"generate_image\", \"refine_text\", \"refine_image\"],\\n    )\\n    return app\\n\\n\\n# Convenience API mirroring notebook helpers\\napp = create_workflow()\\n\\n\\ndef start_workflow(user_input: str, thread_id: str = \"default\") -> WorkflowState:\\n    initial_state = WorkflowState(user_input=user_input)\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\n    result = app.invoke(initial_state, config)\\n    return WorkflowState(**result)\\n\\n\\ndef continue_workflow(user_feedback: str, thread_id: str = \"default\") -> WorkflowState:\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\n    app.update_state(config, {\"user_feedback\": user_feedback})\\n    result = app.invoke(None, config)\\n    return WorkflowState(**result)\\n\\n\\n',\n",
       "  'commit_count': 6,\n",
       "  'branches': ['main']},\n",
       " {'similarity_score': 0.773327602545758,\n",
       "  'repo_name': 'tmpqv23rcta',\n",
       "  'file_path': 'README.md',\n",
       "  'content': '# LinkedIn-Booster 🚀\\n\\nAn intelligent AI-powered LinkedIn post creation workflow that combines the power of LangChain, LangGraph, and multiple AI services to create engaging LinkedIn content with matching visuals.\\n\\n![LangGraph Workflow](images/LangGraph-workflow-diagram.png)\\n\\n## ✨ Features\\n\\n- **🤖 AI-Powered Generation**: Google Gemini for high-quality LinkedIn post drafting\\n- **🔍 Web Research**: Tavily search integration for up-to-date context and insights\\n- **🔄 Interactive Feedback Loop**: Intelligent feedback classification and content refinement\\n- **🎨 Image Generation**: DALL·E 3 integration for compelling visual content\\n- **📧 Email Delivery**: Gmail API integration for seamless content distribution\\n- **🖥️ Multiple Interfaces**: CLI and web-based Chainlit UI options\\n- **📊 Workflow Visualization**: LangGraph Studio integration for workflow monitoring\\n\\n## 🏗️ Architecture\\n\\nThe project follows a modular architecture with clear separation of concerns:\\n\\n```\\nLinkedIn-Booster/\\n├── 📄 main.py                    # CLI interface\\n├── 🌐 app.py                     # Chainlit web interface  \\n├── 📁 workflow/\\n│   ├── __init__.py\\n│   ├── state.py                  # Workflow state management\\n│   ├── tools.py                  # External API integrations (Tavily)\\n│   ├── nodes.py                  # Core workflow nodes\\n│   └── graph.py                  # LangGraph workflow orchestration\\n├── 📁 utils/\\n│   ├── __init__.py\\n│   └── config.py                 # Configuration and environment setup\\n├── 📁 studio/                    # LangGraph Studio configuration\\n├── 📁 notebooks/                 # Development notebooks\\n├── 📁 images/                    # Documentation assets\\n├── 📄 requirements.txt\\n└── 📄 .env                       # Environment variables\\n```\\n\\n## 🚀 Quick Start\\n\\n### Prerequisites\\n- Python 3.8+\\n- API keys for OpenAI, Google Gemini, and Tavily\\n- Gmail API credentials (optional, for email functionality)\\n\\n### Installation\\n\\n1. **Clone and install dependencies**\\n```bash\\ngit clone <repository-url>\\ncd LinkedIn-Booster\\npip install -r requirements.txt\\n```\\n\\n2. **Configure environment variables**\\nCreate a `.env` file in the root directory:\\n```bash\\n# AI Service APIs\\nOPENAI_API_KEY=your_openai_api_key\\nGOOGLE_API_KEY=your_google_gemini_api_key\\nTAVILY_API_KEY=your_tavily_api_key\\n\\n# Gmail Integration (Optional)\\nGMAIL_CREDENTIALS_FILE=credentials.json\\nGMAIL_TOKEN_FILE=token.json\\nRECIPIENT_EMAIL=recipient@example.com\\n```\\n\\n3. **Gmail Setup (Optional)**\\nFor email functionality:\\n- Enable Gmail API in [Google Cloud Console](https://console.cloud.google.com/)\\n- Create OAuth2 credentials and download as `credentials.json`\\n- Place the file in the project root\\n- First run will generate `token.json` automatically\\n\\n## 💻 Usage\\n\\n### Option 1: Web Interface (Recommended)\\nLaunch the interactive Chainlit web interface:\\n\\n```bash\\nchainlit run app.py\\n```\\n\\n![Chainlit Interface](images/ChainLit-user%20Interface.png)\\n\\nThe web interface provides:\\n- 🎯 Guided workflow with clear instructions\\n- 📱 Real-time feedback and refinement\\n- 🖼️ Visual preview of generated content\\n- ✅ Easy approval and sending process\\n\\n### Option 2: Command Line Interface\\nFor quick automation or scripting:\\n\\n```bash\\npython main.py \"Create a post about the importance of drinking water\"\\n```\\n\\n### Option 3: LangGraph Studio\\nFor workflow development and debugging:\\n\\n![LangGraph Studio](images/LangGraphStudio-workflow-diagram.png)\\n\\n```bash\\ncd studio\\nlanggraph dev\\n```\\n\\n### Option 4: n8n Integration\\nFor no-code workflow automation:\\n\\n![n8n Workflow](images/n8n-workflow-diagram.png)\\n\\nImport the workflow from `n8n/main (NO img2img).json` into your n8n instance.\\n\\n## 🔄 Workflow Process\\n\\nThe LinkedIn Booster follows an intelligent multi-step process:\\n\\n1. **📝 Content Creation**: AI generates LinkedIn post based on your topic\\n2. **🎨 Image Generation**: Creates matching visual content using DALL·E 3\\n3. **👀 Review & Feedback**: Present content for user review\\n4. **🤖 Smart Classification**: AI analyzes feedback and determines next action\\n5. **✨ Refinement**: Improves text or image based on feedback\\n6. **✅ Approval & Delivery**: Sends final content via email when approved\\n\\nThe workflow uses LangGraph\\'s stateful execution with strategic interrupts, allowing for human-in-the-loop refinement at key decision points.\\n\\n## 🛠️ Development\\n\\n### Running Tests\\n```bash\\npython test_imports.py\\npython test_create_post.py\\npython send_mail_test.py\\n```\\n\\n### Jupyter Notebook\\nExplore the original development process:\\n```bash\\njupyter notebook notebooks/booster_workflow_v0.2.ipynb\\n```\\n\\n## 📋 Requirements\\n\\nKey dependencies include:\\n- `langchain` - LLM framework and integrations\\n- `langgraph` - Workflow orchestration\\n- `chainlit` - Web interface\\n- `google-api-python-client` - Gmail integration\\n- `openai` - DALL·E 3 image generation\\n- `tavily-python` - Web search capabilities\\n\\nSee `requirements.txt` for complete list.\\n\\n## 🤝 Contributing\\n\\n1. Fork the repository\\n2. Create a feature branch\\n3. Make your changes\\n4. Add tests if applicable\\n5. Submit a pull request\\n\\n## 📄 License\\n\\nMIT License - see LICENSE file for details.\\n',\n",
       "  'commit_count': 6,\n",
       "  'branches': ['main']},\n",
       " {'similarity_score': 0.69371229893785,\n",
       "  'repo_name': 'tmpqv23rcta',\n",
       "  'file_path': 'n8n/main (NO img2img).json',\n",
       "  'content': '{\\n  \"name\": \"main (NO img2img)\",\\n  \"nodes\": [\\n    {\\n      \"parameters\": {\\n        \"options\": {}\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatGoogleGemini\",\\n      \"typeVersion\": 1,\\n      \"position\": [\\n        832,\\n        352\\n      ],\\n      \"id\": \"575c62c0-07d4-4c73-9d70-4fdafe125e86\",\\n      \"name\": \"Google Gemini Chat Model\",\\n      \"credentials\": {\\n        \"googlePalmApi\": {\\n          \"id\": \"mP5d64x2rkeTRzbi\",\\n          \"name\": \"Google Gemini(PaLM) Api account 2\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"toolDescription\": \"Web search tool.\",\\n        \"method\": \"POST\",\\n        \"url\": \"https://api.tavily.com/search\",\\n        \"sendHeaders\": true,\\n        \"headerParameters\": {\\n          \"parameters\": [\\n            {\\n              \"name\": \"Authorization\",\\n              \"value\": \"Bearer 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\"\\n            }\\n          ]\\n        },\\n        \"sendBody\": true,\\n        \"bodyParameters\": {\\n          \"parameters\": [\\n            {\\n              \"name\": \"query\",\\n              \"value\": \"={{ /*n8n-auto-generated-fromAI-override*/ $fromAI(\\'parameters0_Value\\', ``, \\'string\\') }}\"\\n            }\\n          ]\\n        },\\n        \"options\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.httpRequestTool\",\\n      \"typeVersion\": 4.2,\\n      \"position\": [\\n        -496,\\n        -832\\n      ],\\n      \"id\": \"f6bfb373-1e88-448b-8dfd-da7c90016a8d\",\\n      \"name\": \"Tavily\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"inputText\": \"={{ $json.data.text }}\",\\n        \"categories\": {\\n          \"categories\": [\\n            {\\n              \"category\": \"Approved\",\\n              \"description\": \"Approval message refers that its ok to proceed with the current status.\"\\n            },\\n            {\\n              \"category\": \"Refine Text\",\\n              \"description\": \"Request to Refine the post text\"\\n            },\\n            {\\n              \"category\": \"Terminate\",\\n              \"description\": \"Termination request for the process.\"\\n            },\\n            {\\n              \"category\": \"Refine Image\",\\n              \"description\": \"Request to Refine the post Image\"\\n            }\\n          ]\\n        },\\n        \"options\": {}\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.textClassifier\",\\n      \"typeVersion\": 1.1,\\n      \"position\": [\\n        624,\\n        -464\\n      ],\\n      \"id\": \"abf5e791-c5f8-4b32-8a4b-58804e853434\",\\n      \"name\": \"Text Classifier\"\\n    },\\n    {\\n      \"parameters\": {},\\n      \"type\": \"n8n-nodes-base.noOp\",\\n      \"typeVersion\": 1,\\n      \"position\": [\\n        1296,\\n        64\\n      ],\\n      \"id\": \"04ec9776-b607-4c47-8140-bfeb62086c9c\",\\n      \"name\": \"No Operation, do nothing\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"authentication\": \"communityManagement\",\\n        \"person\": \"=Mohamed Mowina\",\\n        \"text\": \"={{ $(\\'Post writer\\').item.json.output }}\",\\n        \"shareMediaCategory\": \"IMAGE\",\\n        \"additionalFields\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.linkedIn\",\\n      \"typeVersion\": 1,\\n      \"position\": [\\n        1616,\\n        -1008\\n      ],\\n      \"id\": \"c56079bb-4e3e-471b-819c-3a6f73f2496b\",\\n      \"name\": \"Create a post\",\\n      \"disabled\": true\\n    },\\n    {\\n      \"parameters\": {\\n        \"updates\": [\\n          \"message\"\\n        ],\\n        \"additionalFields\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.telegramTrigger\",\\n      \"typeVersion\": 1.2,\\n      \"position\": [\\n        -1168,\\n        -1008\\n      ],\\n      \"id\": \"c6d21d0a-52b5-405d-b16d-e7568dd8e839\",\\n      \"name\": \"Telegram Trigger\",\\n      \"webhookId\": \"931413d8-41c9-43f0-85bc-1b1b1fb80fa4\",\\n      \"credentials\": {\\n        \"telegramApi\": {\\n          \"id\": \"EFoDusKQAfWdyYaO\",\\n          \"name\": \"Telegram account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"promptType\": \"define\",\\n        \"text\": \"={{ $json.message.text }}\",\\n        \"options\": {\\n          \"systemMessage\": \"=Your tone should be:\\\\n\\\\nProfessional yet approachable: Confident and authoritative, but also conversational and relatable.\\\\n\\\\nConcise and clear: Get straight to the point while using compelling language.\\\\n\\\\nAction-oriented: Focus on providing value and prompting engagement.\\\\n\\\\nCore Functionality\\\\nYou will perform the following tasks in a structured workflow:\\\\n\\\\nInput Analysis & Goal Clarification:\\\\n\\\\nUpon receiving a request, first identify the core topic and the user\\'s primary goal (e.g., generate leads, build thought leadership, share a company update, get job offers).\\\\n\\\\nIdentify the target audience and their pain points or interests.\\\\n\\\\nRecognize any specific constraints (e.g., character limit, inclusion of a specific link, mention of a name).\\\\n\\\\nStrategy & Brainstorming:\\\\n\\\\nBased on your analysis, brainstorm a minimum of three potential angles or hooks for the post. Each hook should be designed to capture attention and align with the user\\'s goal.\\\\n\\\\nConsider different post formats: a personal story, a listicle, a thought-provoking question, a concise tip, a problem/solution narrative.\\\\n\\\\nDetermine the most effective approach for the given topic and goal.\\\\n\\\\nDraft Generation:\\\\n\\\\nWrite a draft of the post using the chosen angle. Follow these rules for the highest quality:\\\\n\\\\nHook: Start with a strong, attention-grabbing first sentence.\\\\n\\\\nBody: Break up text into short, single-sentence paragraphs. Use line breaks frequently for scannability.\\\\n\\\\nElegance: Incorporate relevant emojis to break up text and add personality, but use them sparingly and strategically (1-3 emojis is usually sufficient).\\\\n\\\\nValue: Provide clear, actionable value, a unique perspective, or a compelling story.\\\\n\\\\nCall-to-Action (CTA): End with a clear, concise CTA that encourages engagement (e.g., \\\\\"What are your thoughts?\\\\\", \\\\\"Share your experience below!\\\\\", \\\\\"Comment \\'Yes\\' if you agree.\\\\\").\\\\n\\\\nHashtags: Suggest 3-5 relevant, specific, and popular hashtags at the end of the post.\\\\n\\\\nOutput:\\\\n\\\\nDon\\'t forget you Output only the post, Do not ask flowup questions and don\\'t provide other suggestions.\\\\n\"\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\\n      \"typeVersion\": 2.2,\\n      \"position\": [\\n        -848,\\n        -1008\\n      ],\\n      \"id\": \"2cac353e-25c0-4125-8b49-c13448885f92\",\\n      \"name\": \"Post writer\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"promptType\": \"define\",\\n        \"text\": \"=Original post:  {{ $(\\'Send a photo message\\').item.json.result.caption }}\\\\n\\\\nRequest: {{ $json.data.text }}\",\\n        \"options\": {\\n          \"systemMessage\": \"You are a helpful assistant who improve and refine Linkedin post.\\\\n\\\\nOutput only the post text.\"\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\\n      \"typeVersion\": 2.2,\\n      \"position\": [\\n        1552,\\n        -368\\n      ],\\n      \"id\": \"49ee2b7d-5cac-4a5d-9696-8d835045a536\",\\n      \"name\": \"Post refiner\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"operation\": \"sendAndWait\",\\n        \"chatId\": \"={{ $(\\'Telegram Trigger\\').item.json.message.chat.id }}\",\\n        \"message\": \"=Your LinkedIn post draft is ready (text + image).\\\\nYou can now decide what to do next. Simply type your instruction in plain language, and the system will handle it.\\\\nExamples:\\\\n\\\\n‘Looks good, post it.’ → Post\\\\n\\\\n‘Change the text, it feels too formal.’ → Edit text\\\\n\\\\n‘The image should focus more on teamwork.’ → Edit image\\\\n\\\\n‘Cancel this, I don’t want to continue.’ → Terminate\\\\n\\\\n👉 You are free to write anything — the system will classify your input and act accordingly.\",\\n        \"responseType\": \"freeText\",\\n        \"options\": {\\n          \"appendAttribution\": false\\n        }\\n      },\\n      \"type\": \"n8n-nodes-base.telegram\",\\n      \"typeVersion\": 1.2,\\n      \"position\": [\\n        320,\\n        112\\n      ],\\n      \"id\": \"07eb16f1-1579-41e4-8443-c2ef5445eb45\",\\n      \"name\": \"Wait for feedback\",\\n      \"webhookId\": \"a39f1d60-d759-43ca-b5b1-9e0b565f9e8a\",\\n      \"credentials\": {\\n        \"telegramApi\": {\\n          \"id\": \"EFoDusKQAfWdyYaO\",\\n          \"name\": \"Telegram account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Create post\\'s text\",\\n        \"height\": 448,\\n        \"width\": 640,\\n        \"color\": 7\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        -976,\\n        -1104\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"22b6a2cf-2715-479b-81d5-ce549dc5c223\",\\n      \"name\": \"Sticky Note\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Create post\\'s Image\",\\n        \"height\": 448,\\n        \"width\": 640,\\n        \"color\": 7\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        -752,\\n        -576\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"9ece431d-1cba-486c-abff-28e2d035c987\",\\n      \"name\": \"Sticky Note1\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Feedback\",\\n        \"height\": 448,\\n        \"width\": 816\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        -352,\\n        -48\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"3dd8f3c0-3ebf-45b4-ad4d-c2ce905863dd\",\\n      \"name\": \"Sticky Note2\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Terminate operation\",\\n        \"height\": 352,\\n        \"width\": 368,\\n        \"color\": 3\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1152,\\n        -96\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"f507679e-8023-4ee4-9a91-f067273627c9\",\\n      \"name\": \"Sticky Note3\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Post\",\\n        \"height\": 272,\\n        \"width\": 336,\\n        \"color\": 4\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1152,\\n        -1088\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"b9503555-ca57-49dc-8d12-d9ec43450bd4\",\\n      \"name\": \"Sticky Note4\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"# Refine post\",\\n        \"height\": 688,\\n        \"width\": 848,\\n        \"color\": 5\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1152,\\n        -800\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"4597f92e-43d9-4e6f-9aa9-10036656f7e5\",\\n      \"name\": \"Sticky Note5\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"## Refine Image\",\\n        \"height\": 272,\\n        \"width\": 576,\\n        \"color\": 6\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1376,\\n        -784\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"e0a0047b-199a-4590-9c25-155788058cb5\",\\n      \"name\": \"Sticky Note6\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"content\": \"## Refine Text\",\\n        \"height\": 256,\\n        \"width\": 368,\\n        \"color\": 6\\n      },\\n      \"type\": \"n8n-nodes-base.stickyNote\",\\n      \"position\": [\\n        1472,\\n        -432\\n      ],\\n      \"typeVersion\": 1,\\n      \"id\": \"c603d205-3fe1-409d-9ef4-aa4b4b44f86e\",\\n      \"name\": \"Sticky Note7\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"sendTo\": \"mohammed-mowina@outlook.com\",\\n        \"subject\": \"LinkedIn Booster\",\\n        \"emailType\": \"text\",\\n        \"message\": \"={{ $(\\'Merge\\').item.json.output }}\",\\n        \"options\": {\\n          \"appendAttribution\": false,\\n          \"attachmentsUi\": {\\n            \"attachmentsBinary\": [\\n              {\\n                \"property\": \"={{ $(\\'Merge\\').item.json.fileName }}\"\\n              }\\n            ]\\n          }\\n        }\\n      },\\n      \"type\": \"n8n-nodes-base.gmail\",\\n      \"typeVersion\": 2.1,\\n      \"position\": [\\n        1360,\\n        -992\\n      ],\\n      \"id\": \"b9b5dfed-369a-4e5d-86e5-8e8324b820a6\",\\n      \"name\": \"Send a message\",\\n      \"webhookId\": \"9e818c1d-3573-4592-8465-6953a6754cef\",\\n      \"credentials\": {\\n        \"gmailOAuth2\": {\\n          \"id\": \"Z3qIvY9PLNzWAis2\",\\n          \"name\": \"Gmail account 2\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"operation\": \"sendPhoto\",\\n        \"chatId\": \"={{ $(\\'Telegram Trigger\\').item.json.message.chat.id }}\",\\n        \"binaryData\": true,\\n        \"additionalFields\": {\\n          \"caption\": \"={{ $(\\'Post\\').item.json.output }}\"\\n        }\\n      },\\n      \"type\": \"n8n-nodes-base.telegram\",\\n      \"typeVersion\": 1.2,\\n      \"position\": [\\n        96,\\n        112\\n      ],\\n      \"id\": \"f6891562-43fb-4bb4-9d63-4494ef2e8b51\",\\n      \"name\": \"Send a photo message\",\\n      \"webhookId\": \"94294c72-0650-4b8b-8444-c6ec80b12d2a\",\\n      \"credentials\": {\\n        \"telegramApi\": {\\n          \"id\": \"EFoDusKQAfWdyYaO\",\\n          \"name\": \"Telegram account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"assignments\": {\\n          \"assignments\": [\\n            {\\n              \"id\": \"ef82052d-4309-4359-aa18-b6f2c8313036\",\\n              \"name\": \"output\",\\n              \"value\": \"={{ $json.output }}\",\\n              \"type\": \"string\"\\n            }\\n          ]\\n        },\\n        \"options\": {\\n          \"ignoreConversionErrors\": true,\\n          \"dotNotation\": true\\n        }\\n      },\\n      \"type\": \"n8n-nodes-base.set\",\\n      \"typeVersion\": 3.4,\\n      \"position\": [\\n        -304,\\n        112\\n      ],\\n      \"id\": \"f2baa1ec-2bcb-466e-881b-eb8f8b926d6e\",\\n      \"name\": \"Post\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"promptType\": \"define\",\\n        \"text\": \"=Old Prompt: {{ $(\\'AI Prompt Writer\\').item.json.output }}\\\\nUser comment: {{ $(\\'Wait for feedback\\').item.json.data.text }}\",\\n        \"options\": {\\n          \"systemMessage\": \"=**Overview**:\\\\nYou are an expert LinkedIn Image Prompt Engineer. For each request, generate AI image prompts following this structure. You work in a workflow and your responsability is to refine an already generated image by making a modified propmt to the Image Generator Model.\\\\n\\\\n*Input*:\\\\n- The old prompt for the generated Image.\\\\n- The user comment on the Image to change it.\\\\n\\\\n*Instructions*:\\\\nA modern, minimalistic professional LinkedIn illustration about the topic\\\\nAvoid text images just some\\\\n\\\\n*Style*:\\\\n3D Object Generator with clean flat vector, LinkedIn color palette (blue, white, grey), simple, corporate-appropriate. \\\\nAvoid clutter, make it inspiring and easy to understand.\\\\n\\\\n*Output*:\\\\nOnly output the Prompt no other suggestions or flowup questions.\"\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\\n      \"typeVersion\": 2.2,\\n      \"position\": [\\n        1456,\\n        -704\\n      ],\\n      \"id\": \"b464c76d-0cde-41f1-ab56-7e3f4690bdd7\",\\n      \"name\": \"AI Prompt Refiner\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"promptType\": \"define\",\\n        \"text\": \"={{ $json.output }}\",\\n        \"options\": {\\n          \"systemMessage\": \"=**Overview**:\\\\nYou are an expert LinkedIn Image Prompt Engineer. For each request, generate AI image prompts following this structure.\\\\n\\\\n*Input*:\\\\nYou will have the post\\'s text as the user Prompt.\\\\n\\\\n*Instructions*:\\\\nA modern, minimalistic professional LinkedIn illustration about the topic\\\\nAvoid text images just some\\\\n\\\\n*Style*:\\\\n3D Object Generator with clean flat vector, LinkedIn color palette (blue, white, grey), simple, corporate-appropriate. \\\\nAvoid clutter, make it inspiring and easy to understand.\\\\n\\\\n*Output*:\\\\nOnly output the Prompt no other suggestions or flowup questions.\"\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\\n      \"typeVersion\": 2.2,\\n      \"position\": [\\n        -656,\\n        -480\\n      ],\\n      \"id\": \"df8b4dfb-b2b9-4e65-bf2d-108a6fb99294\",\\n      \"name\": \"AI Prompt Writer\"\\n    },\\n    {\\n      \"parameters\": {\\n        \"resource\": \"image\",\\n        \"prompt\": \"={{ $json.output }}\",\\n        \"options\": {\\n          \"returnImageUrls\": false\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.openAi\",\\n      \"typeVersion\": 1.8,\\n      \"position\": [\\n        -352,\\n        -480\\n      ],\\n      \"id\": \"85253bd1-a207-42be-8510-bec86925c7d9\",\\n      \"name\": \"OpenAI Post Image Generation\",\\n      \"credentials\": {\\n        \"openAiApi\": {\\n          \"id\": \"WSrQfOLqmZvD07lC\",\\n          \"name\": \"OpenAi account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"resource\": \"image\",\\n        \"prompt\": \"={{ $json.output }}\",\\n        \"options\": {\\n          \"returnImageUrls\": false\\n        }\\n      },\\n      \"type\": \"@n8n/n8n-nodes-langchain.openAi\",\\n      \"typeVersion\": 1.8,\\n      \"position\": [\\n        1760,\\n        -704\\n      ],\\n      \"id\": \"a9bb6412-f5a5-4514-881f-20f2f19643e8\",\\n      \"name\": \"Regenerate an image\",\\n      \"credentials\": {\\n        \"openAiApi\": {\\n          \"id\": \"WSrQfOLqmZvD07lC\",\\n          \"name\": \"OpenAi account\"\\n        }\\n      }\\n    },\\n    {\\n      \"parameters\": {\\n        \"mode\": \"combine\",\\n        \"combineBy\": \"combineByPosition\",\\n        \"options\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.merge\",\\n      \"typeVersion\": 3.2,\\n      \"position\": [\\n        -96,\\n        112\\n      ],\\n      \"id\": \"33ab8e06-59b3-4428-9fb6-d376bd565ace\",\\n      \"name\": \"Merge\",\\n      \"retryOnFail\": false\\n    },\\n    {\\n      \"parameters\": {\\n        \"mode\": \"combine\",\\n        \"combineBy\": \"combineByPosition\",\\n        \"options\": {}\\n      },\\n      \"type\": \"n8n-nodes-base.merge\",\\n      \"typeVersion\": 3.2,\\n      \"position\": [\\n        1200,\\n        -992\\n      ],\\n      \"id\": \"9e582664-2a82-469a-b642-20bdf2d52f01\",\\n      \"name\": \"Merge1\",\\n      \"retryOnFail\": false\\n    }\\n  ],\\n  \"pinData\": {},\\n  \"connections\": {\\n    \"Google Gemini Chat Model\": {\\n      \"ai_languageModel\": [\\n        [\\n          {\\n            \"node\": \"Post writer\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"Text Classifier\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"Post refiner\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"AI Prompt Writer\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"AI Prompt Refiner\",\\n            \"type\": \"ai_languageModel\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Tavily\": {\\n      \"ai_tool\": [\\n        [\\n          {\\n            \"node\": \"Post writer\",\\n            \"type\": \"ai_tool\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Text Classifier\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Merge1\",\\n            \"type\": \"main\",\\n            \"index\": 1\\n          }\\n        ],\\n        [\\n          {\\n            \"node\": \"Post refiner\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ],\\n        [\\n          {\\n            \"node\": \"No Operation, do nothing\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ],\\n        [\\n          {\\n            \"node\": \"AI Prompt Refiner\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Telegram Trigger\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Post writer\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Post writer\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"AI Prompt Writer\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"Post\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Post refiner\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Post\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Wait for feedback\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Text Classifier\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Create a post\": {\\n      \"main\": [\\n        []\\n      ]\\n    },\\n    \"Send a photo message\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Wait for feedback\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Post\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Merge\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"AI Prompt Refiner\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Regenerate an image\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"AI Prompt Writer\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"OpenAI Post Image Generation\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"OpenAI Post Image Generation\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Merge\",\\n            \"type\": \"main\",\\n            \"index\": 1\\n          }\\n        ]\\n      ]\\n    },\\n    \"Regenerate an image\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Merge\",\\n            \"type\": \"main\",\\n            \"index\": 1\\n          }\\n        ]\\n      ]\\n    },\\n    \"Merge\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Send a photo message\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          },\\n          {\\n            \"node\": \"Merge1\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    },\\n    \"Merge1\": {\\n      \"main\": [\\n        [\\n          {\\n            \"node\": \"Send a message\",\\n            \"type\": \"main\",\\n            \"index\": 0\\n          }\\n        ]\\n      ]\\n    }\\n  },\\n  \"active\": false,\\n  \"settings\": {\\n    \"executionOrder\": \"v1\"\\n  },\\n  \"versionId\": \"d01124c9-2c7a-40fc-ac9e-7edaa4535fc4\",\\n  \"meta\": {\\n    \"templateCredsSetupCompleted\": true,\\n    \"instanceId\": \"e42899c7dfda6042a4f9f2f2b7e15acf58d2ab8ee4dfbe7dec48ee2a3be4e756\"\\n  },\\n  \"id\": \"dlis4ObYbraVh6Bt\",\\n  \"tags\": []\\n}',\n",
       "  'commit_count': 6,\n",
       "  'branches': ['main']},\n",
       " {'similarity_score': 0.663434237241743,\n",
       "  'repo_name': 'tmpqv23rcta',\n",
       "  'file_path': 'workflow/nodes.py',\n",
       "  'content': 'import os\\nfrom typing import List\\n\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain.schema import SystemMessage, HumanMessage\\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\\nfrom langgraph.errors import GraphRecursionError\\nfrom langgraph.prebuilt import create_react_agent\\n# Gmail imports will be done lazily in send_email function\\n\\nfrom workflow.state import WorkflowState\\nfrom workflow.tools import tavily_tool\\n\\n\\n# Initialize models\\ngemini_model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\\n\\ndef create_post_text(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Generate LinkedIn post text based on user input\"\"\"\\n    system_message = \"\"\"\\n    You are a professional LinkedIn content creator. Create engaging, professional LinkedIn posts.\\n\\n    Your tone should be:\\n    - Professional yet approachable\\n    - Concise and clear\\n    - Action-oriented with clear value\\n\\n    Format your post with:\\n    - A strong hook in the first sentence\\n    - Short, scannable paragraphs\\n    - 1-3 relevant emojis strategically placed\\n    - A clear call-to-action\\n    - 3-5 relevant hashtags at the end\\n\\n    Output ONLY the post text, nothing else.\"\"\"\\n    \\n    try:\\n        max_iterations = 3\\n        recursion_limit = 2 * max_iterations + 1\\n        \\n        agent = create_react_agent(\\n            model=\"openai:gpt-4o-mini\",\\n            tools=[tavily_tool],\\n            prompt=system_message,\\n            debug=True\\n        )\\n        \\n        try:\\n            response = agent.invoke(\\n                {\"messages\": [{\"role\": \"user\", \"content\": state.user_input}]},\\n                {\"recursion_limit\": recursion_limit},\\n            )\\n            \\n            state.post_text = response[\\'messages\\'][-1].content\\n            state.current_step = \"create_image\"\\n            \\n        except GraphRecursionError:\\n            print(\"Agent stopped due to max iterations.\")\\n            state.error_message = \"Agent stopped due to max iterations\"\\n            state.current_step = \"error\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error creating post text: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef generate_image(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Generate image directly from post text\"\"\"\\n    \\n    try:\\n        post_text = state.post_text\\n        \\n        # Create a proper prompt for image generation\\n        image_prompt_template = \"\"\"\\n        Create a modern, minimalistic professional LinkedIn illustration for the following post: {post_content}\\n        \\n        Style: 3D Object Generator with clean flat vector, LinkedIn color palette (blue, white, grey), simple, corporate-appropriate.\\n        Avoid clutter, make it inspiring and easy to understand.\\n        No text in the image, just visual elements.\\n        \\n        Generate only the image description prompt, no other text.\\n        \"\"\"\\n        \\n        # Create messages for the LLM\\n        messages = [\\n            SystemMessage(content=\"You are an expert LinkedIn Image Prompt Engineer. Generate concise AI image prompts.\"),\\n            HumanMessage(content=image_prompt_template.format(post_content=post_text))\\n        ]\\n        \\n        # Get the image prompt from Gemini\\n        response = gemini_model.invoke(messages)\\n        image_prompt = response.content.strip()\\n        \\n        print(f\"Image prompt: {image_prompt}\")\\n        \\n        # Generate image using DALL-E\\n        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\\n        image_url = dalle_wrapper.run(image_prompt)\\n        \\n        # Update state\\n        state.image_url = image_url\\n        state.image_prompt = image_prompt\\n        state.current_step = \"wait_feedback\" \\n        \\n    except Exception as e:\\n        state.error_message = f\"Error generating image: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef classify_feedback(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Classify user feedback into categories\"\"\"\\n    system_message = \"\"\"You are a text classifier. Classify the input text into one of these categories:\\n\\n        1. Approved - Approval message refers that its ok to proceed with the current status.\\n        2. Refine Text - Request to refine the post text\\n        3. Terminate - Termination request for the process.\\n        4. Refine Image - Request to refine the post image\\n\\n        Output only the category name, nothing else.\"\"\"\\n\\n    try:\\n        messages = [\\n            SystemMessage(content=system_message),\\n            HumanMessage(content=state.user_feedback)\\n        ]\\n        \\n        response = gemini_model.invoke(messages)\\n        classification = response.content.strip()\\n        \\n        # Map classification to next step\\n        if \"Approved\" in classification:\\n            state.classification = \"Approved\"\\n            state.current_step = \"post_content\"\\n            state.final_post_ready = True\\n        elif \"Refine Text\" in classification:\\n            state.classification = \"Refine Text\"\\n            state.current_step = \"refine_text\"\\n        elif \"Terminate\" in classification:\\n            state.classification = \"Terminate\"\\n            state.current_step = \"terminate\"\\n        elif \"Refine Image\" in classification:\\n            state.classification = \"Refine Image\"\\n            state.current_step = \"refine_image\"\\n        else:\\n            state.classification = \"Unknown\"\\n            state.current_step = \"wait_feedback\"\\n            state.error_message = f\"Unrecognized feedback classification: {classification}\"\\n        \\n        print(f\"DEBUG: Updated state - classification: {state.classification}, current_step: {state.current_step}\")\\n            \\n    except Exception as e:\\n        state.error_message = f\"Error classifying feedback: {str(e)}\"\\n        state.current_step = \"error\"\\n        print(f\"DEBUG: Exception in classify_feedback: {str(e)}\")\\n    \\n    return state\\n\\n\\ndef refine_text(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Refine the post text based on user feedback\"\"\"\\n    system_message = \"\"\"\\n        You are a helpful assistant who improve and refine LinkedIn post.\\n\\n        Output only the post text.\"\"\"\\n\\n    try:\\n        prompt = f\"Original post: {state.post_text}\\\\\\\\n\\\\\\\\nRequest: {state.user_feedback}\"\\n        \\n        messages = [\\n            SystemMessage(content=system_message),\\n            HumanMessage(content=prompt)\\n        ]\\n        \\n        response = gemini_model.invoke(messages)\\n        state.refined_text = response.content\\n        state.post_text = state.refined_text  # Update the main post text\\n        state.current_step = \"wait_feedback\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error refining text: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef refine_image(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Refine the image based on user feedback\"\"\"\\n    try:\\n        # Use the existing image prompt and add user feedback\\n        modified_prompt = f\"{state.image_prompt}. {state.user_feedback}\"\\n        \\n        print(f\"Refined image prompt: {modified_prompt}\")\\n        \\n        # Generate image using DALL-E with the modified prompt\\n        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\\n        image_url = dalle_wrapper.run(modified_prompt)\\n        \\n        # Update state with the refined image\\n        state.image_url = image_url\\n        state.image_prompt = modified_prompt  # Update the prompt with feedback\\n        state.current_step = \"wait_feedback\"  # Go back to wait for more feedback\\n        \\n        print(f\"Refined image URL: {image_url}\")\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error refining image: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef send_email(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Send email with the final post content\"\"\"\\n    try:\\n        # Lazy import Gmail functionality\\n        from langchain_google_community import GmailToolkit\\n        from langchain_google_community.gmail.utils import (\\n            build_resource_service,\\n            get_gmail_credentials,\\n        )\\n\\n        # Test imports without initializing toolkit\\n        print(\"Imports successful!\")\\n\\n            \\n        # Initialize Gmail toolkit\\n        credentials = get_gmail_credentials(\\n            token_file=\"token.json\",\\n            scopes=[\"https://mail.google.com/\"],\\n            client_secrets_file=\"credentials.json\",\\n        )\\n        \\n        api_resource = build_resource_service(credentials=credentials)\\n        gmail_toolkit = GmailToolkit(api_resource=api_resource)\\n        \\n        tools = gmail_toolkit.get_tools()\\n        print(f\\'🚩DEBUG: send mail tool: {tools[1]}\\')\\n        # Compose email content\\n        subject = \"LinkedIn Post Ready for Publishing\"\\n        body = f\"\"\"\\n            Your LinkedIn post is ready:\\n\\n            {state.post_text}\\n\\n            Image URL: {state.image_url}\\n\\n            Best regards,\\n            LinkedIn Post Creator\\n            \"\"\"\\n        \\n        # Send email using Gmail toolkit\\n        send_message_tool = gmail_toolkit.get_tools()[1]  # send_gmail_message tool\\n        \\n        email_result = send_message_tool.run({\\n            \"to\": \"mohammed-mowina@outlook.com\",\\n            \"subject\": subject,\\n            \"message\": body\\n        })\\n        \\n        state.current_step = \"completed\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"🚩Error sending email: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state',\n",
       "  'commit_count': 6,\n",
       "  'branches': ['main']}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "import os\n",
       "from typing import List\n",
       "\n",
       "from dotenv import load_dotenv\n",
       "load_dotenv()\n",
       "\n",
       "from langchain.chat_models import init_chat_model\n",
       "from langchain.schema import SystemMessage, HumanMessage\n",
       "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
       "from langgraph.errors import GraphRecursionError\n",
       "from langgraph.prebuilt import create_react_agent\n",
       "# Gmail imports will be done lazily in send_email function\n",
       "\n",
       "from workflow.state import WorkflowState\n",
       "from workflow.tools import tavily_tool\n",
       "\n",
       "\n",
       "# Initialize models\n",
       "gemini_model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
       "\n",
       "def create_post_text(state: WorkflowState) -> WorkflowState:\n",
       "    \"\"\"Generate LinkedIn post text based on user input\"\"\"\n",
       "    system_message = \"\"\"\n",
       "    You are a professional LinkedIn content creator. Create engaging, professional LinkedIn posts.\n",
       "\n",
       "    Your tone should be:\n",
       "    - Professional yet approachable\n",
       "    - Concise and clear\n",
       "    - Action-oriented with clear value\n",
       "\n",
       "    Format your post with:\n",
       "    - A strong hook in the first sentence\n",
       "    - Short, scannable paragraphs\n",
       "    - 1-3 relevant emojis strategically placed\n",
       "    - A clear call-to-action\n",
       "    - 3-5 relevant hashtags at the end\n",
       "\n",
       "    Output ONLY the post text, nothing else.\"\"\"\n",
       "    \n",
       "    try:\n",
       "        max_iterations = 3\n",
       "        recursion_limit = 2 * max_iterations + 1\n",
       "        \n",
       "        agent = create_react_agent(\n",
       "            model=\"openai:gpt-4o-mini\",\n",
       "            tools=[tavily_tool],\n",
       "            prompt=system_message,\n",
       "            debug=True\n",
       "        )\n",
       "        \n",
       "        try:\n",
       "            response = agent.invoke(\n",
       "                {\"messages\": [{\"role\": \"user\", \"content\": state.user_input}]},\n",
       "                {\"recursion_limit\": recursion_limit},\n",
       "            )\n",
       "            \n",
       "            state.post_text = response['messages'][-1].content\n",
       "            state.current_step = \"create_image\"\n",
       "            \n",
       "        except GraphRecursionError:\n",
       "            print(\"Agent stopped due to max iterations.\")\n",
       "            state.error_message = \"Agent stopped due to max iterations\"\n",
       "            state.current_step = \"error\"\n",
       "        \n",
       "    except Exception as e:\n",
       "        state.error_message = f\"Error creating post text: {str(e)}\"\n",
       "        state.current_step = \"error\"\n",
       "    \n",
       "    return state\n",
       "\n",
       "\n",
       "def generate_image(state: WorkflowState) -> WorkflowState:\n",
       "    \"\"\"Generate image directly from post text\"\"\"\n",
       "    \n",
       "    try:\n",
       "        post_text = state.post_text\n",
       "        \n",
       "        # Create a proper prompt for image generation\n",
       "        image_prompt_template = \"\"\"\n",
       "        Create a modern, minimalistic professional LinkedIn illustration for the following post: {post_content}\n",
       "        \n",
       "        Style: 3D Object Generator with clean flat vector, LinkedIn color palette (blue, white, grey), simple, corporate-appropriate.\n",
       "        Avoid clutter, make it inspiring and easy to understand.\n",
       "        No text in the image, just visual elements.\n",
       "        \n",
       "        Generate only the image description prompt, no other text.\n",
       "        \"\"\"\n",
       "        \n",
       "        # Create messages for the LLM\n",
       "        messages = [\n",
       "            SystemMessage(content=\"You are an expert LinkedIn Image Prompt Engineer. Generate concise AI image prompts.\"),\n",
       "            HumanMessage(content=image_prompt_template.format(post_content=post_text))\n",
       "        ]\n",
       "        \n",
       "        # Get the image prompt from Gemini\n",
       "        response = gemini_model.invoke(messages)\n",
       "        image_prompt = response.content.strip()\n",
       "        \n",
       "        print(f\"Image prompt: {image_prompt}\")\n",
       "        \n",
       "        # Generate image using DALL-E\n",
       "        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\n",
       "        image_url = dalle_wrapper.run(image_prompt)\n",
       "        \n",
       "        # Update state\n",
       "        state.image_url = image_url\n",
       "        state.image_prompt = image_prompt\n",
       "        state.current_step = \"wait_feedback\" \n",
       "        \n",
       "    except Exception as e:\n",
       "        state.error_message = f\"Error generating image: {str(e)}\"\n",
       "        state.current_step = \"error\"\n",
       "    \n",
       "    return state\n",
       "\n",
       "\n",
       "def classify_feedback(state: WorkflowState) -> WorkflowState:\n",
       "    \"\"\"Classify user feedback into categories\"\"\"\n",
       "    system_message = \"\"\"You are a text classifier. Classify the input text into one of these categories:\n",
       "\n",
       "        1. Approved - Approval message refers that its ok to proceed with the current status.\n",
       "        2. Refine Text - Request to refine the post text\n",
       "        3. Terminate - Termination request for the process.\n",
       "        4. Refine Image - Request to refine the post image\n",
       "\n",
       "        Output only the category name, nothing else.\"\"\"\n",
       "\n",
       "    try:\n",
       "        messages = [\n",
       "            SystemMessage(content=system_message),\n",
       "            HumanMessage(content=state.user_feedback)\n",
       "        ]\n",
       "        \n",
       "        response = gemini_model.invoke(messages)\n",
       "        classification = response.content.strip()\n",
       "        \n",
       "        # Map classification to next step\n",
       "        if \"Approved\" in classification:\n",
       "            state.classification = \"Approved\"\n",
       "            state.current_step = \"post_content\"\n",
       "            state.final_post_ready = True\n",
       "        elif \"Refine Text\" in classification:\n",
       "            state.classification = \"Refine Text\"\n",
       "            state.current_step = \"refine_text\"\n",
       "        elif \"Terminate\" in classification:\n",
       "            state.classification = \"Terminate\"\n",
       "            state.current_step = \"terminate\"\n",
       "        elif \"Refine Image\" in classification:\n",
       "            state.classification = \"Refine Image\"\n",
       "            state.current_step = \"refine_image\"\n",
       "        else:\n",
       "            state.classification = \"Unknown\"\n",
       "            state.current_step = \"wait_feedback\"\n",
       "            state.error_message = f\"Unrecognized feedback classification: {classification}\"\n",
       "        \n",
       "        print(f\"DEBUG: Updated state - classification: {state.classification}, current_step: {state.current_step}\")\n",
       "            \n",
       "    except Exception as e:\n",
       "        state.error_message = f\"Error classifying feedback: {str(e)}\"\n",
       "        state.current_step = \"error\"\n",
       "        print(f\"DEBUG: Exception in classify_feedback: {str(e)}\")\n",
       "    \n",
       "    return state\n",
       "\n",
       "\n",
       "def refine_text(state: WorkflowState) -> WorkflowState:\n",
       "    \"\"\"Refine the post text based on user feedback\"\"\"\n",
       "    system_message = \"\"\"\n",
       "        You are a helpful assistant who improve and refine LinkedIn post.\n",
       "\n",
       "        Output only the post text.\"\"\"\n",
       "\n",
       "    try:\n",
       "        prompt = f\"Original post: {state.post_text}\\\\n\\\\nRequest: {state.user_feedback}\"\n",
       "        \n",
       "        messages = [\n",
       "            SystemMessage(content=system_message),\n",
       "            HumanMessage(content=prompt)\n",
       "        ]\n",
       "        \n",
       "        response = gemini_model.invoke(messages)\n",
       "        state.refined_text = response.content\n",
       "        state.post_text = state.refined_text  # Update the main post text\n",
       "        state.current_step = \"wait_feedback\"\n",
       "        \n",
       "    except Exception as e:\n",
       "        state.error_message = f\"Error refining text: {str(e)}\"\n",
       "        state.current_step = \"error\"\n",
       "    \n",
       "    return state\n",
       "\n",
       "\n",
       "def refine_image(state: WorkflowState) -> WorkflowState:\n",
       "    \"\"\"Refine the image based on user feedback\"\"\"\n",
       "    try:\n",
       "        # Use the existing image prompt and add user feedback\n",
       "        modified_prompt = f\"{state.image_prompt}. {state.user_feedback}\"\n",
       "        \n",
       "        print(f\"Refined image prompt: {modified_prompt}\")\n",
       "        \n",
       "        # Generate image using DALL-E with the modified prompt\n",
       "        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\n",
       "        image_url = dalle_wrapper.run(modified_prompt)\n",
       "        \n",
       "        # Update state with the refined image\n",
       "        state.image_url = image_url\n",
       "        state.image_prompt = modified_prompt  # Update the prompt with feedback\n",
       "        state.current_step = \"wait_feedback\"  # Go back to wait for more feedback\n",
       "        \n",
       "        print(f\"Refined image URL: {image_url}\")\n",
       "        \n",
       "    except Exception as e:\n",
       "        state.error_message = f\"Error refining image: {str(e)}\"\n",
       "        state.current_step = \"error\"\n",
       "    \n",
       "    return state\n",
       "\n",
       "\n",
       "def send_email(state: WorkflowState) -> WorkflowState:\n",
       "    \"\"\"Send email with the final post content\"\"\"\n",
       "    try:\n",
       "        # Lazy import Gmail functionality\n",
       "        from langchain_google_community import GmailToolkit\n",
       "        from langchain_google_community.gmail.utils import (\n",
       "            build_resource_service,\n",
       "            get_gmail_credentials,\n",
       "        )\n",
       "\n",
       "        # Test imports without initializing toolkit\n",
       "        print(\"Imports successful!\")\n",
       "\n",
       "            \n",
       "        # Initialize Gmail toolkit\n",
       "        credentials = get_gmail_credentials(\n",
       "            token_file=\"token.json\",\n",
       "            scopes=[\"https://mail.google.com/\"],\n",
       "            client_secrets_file=\"credentials.json\",\n",
       "        )\n",
       "        \n",
       "        api_resource = build_resource_service(credentials=credentials)\n",
       "        gmail_toolkit = GmailToolkit(api_resource=api_resource)\n",
       "        \n",
       "        tools = gmail_toolkit.get_tools()\n",
       "        print(f'🚩DEBUG: send mail tool: {tools[1]}')\n",
       "        # Compose email content\n",
       "        subject = \"LinkedIn Post Ready for Publishing\"\n",
       "        body = f\"\"\"\n",
       "            Your LinkedIn post is ready:\n",
       "\n",
       "            {state.post_text}\n",
       "\n",
       "            Image URL: {state.image_url}\n",
       "\n",
       "            Best regards,\n",
       "            LinkedIn Post Creator\n",
       "            \"\"\"\n",
       "        \n",
       "        # Send email using Gmail toolkit\n",
       "        send_message_tool = gmail_toolkit.get_tools()[1]  # send_gmail_message tool\n",
       "        \n",
       "        email_result = send_message_tool.run({\n",
       "            \"to\": \"mohammed-mowina@outlook.com\",\n",
       "            \"subject\": subject,\n",
       "            \"message\": body\n",
       "        })\n",
       "        \n",
       "        state.current_step = \"completed\"\n",
       "        \n",
       "    except Exception as e:\n",
       "        state.error_message = f\"🚩Error sending email: {str(e)}\"\n",
       "        state.current_step = \"error\"\n",
       "    \n",
       "    return state"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(results[4]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_search_results(results, max_content_length=500):\n",
    "    \"\"\"\n",
    "    Format search results for better readability.\n",
    "    \n",
    "    Args:\n",
    "        results (list): Results from search_repo_code\n",
    "        max_content_length (int): Maximum length of content to display\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted results string\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return \"No relevant results found.\"\n",
    "    \n",
    "    formatted_output = f\"Found {len(results)} relevant results:\\n\\n\"\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        # Truncate content if too long\n",
    "        content = result['content']\n",
    "        if len(content) > max_content_length:\n",
    "            content = content[:max_content_length] + \"...\"\n",
    "        \n",
    "        formatted_output += f\"--- Result {i} ---\\n\"\n",
    "        formatted_output += f\"File: {result['file_path']}\\n\"\n",
    "        formatted_output += f\"Repository: {result['repo_name']}\\n\"\n",
    "        formatted_output += f\"Similarity Score: {result['similarity_score']:.3f}\\n\"\n",
    "        formatted_output += f\"Content:\\n{content}\\n\\n\"\n",
    "    \n",
    "    return formatted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 relevant results:\n",
      "\n",
      "--- Result 1 ---\n",
      "File: main.py\n",
      "Repository: tmpqv23rcta\n",
      "Similarity Score: 0.784\n",
      "Content:\n",
      "import sys\n",
      "from utils import load_env\n",
      "from workflow.graph import start_workflow, continue_workflow\n",
      "\n",
      "\n",
      "def main() -> None:\n",
      "    load_env()\n",
      "    if len(sys.argv) < 2:\n",
      "        print(\"Usage: python main.py \\\"Your post topic...\\\"\")\n",
      "        sys.exit(1)\n",
      "    topic = sys.argv[1]\n",
      "    thread_id = \"cli\"\n",
      "    state = start_workflow(topic, thread_id)\n",
      "    if state.current_step != \"wait_feedback\":\n",
      "        print(f\"Error: {state.error_message}\")\n",
      "        sys.exit(1)\n",
      "    print(\"\\nGenerated Post:\\n\")\n",
      "    print(state.pos...\n",
      "\n",
      "--- Result 2 ---\n",
      "File: workflow/graph.py\n",
      "Repository: tmpqv23rcta\n",
      "Similarity Score: 0.781\n",
      "Content:\n",
      "from langgraph.graph import StateGraph, END\n",
      "from langgraph.checkpoint.memory import MemorySaver\n",
      "\n",
      "from workflow.state import WorkflowState\n",
      "from workflow.nodes import (\n",
      "    create_post_text,\n",
      "    generate_image,\n",
      "    classify_feedback,\n",
      "    refine_text,\n",
      "    refine_image,\n",
      "    send_email,\n",
      ")\n",
      "\n",
      "\n",
      "def route_feedback(state: WorkflowState) -> str:\n",
      "    \"\"\"Route based on feedback classification.\"\"\"\n",
      "    classification = state.classification.lower()\n",
      "    if \"approved\" in classification:\n",
      "        return \"approved\"\n",
      " ...\n",
      "\n",
      "--- Result 3 ---\n",
      "File: README.md\n",
      "Repository: tmpqv23rcta\n",
      "Similarity Score: 0.773\n",
      "Content:\n",
      "# LinkedIn-Booster 🚀\n",
      "\n",
      "An intelligent AI-powered LinkedIn post creation workflow that combines the power of LangChain, LangGraph, and multiple AI services to create engaging LinkedIn content with matching visuals.\n",
      "\n",
      "![LangGraph Workflow](images/LangGraph-workflow-diagram.png)\n",
      "\n",
      "## ✨ Features\n",
      "\n",
      "- **🤖 AI-Powered Generation**: Google Gemini for high-quality LinkedIn post drafting\n",
      "- **🔍 Web Research**: Tavily search integration for up-to-date context and insights\n",
      "- **🔄 Interactive Feedback Loop**: Intel...\n",
      "\n",
      "--- Result 4 ---\n",
      "File: n8n/main (NO img2img).json\n",
      "Repository: tmpqv23rcta\n",
      "Similarity Score: 0.694\n",
      "Content:\n",
      "{\n",
      "  \"name\": \"main (NO img2img)\",\n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"parameters\": {\n",
      "        \"options\": {}\n",
      "      },\n",
      "      \"type\": \"@n8n/n8n-nodes-langchain.lmChatGoogleGemini\",\n",
      "      \"typeVersion\": 1,\n",
      "      \"position\": [\n",
      "        832,\n",
      "        352\n",
      "      ],\n",
      "      \"id\": \"575c62c0-07d4-4c73-9d70-4fdafe125e86\",\n",
      "      \"name\": \"Google Gemini Chat Model\",\n",
      "      \"credentials\": {\n",
      "        \"googlePalmApi\": {\n",
      "          \"id\": \"mP5d64x2rkeTRzbi\",\n",
      "          \"name\": \"Google Gemini(PaLM) Api account 2\"\n",
      "        }\n",
      "      }\n",
      "    ...\n",
      "\n",
      "--- Result 5 ---\n",
      "File: workflow/nodes.py\n",
      "Repository: tmpqv23rcta\n",
      "Similarity Score: 0.663\n",
      "Content:\n",
      "import os\n",
      "from typing import List\n",
      "\n",
      "from dotenv import load_dotenv\n",
      "load_dotenv()\n",
      "\n",
      "from langchain.chat_models import init_chat_model\n",
      "from langchain.schema import SystemMessage, HumanMessage\n",
      "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
      "from langgraph.errors import GraphRecursionError\n",
      "from langgraph.prebuilt import create_react_agent\n",
      "# Gmail imports will be done lazily in send_email function\n",
      "\n",
      "from workflow.state import WorkflowState\n",
      "from workflow.tools import tavil...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(format_search_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG S&A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_search_and_answer(question, context_limit=3):\n",
    "    \"\"\"\n",
    "    Perform RAG search and generate an answer using the retrieved context.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to answer\n",
    "        context_limit (int): Number of top results to use as context\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains the answer and source information\n",
    "    \"\"\"\n",
    "    # Search for relevant code\n",
    "    search_results = search_repo_code(question, top_k=context_limit)\n",
    "    \n",
    "    if not search_results:\n",
    "        return {\n",
    "            'answer': \"I couldn't find relevant information in the repository to answer your question.\",\n",
    "            'sources': []\n",
    "        }\n",
    "    \n",
    "    # Build context from search results\n",
    "    context_parts = []\n",
    "    sources = []\n",
    "    \n",
    "    for result in search_results:\n",
    "        context_parts.append(f\"File: {result['file_path']}\\n{result['content']}\")\n",
    "        sources.append({\n",
    "            'file_path': result['file_path'],\n",
    "            'repo_name': result['repo_name'],\n",
    "            'similarity_score': result['similarity_score']\n",
    "        })\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Create a prompt for the LLM (you can customize this)\n",
    "    prompt = f\"\"\"\n",
    "        Based on the following code repository context, please answer the question.\n",
    "\n",
    "        Context from repository:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Please provide a detailed answer based on the code context above. If the context doesn't contain enough information to fully answer the question, please mention what additional information might be needed.\n",
    "    \"\"\"\n",
    "\n",
    "    return {\n",
    "        'prompt': prompt,\n",
    "        'context': context,\n",
    "        'sources': sources,\n",
    "        'search_results': search_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohamed mowina\\Desktop\\Areeb\\DevOps-Analysis\\.venv\\lib\\site-packages\\vecs\\collection.py:506: UserWarning: Query does not have a covering index for cosine_distance. See Collection.create_index\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results2 = rag_search_and_answer('What is the part in the project responsible on the generate image functionality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '\\n        Based on the following code repository context, please answer the question.\\n\\n        Context from repository:\\n        File: README.md\\n# LinkedIn-Booster 🚀\\n\\nAn intelligent AI-powered LinkedIn post creation workflow that combines the power of LangChain, LangGraph, and multiple AI services to create engaging LinkedIn content with matching visuals.\\n\\n![LangGraph Workflow](images/LangGraph-workflow-diagram.png)\\n\\n## ✨ Features\\n\\n- **🤖 AI-Powered Generation**: Google Gemini for high-quality LinkedIn post drafting\\n- **🔍 Web Research**: Tavily search integration for up-to-date context and insights\\n- **🔄 Interactive Feedback Loop**: Intelligent feedback classification and content refinement\\n- **🎨 Image Generation**: DALL·E 3 integration for compelling visual content\\n- **📧 Email Delivery**: Gmail API integration for seamless content distribution\\n- **🖥️ Multiple Interfaces**: CLI and web-based Chainlit UI options\\n- **📊 Workflow Visualization**: LangGraph Studio integration for workflow monitoring\\n\\n## 🏗️ Architecture\\n\\nThe project follows a modular architecture with clear separation of concerns:\\n\\n```\\nLinkedIn-Booster/\\n├── 📄 main.py                    # CLI interface\\n├── 🌐 app.py                     # Chainlit web interface  \\n├── 📁 workflow/\\n│   ├── __init__.py\\n│   ├── state.py                  # Workflow state management\\n│   ├── tools.py                  # External API integrations (Tavily)\\n│   ├── nodes.py                  # Core workflow nodes\\n│   └── graph.py                  # LangGraph workflow orchestration\\n├── 📁 utils/\\n│   ├── __init__.py\\n│   └── config.py                 # Configuration and environment setup\\n├── 📁 studio/                    # LangGraph Studio configuration\\n├── 📁 notebooks/                 # Development notebooks\\n├── 📁 images/                    # Documentation assets\\n├── 📄 requirements.txt\\n└── 📄 .env                       # Environment variables\\n```\\n\\n## 🚀 Quick Start\\n\\n### Prerequisites\\n- Python 3.8+\\n- API keys for OpenAI, Google Gemini, and Tavily\\n- Gmail API credentials (optional, for email functionality)\\n\\n### Installation\\n\\n1. **Clone and install dependencies**\\n```bash\\ngit clone <repository-url>\\ncd LinkedIn-Booster\\npip install -r requirements.txt\\n```\\n\\n2. **Configure environment variables**\\nCreate a `.env` file in the root directory:\\n```bash\\n# AI Service APIs\\nOPENAI_API_KEY=your_openai_api_key\\nGOOGLE_API_KEY=your_google_gemini_api_key\\nTAVILY_API_KEY=your_tavily_api_key\\n\\n# Gmail Integration (Optional)\\nGMAIL_CREDENTIALS_FILE=credentials.json\\nGMAIL_TOKEN_FILE=token.json\\nRECIPIENT_EMAIL=recipient@example.com\\n```\\n\\n3. **Gmail Setup (Optional)**\\nFor email functionality:\\n- Enable Gmail API in [Google Cloud Console](https://console.cloud.google.com/)\\n- Create OAuth2 credentials and download as `credentials.json`\\n- Place the file in the project root\\n- First run will generate `token.json` automatically\\n\\n## 💻 Usage\\n\\n### Option 1: Web Interface (Recommended)\\nLaunch the interactive Chainlit web interface:\\n\\n```bash\\nchainlit run app.py\\n```\\n\\n![Chainlit Interface](images/ChainLit-user%20Interface.png)\\n\\nThe web interface provides:\\n- 🎯 Guided workflow with clear instructions\\n- 📱 Real-time feedback and refinement\\n- 🖼️ Visual preview of generated content\\n- ✅ Easy approval and sending process\\n\\n### Option 2: Command Line Interface\\nFor quick automation or scripting:\\n\\n```bash\\npython main.py \"Create a post about the importance of drinking water\"\\n```\\n\\n### Option 3: LangGraph Studio\\nFor workflow development and debugging:\\n\\n![LangGraph Studio](images/LangGraphStudio-workflow-diagram.png)\\n\\n```bash\\ncd studio\\nlanggraph dev\\n```\\n\\n### Option 4: n8n Integration\\nFor no-code workflow automation:\\n\\n![n8n Workflow](images/n8n-workflow-diagram.png)\\n\\nImport the workflow from `n8n/main (NO img2img).json` into your n8n instance.\\n\\n## 🔄 Workflow Process\\n\\nThe LinkedIn Booster follows an intelligent multi-step process:\\n\\n1. **📝 Content Creation**: AI generates LinkedIn post based on your topic\\n2. **🎨 Image Generation**: Creates matching visual content using DALL·E 3\\n3. **👀 Review & Feedback**: Present content for user review\\n4. **🤖 Smart Classification**: AI analyzes feedback and determines next action\\n5. **✨ Refinement**: Improves text or image based on feedback\\n6. **✅ Approval & Delivery**: Sends final content via email when approved\\n\\nThe workflow uses LangGraph\\'s stateful execution with strategic interrupts, allowing for human-in-the-loop refinement at key decision points.\\n\\n## 🛠️ Development\\n\\n### Running Tests\\n```bash\\npython test_imports.py\\npython test_create_post.py\\npython send_mail_test.py\\n```\\n\\n### Jupyter Notebook\\nExplore the original development process:\\n```bash\\njupyter notebook notebooks/booster_workflow_v0.2.ipynb\\n```\\n\\n## 📋 Requirements\\n\\nKey dependencies include:\\n- `langchain` - LLM framework and integrations\\n- `langgraph` - Workflow orchestration\\n- `chainlit` - Web interface\\n- `google-api-python-client` - Gmail integration\\n- `openai` - DALL·E 3 image generation\\n- `tavily-python` - Web search capabilities\\n\\nSee `requirements.txt` for complete list.\\n\\n## 🤝 Contributing\\n\\n1. Fork the repository\\n2. Create a feature branch\\n3. Make your changes\\n4. Add tests if applicable\\n5. Submit a pull request\\n\\n## 📄 License\\n\\nMIT License - see LICENSE file for details.\\n\\n\\n---\\n\\nFile: workflow/nodes.py\\nimport os\\nfrom typing import List\\n\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain.schema import SystemMessage, HumanMessage\\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\\nfrom langgraph.errors import GraphRecursionError\\nfrom langgraph.prebuilt import create_react_agent\\n# Gmail imports will be done lazily in send_email function\\n\\nfrom workflow.state import WorkflowState\\nfrom workflow.tools import tavily_tool\\n\\n\\n# Initialize models\\ngemini_model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\\n\\ndef create_post_text(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Generate LinkedIn post text based on user input\"\"\"\\n    system_message = \"\"\"\\n    You are a professional LinkedIn content creator. Create engaging, professional LinkedIn posts.\\n\\n    Your tone should be:\\n    - Professional yet approachable\\n    - Concise and clear\\n    - Action-oriented with clear value\\n\\n    Format your post with:\\n    - A strong hook in the first sentence\\n    - Short, scannable paragraphs\\n    - 1-3 relevant emojis strategically placed\\n    - A clear call-to-action\\n    - 3-5 relevant hashtags at the end\\n\\n    Output ONLY the post text, nothing else.\"\"\"\\n    \\n    try:\\n        max_iterations = 3\\n        recursion_limit = 2 * max_iterations + 1\\n        \\n        agent = create_react_agent(\\n            model=\"openai:gpt-4o-mini\",\\n            tools=[tavily_tool],\\n            prompt=system_message,\\n            debug=True\\n        )\\n        \\n        try:\\n            response = agent.invoke(\\n                {\"messages\": [{\"role\": \"user\", \"content\": state.user_input}]},\\n                {\"recursion_limit\": recursion_limit},\\n            )\\n            \\n            state.post_text = response[\\'messages\\'][-1].content\\n            state.current_step = \"create_image\"\\n            \\n        except GraphRecursionError:\\n            print(\"Agent stopped due to max iterations.\")\\n            state.error_message = \"Agent stopped due to max iterations\"\\n            state.current_step = \"error\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error creating post text: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef generate_image(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Generate image directly from post text\"\"\"\\n    \\n    try:\\n        post_text = state.post_text\\n        \\n        # Create a proper prompt for image generation\\n        image_prompt_template = \"\"\"\\n        Create a modern, minimalistic professional LinkedIn illustration for the following post: {post_content}\\n        \\n        Style: 3D Object Generator with clean flat vector, LinkedIn color palette (blue, white, grey), simple, corporate-appropriate.\\n        Avoid clutter, make it inspiring and easy to understand.\\n        No text in the image, just visual elements.\\n        \\n        Generate only the image description prompt, no other text.\\n        \"\"\"\\n        \\n        # Create messages for the LLM\\n        messages = [\\n            SystemMessage(content=\"You are an expert LinkedIn Image Prompt Engineer. Generate concise AI image prompts.\"),\\n            HumanMessage(content=image_prompt_template.format(post_content=post_text))\\n        ]\\n        \\n        # Get the image prompt from Gemini\\n        response = gemini_model.invoke(messages)\\n        image_prompt = response.content.strip()\\n        \\n        print(f\"Image prompt: {image_prompt}\")\\n        \\n        # Generate image using DALL-E\\n        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\\n        image_url = dalle_wrapper.run(image_prompt)\\n        \\n        # Update state\\n        state.image_url = image_url\\n        state.image_prompt = image_prompt\\n        state.current_step = \"wait_feedback\" \\n        \\n    except Exception as e:\\n        state.error_message = f\"Error generating image: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef classify_feedback(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Classify user feedback into categories\"\"\"\\n    system_message = \"\"\"You are a text classifier. Classify the input text into one of these categories:\\n\\n        1. Approved - Approval message refers that its ok to proceed with the current status.\\n        2. Refine Text - Request to refine the post text\\n        3. Terminate - Termination request for the process.\\n        4. Refine Image - Request to refine the post image\\n\\n        Output only the category name, nothing else.\"\"\"\\n\\n    try:\\n        messages = [\\n            SystemMessage(content=system_message),\\n            HumanMessage(content=state.user_feedback)\\n        ]\\n        \\n        response = gemini_model.invoke(messages)\\n        classification = response.content.strip()\\n        \\n        # Map classification to next step\\n        if \"Approved\" in classification:\\n            state.classification = \"Approved\"\\n            state.current_step = \"post_content\"\\n            state.final_post_ready = True\\n        elif \"Refine Text\" in classification:\\n            state.classification = \"Refine Text\"\\n            state.current_step = \"refine_text\"\\n        elif \"Terminate\" in classification:\\n            state.classification = \"Terminate\"\\n            state.current_step = \"terminate\"\\n        elif \"Refine Image\" in classification:\\n            state.classification = \"Refine Image\"\\n            state.current_step = \"refine_image\"\\n        else:\\n            state.classification = \"Unknown\"\\n            state.current_step = \"wait_feedback\"\\n            state.error_message = f\"Unrecognized feedback classification: {classification}\"\\n        \\n        print(f\"DEBUG: Updated state - classification: {state.classification}, current_step: {state.current_step}\")\\n            \\n    except Exception as e:\\n        state.error_message = f\"Error classifying feedback: {str(e)}\"\\n        state.current_step = \"error\"\\n        print(f\"DEBUG: Exception in classify_feedback: {str(e)}\")\\n    \\n    return state\\n\\n\\ndef refine_text(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Refine the post text based on user feedback\"\"\"\\n    system_message = \"\"\"\\n        You are a helpful assistant who improve and refine LinkedIn post.\\n\\n        Output only the post text.\"\"\"\\n\\n    try:\\n        prompt = f\"Original post: {state.post_text}\\\\\\\\n\\\\\\\\nRequest: {state.user_feedback}\"\\n        \\n        messages = [\\n            SystemMessage(content=system_message),\\n            HumanMessage(content=prompt)\\n        ]\\n        \\n        response = gemini_model.invoke(messages)\\n        state.refined_text = response.content\\n        state.post_text = state.refined_text  # Update the main post text\\n        state.current_step = \"wait_feedback\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error refining text: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef refine_image(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Refine the image based on user feedback\"\"\"\\n    try:\\n        # Use the existing image prompt and add user feedback\\n        modified_prompt = f\"{state.image_prompt}. {state.user_feedback}\"\\n        \\n        print(f\"Refined image prompt: {modified_prompt}\")\\n        \\n        # Generate image using DALL-E with the modified prompt\\n        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\\n        image_url = dalle_wrapper.run(modified_prompt)\\n        \\n        # Update state with the refined image\\n        state.image_url = image_url\\n        state.image_prompt = modified_prompt  # Update the prompt with feedback\\n        state.current_step = \"wait_feedback\"  # Go back to wait for more feedback\\n        \\n        print(f\"Refined image URL: {image_url}\")\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error refining image: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef send_email(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Send email with the final post content\"\"\"\\n    try:\\n        # Lazy import Gmail functionality\\n        from langchain_google_community import GmailToolkit\\n        from langchain_google_community.gmail.utils import (\\n            build_resource_service,\\n            get_gmail_credentials,\\n        )\\n\\n        # Test imports without initializing toolkit\\n        print(\"Imports successful!\")\\n\\n            \\n        # Initialize Gmail toolkit\\n        credentials = get_gmail_credentials(\\n            token_file=\"token.json\",\\n            scopes=[\"https://mail.google.com/\"],\\n            client_secrets_file=\"credentials.json\",\\n        )\\n        \\n        api_resource = build_resource_service(credentials=credentials)\\n        gmail_toolkit = GmailToolkit(api_resource=api_resource)\\n        \\n        tools = gmail_toolkit.get_tools()\\n        print(f\\'🚩DEBUG: send mail tool: {tools[1]}\\')\\n        # Compose email content\\n        subject = \"LinkedIn Post Ready for Publishing\"\\n        body = f\"\"\"\\n            Your LinkedIn post is ready:\\n\\n            {state.post_text}\\n\\n            Image URL: {state.image_url}\\n\\n            Best regards,\\n            LinkedIn Post Creator\\n            \"\"\"\\n        \\n        # Send email using Gmail toolkit\\n        send_message_tool = gmail_toolkit.get_tools()[1]  # send_gmail_message tool\\n        \\n        email_result = send_message_tool.run({\\n            \"to\": \"mohammed-mowina@outlook.com\",\\n            \"subject\": subject,\\n            \"message\": body\\n        })\\n        \\n        state.current_step = \"completed\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"🚩Error sending email: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n        Question: What is the part in the project responsible on the generate image functionality\\n\\n        Please provide a detailed answer based on the code context above. If the context doesn\\'t contain enough information to fully answer the question, please mention what additional information might be needed.\\n    ',\n",
       " 'context': 'File: README.md\\n# LinkedIn-Booster 🚀\\n\\nAn intelligent AI-powered LinkedIn post creation workflow that combines the power of LangChain, LangGraph, and multiple AI services to create engaging LinkedIn content with matching visuals.\\n\\n![LangGraph Workflow](images/LangGraph-workflow-diagram.png)\\n\\n## ✨ Features\\n\\n- **🤖 AI-Powered Generation**: Google Gemini for high-quality LinkedIn post drafting\\n- **🔍 Web Research**: Tavily search integration for up-to-date context and insights\\n- **🔄 Interactive Feedback Loop**: Intelligent feedback classification and content refinement\\n- **🎨 Image Generation**: DALL·E 3 integration for compelling visual content\\n- **📧 Email Delivery**: Gmail API integration for seamless content distribution\\n- **🖥️ Multiple Interfaces**: CLI and web-based Chainlit UI options\\n- **📊 Workflow Visualization**: LangGraph Studio integration for workflow monitoring\\n\\n## 🏗️ Architecture\\n\\nThe project follows a modular architecture with clear separation of concerns:\\n\\n```\\nLinkedIn-Booster/\\n├── 📄 main.py                    # CLI interface\\n├── 🌐 app.py                     # Chainlit web interface  \\n├── 📁 workflow/\\n│   ├── __init__.py\\n│   ├── state.py                  # Workflow state management\\n│   ├── tools.py                  # External API integrations (Tavily)\\n│   ├── nodes.py                  # Core workflow nodes\\n│   └── graph.py                  # LangGraph workflow orchestration\\n├── 📁 utils/\\n│   ├── __init__.py\\n│   └── config.py                 # Configuration and environment setup\\n├── 📁 studio/                    # LangGraph Studio configuration\\n├── 📁 notebooks/                 # Development notebooks\\n├── 📁 images/                    # Documentation assets\\n├── 📄 requirements.txt\\n└── 📄 .env                       # Environment variables\\n```\\n\\n## 🚀 Quick Start\\n\\n### Prerequisites\\n- Python 3.8+\\n- API keys for OpenAI, Google Gemini, and Tavily\\n- Gmail API credentials (optional, for email functionality)\\n\\n### Installation\\n\\n1. **Clone and install dependencies**\\n```bash\\ngit clone <repository-url>\\ncd LinkedIn-Booster\\npip install -r requirements.txt\\n```\\n\\n2. **Configure environment variables**\\nCreate a `.env` file in the root directory:\\n```bash\\n# AI Service APIs\\nOPENAI_API_KEY=your_openai_api_key\\nGOOGLE_API_KEY=your_google_gemini_api_key\\nTAVILY_API_KEY=your_tavily_api_key\\n\\n# Gmail Integration (Optional)\\nGMAIL_CREDENTIALS_FILE=credentials.json\\nGMAIL_TOKEN_FILE=token.json\\nRECIPIENT_EMAIL=recipient@example.com\\n```\\n\\n3. **Gmail Setup (Optional)**\\nFor email functionality:\\n- Enable Gmail API in [Google Cloud Console](https://console.cloud.google.com/)\\n- Create OAuth2 credentials and download as `credentials.json`\\n- Place the file in the project root\\n- First run will generate `token.json` automatically\\n\\n## 💻 Usage\\n\\n### Option 1: Web Interface (Recommended)\\nLaunch the interactive Chainlit web interface:\\n\\n```bash\\nchainlit run app.py\\n```\\n\\n![Chainlit Interface](images/ChainLit-user%20Interface.png)\\n\\nThe web interface provides:\\n- 🎯 Guided workflow with clear instructions\\n- 📱 Real-time feedback and refinement\\n- 🖼️ Visual preview of generated content\\n- ✅ Easy approval and sending process\\n\\n### Option 2: Command Line Interface\\nFor quick automation or scripting:\\n\\n```bash\\npython main.py \"Create a post about the importance of drinking water\"\\n```\\n\\n### Option 3: LangGraph Studio\\nFor workflow development and debugging:\\n\\n![LangGraph Studio](images/LangGraphStudio-workflow-diagram.png)\\n\\n```bash\\ncd studio\\nlanggraph dev\\n```\\n\\n### Option 4: n8n Integration\\nFor no-code workflow automation:\\n\\n![n8n Workflow](images/n8n-workflow-diagram.png)\\n\\nImport the workflow from `n8n/main (NO img2img).json` into your n8n instance.\\n\\n## 🔄 Workflow Process\\n\\nThe LinkedIn Booster follows an intelligent multi-step process:\\n\\n1. **📝 Content Creation**: AI generates LinkedIn post based on your topic\\n2. **🎨 Image Generation**: Creates matching visual content using DALL·E 3\\n3. **👀 Review & Feedback**: Present content for user review\\n4. **🤖 Smart Classification**: AI analyzes feedback and determines next action\\n5. **✨ Refinement**: Improves text or image based on feedback\\n6. **✅ Approval & Delivery**: Sends final content via email when approved\\n\\nThe workflow uses LangGraph\\'s stateful execution with strategic interrupts, allowing for human-in-the-loop refinement at key decision points.\\n\\n## 🛠️ Development\\n\\n### Running Tests\\n```bash\\npython test_imports.py\\npython test_create_post.py\\npython send_mail_test.py\\n```\\n\\n### Jupyter Notebook\\nExplore the original development process:\\n```bash\\njupyter notebook notebooks/booster_workflow_v0.2.ipynb\\n```\\n\\n## 📋 Requirements\\n\\nKey dependencies include:\\n- `langchain` - LLM framework and integrations\\n- `langgraph` - Workflow orchestration\\n- `chainlit` - Web interface\\n- `google-api-python-client` - Gmail integration\\n- `openai` - DALL·E 3 image generation\\n- `tavily-python` - Web search capabilities\\n\\nSee `requirements.txt` for complete list.\\n\\n## 🤝 Contributing\\n\\n1. Fork the repository\\n2. Create a feature branch\\n3. Make your changes\\n4. Add tests if applicable\\n5. Submit a pull request\\n\\n## 📄 License\\n\\nMIT License - see LICENSE file for details.\\n\\n\\n---\\n\\nFile: workflow/nodes.py\\nimport os\\nfrom typing import List\\n\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain.schema import SystemMessage, HumanMessage\\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\\nfrom langgraph.errors import GraphRecursionError\\nfrom langgraph.prebuilt import create_react_agent\\n# Gmail imports will be done lazily in send_email function\\n\\nfrom workflow.state import WorkflowState\\nfrom workflow.tools import tavily_tool\\n\\n\\n# Initialize models\\ngemini_model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\\n\\ndef create_post_text(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Generate LinkedIn post text based on user input\"\"\"\\n    system_message = \"\"\"\\n    You are a professional LinkedIn content creator. Create engaging, professional LinkedIn posts.\\n\\n    Your tone should be:\\n    - Professional yet approachable\\n    - Concise and clear\\n    - Action-oriented with clear value\\n\\n    Format your post with:\\n    - A strong hook in the first sentence\\n    - Short, scannable paragraphs\\n    - 1-3 relevant emojis strategically placed\\n    - A clear call-to-action\\n    - 3-5 relevant hashtags at the end\\n\\n    Output ONLY the post text, nothing else.\"\"\"\\n    \\n    try:\\n        max_iterations = 3\\n        recursion_limit = 2 * max_iterations + 1\\n        \\n        agent = create_react_agent(\\n            model=\"openai:gpt-4o-mini\",\\n            tools=[tavily_tool],\\n            prompt=system_message,\\n            debug=True\\n        )\\n        \\n        try:\\n            response = agent.invoke(\\n                {\"messages\": [{\"role\": \"user\", \"content\": state.user_input}]},\\n                {\"recursion_limit\": recursion_limit},\\n            )\\n            \\n            state.post_text = response[\\'messages\\'][-1].content\\n            state.current_step = \"create_image\"\\n            \\n        except GraphRecursionError:\\n            print(\"Agent stopped due to max iterations.\")\\n            state.error_message = \"Agent stopped due to max iterations\"\\n            state.current_step = \"error\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error creating post text: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef generate_image(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Generate image directly from post text\"\"\"\\n    \\n    try:\\n        post_text = state.post_text\\n        \\n        # Create a proper prompt for image generation\\n        image_prompt_template = \"\"\"\\n        Create a modern, minimalistic professional LinkedIn illustration for the following post: {post_content}\\n        \\n        Style: 3D Object Generator with clean flat vector, LinkedIn color palette (blue, white, grey), simple, corporate-appropriate.\\n        Avoid clutter, make it inspiring and easy to understand.\\n        No text in the image, just visual elements.\\n        \\n        Generate only the image description prompt, no other text.\\n        \"\"\"\\n        \\n        # Create messages for the LLM\\n        messages = [\\n            SystemMessage(content=\"You are an expert LinkedIn Image Prompt Engineer. Generate concise AI image prompts.\"),\\n            HumanMessage(content=image_prompt_template.format(post_content=post_text))\\n        ]\\n        \\n        # Get the image prompt from Gemini\\n        response = gemini_model.invoke(messages)\\n        image_prompt = response.content.strip()\\n        \\n        print(f\"Image prompt: {image_prompt}\")\\n        \\n        # Generate image using DALL-E\\n        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\\n        image_url = dalle_wrapper.run(image_prompt)\\n        \\n        # Update state\\n        state.image_url = image_url\\n        state.image_prompt = image_prompt\\n        state.current_step = \"wait_feedback\" \\n        \\n    except Exception as e:\\n        state.error_message = f\"Error generating image: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef classify_feedback(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Classify user feedback into categories\"\"\"\\n    system_message = \"\"\"You are a text classifier. Classify the input text into one of these categories:\\n\\n        1. Approved - Approval message refers that its ok to proceed with the current status.\\n        2. Refine Text - Request to refine the post text\\n        3. Terminate - Termination request for the process.\\n        4. Refine Image - Request to refine the post image\\n\\n        Output only the category name, nothing else.\"\"\"\\n\\n    try:\\n        messages = [\\n            SystemMessage(content=system_message),\\n            HumanMessage(content=state.user_feedback)\\n        ]\\n        \\n        response = gemini_model.invoke(messages)\\n        classification = response.content.strip()\\n        \\n        # Map classification to next step\\n        if \"Approved\" in classification:\\n            state.classification = \"Approved\"\\n            state.current_step = \"post_content\"\\n            state.final_post_ready = True\\n        elif \"Refine Text\" in classification:\\n            state.classification = \"Refine Text\"\\n            state.current_step = \"refine_text\"\\n        elif \"Terminate\" in classification:\\n            state.classification = \"Terminate\"\\n            state.current_step = \"terminate\"\\n        elif \"Refine Image\" in classification:\\n            state.classification = \"Refine Image\"\\n            state.current_step = \"refine_image\"\\n        else:\\n            state.classification = \"Unknown\"\\n            state.current_step = \"wait_feedback\"\\n            state.error_message = f\"Unrecognized feedback classification: {classification}\"\\n        \\n        print(f\"DEBUG: Updated state - classification: {state.classification}, current_step: {state.current_step}\")\\n            \\n    except Exception as e:\\n        state.error_message = f\"Error classifying feedback: {str(e)}\"\\n        state.current_step = \"error\"\\n        print(f\"DEBUG: Exception in classify_feedback: {str(e)}\")\\n    \\n    return state\\n\\n\\ndef refine_text(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Refine the post text based on user feedback\"\"\"\\n    system_message = \"\"\"\\n        You are a helpful assistant who improve and refine LinkedIn post.\\n\\n        Output only the post text.\"\"\"\\n\\n    try:\\n        prompt = f\"Original post: {state.post_text}\\\\\\\\n\\\\\\\\nRequest: {state.user_feedback}\"\\n        \\n        messages = [\\n            SystemMessage(content=system_message),\\n            HumanMessage(content=prompt)\\n        ]\\n        \\n        response = gemini_model.invoke(messages)\\n        state.refined_text = response.content\\n        state.post_text = state.refined_text  # Update the main post text\\n        state.current_step = \"wait_feedback\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error refining text: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef refine_image(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Refine the image based on user feedback\"\"\"\\n    try:\\n        # Use the existing image prompt and add user feedback\\n        modified_prompt = f\"{state.image_prompt}. {state.user_feedback}\"\\n        \\n        print(f\"Refined image prompt: {modified_prompt}\")\\n        \\n        # Generate image using DALL-E with the modified prompt\\n        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\\n        image_url = dalle_wrapper.run(modified_prompt)\\n        \\n        # Update state with the refined image\\n        state.image_url = image_url\\n        state.image_prompt = modified_prompt  # Update the prompt with feedback\\n        state.current_step = \"wait_feedback\"  # Go back to wait for more feedback\\n        \\n        print(f\"Refined image URL: {image_url}\")\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error refining image: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef send_email(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Send email with the final post content\"\"\"\\n    try:\\n        # Lazy import Gmail functionality\\n        from langchain_google_community import GmailToolkit\\n        from langchain_google_community.gmail.utils import (\\n            build_resource_service,\\n            get_gmail_credentials,\\n        )\\n\\n        # Test imports without initializing toolkit\\n        print(\"Imports successful!\")\\n\\n            \\n        # Initialize Gmail toolkit\\n        credentials = get_gmail_credentials(\\n            token_file=\"token.json\",\\n            scopes=[\"https://mail.google.com/\"],\\n            client_secrets_file=\"credentials.json\",\\n        )\\n        \\n        api_resource = build_resource_service(credentials=credentials)\\n        gmail_toolkit = GmailToolkit(api_resource=api_resource)\\n        \\n        tools = gmail_toolkit.get_tools()\\n        print(f\\'🚩DEBUG: send mail tool: {tools[1]}\\')\\n        # Compose email content\\n        subject = \"LinkedIn Post Ready for Publishing\"\\n        body = f\"\"\"\\n            Your LinkedIn post is ready:\\n\\n            {state.post_text}\\n\\n            Image URL: {state.image_url}\\n\\n            Best regards,\\n            LinkedIn Post Creator\\n            \"\"\"\\n        \\n        # Send email using Gmail toolkit\\n        send_message_tool = gmail_toolkit.get_tools()[1]  # send_gmail_message tool\\n        \\n        email_result = send_message_tool.run({\\n            \"to\": \"mohammed-mowina@outlook.com\",\\n            \"subject\": subject,\\n            \"message\": body\\n        })\\n        \\n        state.current_step = \"completed\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"🚩Error sending email: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state',\n",
       " 'sources': [{'file_path': 'README.md',\n",
       "   'repo_name': 'tmpqv23rcta',\n",
       "   'similarity_score': 0.719834880838695},\n",
       "  {'file_path': 'workflow/nodes.py',\n",
       "   'repo_name': 'tmpqv23rcta',\n",
       "   'similarity_score': 0.704819470643995}],\n",
       " 'search_results': [{'similarity_score': 0.719834880838695,\n",
       "   'repo_name': 'tmpqv23rcta',\n",
       "   'file_path': 'README.md',\n",
       "   'content': '# LinkedIn-Booster 🚀\\n\\nAn intelligent AI-powered LinkedIn post creation workflow that combines the power of LangChain, LangGraph, and multiple AI services to create engaging LinkedIn content with matching visuals.\\n\\n![LangGraph Workflow](images/LangGraph-workflow-diagram.png)\\n\\n## ✨ Features\\n\\n- **🤖 AI-Powered Generation**: Google Gemini for high-quality LinkedIn post drafting\\n- **🔍 Web Research**: Tavily search integration for up-to-date context and insights\\n- **🔄 Interactive Feedback Loop**: Intelligent feedback classification and content refinement\\n- **🎨 Image Generation**: DALL·E 3 integration for compelling visual content\\n- **📧 Email Delivery**: Gmail API integration for seamless content distribution\\n- **🖥️ Multiple Interfaces**: CLI and web-based Chainlit UI options\\n- **📊 Workflow Visualization**: LangGraph Studio integration for workflow monitoring\\n\\n## 🏗️ Architecture\\n\\nThe project follows a modular architecture with clear separation of concerns:\\n\\n```\\nLinkedIn-Booster/\\n├── 📄 main.py                    # CLI interface\\n├── 🌐 app.py                     # Chainlit web interface  \\n├── 📁 workflow/\\n│   ├── __init__.py\\n│   ├── state.py                  # Workflow state management\\n│   ├── tools.py                  # External API integrations (Tavily)\\n│   ├── nodes.py                  # Core workflow nodes\\n│   └── graph.py                  # LangGraph workflow orchestration\\n├── 📁 utils/\\n│   ├── __init__.py\\n│   └── config.py                 # Configuration and environment setup\\n├── 📁 studio/                    # LangGraph Studio configuration\\n├── 📁 notebooks/                 # Development notebooks\\n├── 📁 images/                    # Documentation assets\\n├── 📄 requirements.txt\\n└── 📄 .env                       # Environment variables\\n```\\n\\n## 🚀 Quick Start\\n\\n### Prerequisites\\n- Python 3.8+\\n- API keys for OpenAI, Google Gemini, and Tavily\\n- Gmail API credentials (optional, for email functionality)\\n\\n### Installation\\n\\n1. **Clone and install dependencies**\\n```bash\\ngit clone <repository-url>\\ncd LinkedIn-Booster\\npip install -r requirements.txt\\n```\\n\\n2. **Configure environment variables**\\nCreate a `.env` file in the root directory:\\n```bash\\n# AI Service APIs\\nOPENAI_API_KEY=your_openai_api_key\\nGOOGLE_API_KEY=your_google_gemini_api_key\\nTAVILY_API_KEY=your_tavily_api_key\\n\\n# Gmail Integration (Optional)\\nGMAIL_CREDENTIALS_FILE=credentials.json\\nGMAIL_TOKEN_FILE=token.json\\nRECIPIENT_EMAIL=recipient@example.com\\n```\\n\\n3. **Gmail Setup (Optional)**\\nFor email functionality:\\n- Enable Gmail API in [Google Cloud Console](https://console.cloud.google.com/)\\n- Create OAuth2 credentials and download as `credentials.json`\\n- Place the file in the project root\\n- First run will generate `token.json` automatically\\n\\n## 💻 Usage\\n\\n### Option 1: Web Interface (Recommended)\\nLaunch the interactive Chainlit web interface:\\n\\n```bash\\nchainlit run app.py\\n```\\n\\n![Chainlit Interface](images/ChainLit-user%20Interface.png)\\n\\nThe web interface provides:\\n- 🎯 Guided workflow with clear instructions\\n- 📱 Real-time feedback and refinement\\n- 🖼️ Visual preview of generated content\\n- ✅ Easy approval and sending process\\n\\n### Option 2: Command Line Interface\\nFor quick automation or scripting:\\n\\n```bash\\npython main.py \"Create a post about the importance of drinking water\"\\n```\\n\\n### Option 3: LangGraph Studio\\nFor workflow development and debugging:\\n\\n![LangGraph Studio](images/LangGraphStudio-workflow-diagram.png)\\n\\n```bash\\ncd studio\\nlanggraph dev\\n```\\n\\n### Option 4: n8n Integration\\nFor no-code workflow automation:\\n\\n![n8n Workflow](images/n8n-workflow-diagram.png)\\n\\nImport the workflow from `n8n/main (NO img2img).json` into your n8n instance.\\n\\n## 🔄 Workflow Process\\n\\nThe LinkedIn Booster follows an intelligent multi-step process:\\n\\n1. **📝 Content Creation**: AI generates LinkedIn post based on your topic\\n2. **🎨 Image Generation**: Creates matching visual content using DALL·E 3\\n3. **👀 Review & Feedback**: Present content for user review\\n4. **🤖 Smart Classification**: AI analyzes feedback and determines next action\\n5. **✨ Refinement**: Improves text or image based on feedback\\n6. **✅ Approval & Delivery**: Sends final content via email when approved\\n\\nThe workflow uses LangGraph\\'s stateful execution with strategic interrupts, allowing for human-in-the-loop refinement at key decision points.\\n\\n## 🛠️ Development\\n\\n### Running Tests\\n```bash\\npython test_imports.py\\npython test_create_post.py\\npython send_mail_test.py\\n```\\n\\n### Jupyter Notebook\\nExplore the original development process:\\n```bash\\njupyter notebook notebooks/booster_workflow_v0.2.ipynb\\n```\\n\\n## 📋 Requirements\\n\\nKey dependencies include:\\n- `langchain` - LLM framework and integrations\\n- `langgraph` - Workflow orchestration\\n- `chainlit` - Web interface\\n- `google-api-python-client` - Gmail integration\\n- `openai` - DALL·E 3 image generation\\n- `tavily-python` - Web search capabilities\\n\\nSee `requirements.txt` for complete list.\\n\\n## 🤝 Contributing\\n\\n1. Fork the repository\\n2. Create a feature branch\\n3. Make your changes\\n4. Add tests if applicable\\n5. Submit a pull request\\n\\n## 📄 License\\n\\nMIT License - see LICENSE file for details.\\n',\n",
       "   'commit_count': 6,\n",
       "   'branches': ['main']},\n",
       "  {'similarity_score': 0.704819470643995,\n",
       "   'repo_name': 'tmpqv23rcta',\n",
       "   'file_path': 'workflow/nodes.py',\n",
       "   'content': 'import os\\nfrom typing import List\\n\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain.schema import SystemMessage, HumanMessage\\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\\nfrom langgraph.errors import GraphRecursionError\\nfrom langgraph.prebuilt import create_react_agent\\n# Gmail imports will be done lazily in send_email function\\n\\nfrom workflow.state import WorkflowState\\nfrom workflow.tools import tavily_tool\\n\\n\\n# Initialize models\\ngemini_model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\\n\\ndef create_post_text(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Generate LinkedIn post text based on user input\"\"\"\\n    system_message = \"\"\"\\n    You are a professional LinkedIn content creator. Create engaging, professional LinkedIn posts.\\n\\n    Your tone should be:\\n    - Professional yet approachable\\n    - Concise and clear\\n    - Action-oriented with clear value\\n\\n    Format your post with:\\n    - A strong hook in the first sentence\\n    - Short, scannable paragraphs\\n    - 1-3 relevant emojis strategically placed\\n    - A clear call-to-action\\n    - 3-5 relevant hashtags at the end\\n\\n    Output ONLY the post text, nothing else.\"\"\"\\n    \\n    try:\\n        max_iterations = 3\\n        recursion_limit = 2 * max_iterations + 1\\n        \\n        agent = create_react_agent(\\n            model=\"openai:gpt-4o-mini\",\\n            tools=[tavily_tool],\\n            prompt=system_message,\\n            debug=True\\n        )\\n        \\n        try:\\n            response = agent.invoke(\\n                {\"messages\": [{\"role\": \"user\", \"content\": state.user_input}]},\\n                {\"recursion_limit\": recursion_limit},\\n            )\\n            \\n            state.post_text = response[\\'messages\\'][-1].content\\n            state.current_step = \"create_image\"\\n            \\n        except GraphRecursionError:\\n            print(\"Agent stopped due to max iterations.\")\\n            state.error_message = \"Agent stopped due to max iterations\"\\n            state.current_step = \"error\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error creating post text: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef generate_image(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Generate image directly from post text\"\"\"\\n    \\n    try:\\n        post_text = state.post_text\\n        \\n        # Create a proper prompt for image generation\\n        image_prompt_template = \"\"\"\\n        Create a modern, minimalistic professional LinkedIn illustration for the following post: {post_content}\\n        \\n        Style: 3D Object Generator with clean flat vector, LinkedIn color palette (blue, white, grey), simple, corporate-appropriate.\\n        Avoid clutter, make it inspiring and easy to understand.\\n        No text in the image, just visual elements.\\n        \\n        Generate only the image description prompt, no other text.\\n        \"\"\"\\n        \\n        # Create messages for the LLM\\n        messages = [\\n            SystemMessage(content=\"You are an expert LinkedIn Image Prompt Engineer. Generate concise AI image prompts.\"),\\n            HumanMessage(content=image_prompt_template.format(post_content=post_text))\\n        ]\\n        \\n        # Get the image prompt from Gemini\\n        response = gemini_model.invoke(messages)\\n        image_prompt = response.content.strip()\\n        \\n        print(f\"Image prompt: {image_prompt}\")\\n        \\n        # Generate image using DALL-E\\n        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\\n        image_url = dalle_wrapper.run(image_prompt)\\n        \\n        # Update state\\n        state.image_url = image_url\\n        state.image_prompt = image_prompt\\n        state.current_step = \"wait_feedback\" \\n        \\n    except Exception as e:\\n        state.error_message = f\"Error generating image: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef classify_feedback(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Classify user feedback into categories\"\"\"\\n    system_message = \"\"\"You are a text classifier. Classify the input text into one of these categories:\\n\\n        1. Approved - Approval message refers that its ok to proceed with the current status.\\n        2. Refine Text - Request to refine the post text\\n        3. Terminate - Termination request for the process.\\n        4. Refine Image - Request to refine the post image\\n\\n        Output only the category name, nothing else.\"\"\"\\n\\n    try:\\n        messages = [\\n            SystemMessage(content=system_message),\\n            HumanMessage(content=state.user_feedback)\\n        ]\\n        \\n        response = gemini_model.invoke(messages)\\n        classification = response.content.strip()\\n        \\n        # Map classification to next step\\n        if \"Approved\" in classification:\\n            state.classification = \"Approved\"\\n            state.current_step = \"post_content\"\\n            state.final_post_ready = True\\n        elif \"Refine Text\" in classification:\\n            state.classification = \"Refine Text\"\\n            state.current_step = \"refine_text\"\\n        elif \"Terminate\" in classification:\\n            state.classification = \"Terminate\"\\n            state.current_step = \"terminate\"\\n        elif \"Refine Image\" in classification:\\n            state.classification = \"Refine Image\"\\n            state.current_step = \"refine_image\"\\n        else:\\n            state.classification = \"Unknown\"\\n            state.current_step = \"wait_feedback\"\\n            state.error_message = f\"Unrecognized feedback classification: {classification}\"\\n        \\n        print(f\"DEBUG: Updated state - classification: {state.classification}, current_step: {state.current_step}\")\\n            \\n    except Exception as e:\\n        state.error_message = f\"Error classifying feedback: {str(e)}\"\\n        state.current_step = \"error\"\\n        print(f\"DEBUG: Exception in classify_feedback: {str(e)}\")\\n    \\n    return state\\n\\n\\ndef refine_text(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Refine the post text based on user feedback\"\"\"\\n    system_message = \"\"\"\\n        You are a helpful assistant who improve and refine LinkedIn post.\\n\\n        Output only the post text.\"\"\"\\n\\n    try:\\n        prompt = f\"Original post: {state.post_text}\\\\\\\\n\\\\\\\\nRequest: {state.user_feedback}\"\\n        \\n        messages = [\\n            SystemMessage(content=system_message),\\n            HumanMessage(content=prompt)\\n        ]\\n        \\n        response = gemini_model.invoke(messages)\\n        state.refined_text = response.content\\n        state.post_text = state.refined_text  # Update the main post text\\n        state.current_step = \"wait_feedback\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error refining text: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef refine_image(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Refine the image based on user feedback\"\"\"\\n    try:\\n        # Use the existing image prompt and add user feedback\\n        modified_prompt = f\"{state.image_prompt}. {state.user_feedback}\"\\n        \\n        print(f\"Refined image prompt: {modified_prompt}\")\\n        \\n        # Generate image using DALL-E with the modified prompt\\n        dalle_wrapper = DallEAPIWrapper(model=\"dall-e-3\")\\n        image_url = dalle_wrapper.run(modified_prompt)\\n        \\n        # Update state with the refined image\\n        state.image_url = image_url\\n        state.image_prompt = modified_prompt  # Update the prompt with feedback\\n        state.current_step = \"wait_feedback\"  # Go back to wait for more feedback\\n        \\n        print(f\"Refined image URL: {image_url}\")\\n        \\n    except Exception as e:\\n        state.error_message = f\"Error refining image: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state\\n\\n\\ndef send_email(state: WorkflowState) -> WorkflowState:\\n    \"\"\"Send email with the final post content\"\"\"\\n    try:\\n        # Lazy import Gmail functionality\\n        from langchain_google_community import GmailToolkit\\n        from langchain_google_community.gmail.utils import (\\n            build_resource_service,\\n            get_gmail_credentials,\\n        )\\n\\n        # Test imports without initializing toolkit\\n        print(\"Imports successful!\")\\n\\n            \\n        # Initialize Gmail toolkit\\n        credentials = get_gmail_credentials(\\n            token_file=\"token.json\",\\n            scopes=[\"https://mail.google.com/\"],\\n            client_secrets_file=\"credentials.json\",\\n        )\\n        \\n        api_resource = build_resource_service(credentials=credentials)\\n        gmail_toolkit = GmailToolkit(api_resource=api_resource)\\n        \\n        tools = gmail_toolkit.get_tools()\\n        print(f\\'🚩DEBUG: send mail tool: {tools[1]}\\')\\n        # Compose email content\\n        subject = \"LinkedIn Post Ready for Publishing\"\\n        body = f\"\"\"\\n            Your LinkedIn post is ready:\\n\\n            {state.post_text}\\n\\n            Image URL: {state.image_url}\\n\\n            Best regards,\\n            LinkedIn Post Creator\\n            \"\"\"\\n        \\n        # Send email using Gmail toolkit\\n        send_message_tool = gmail_toolkit.get_tools()[1]  # send_gmail_message tool\\n        \\n        email_result = send_message_tool.run({\\n            \"to\": \"mohammed-mowina@outlook.com\",\\n            \"subject\": subject,\\n            \"message\": body\\n        })\\n        \\n        state.current_step = \"completed\"\\n        \\n    except Exception as e:\\n        state.error_message = f\"🚩Error sending email: {str(e)}\"\\n        state.current_step = \"error\"\\n    \\n    return state',\n",
       "   'commit_count': 6,\n",
       "   'branches': ['main']}]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage functions\n",
    "def demo_search():\n",
    "    \"\"\"Demo function to show how to use the search functionality.\"\"\"\n",
    "    \n",
    "    # Example searches\n",
    "    queries = [\n",
    "        \"How does the workflow work?\",\n",
    "        \"What are the main Python functions?\",\n",
    "        \"How to configure the application?\",\n",
    "        \"What dependencies are required?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== Repository Search Demo ===\\n\")\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"Query: {query}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        results = search_repo_code(query, top_k=3, similarity_threshold=0.6)\n",
    "        formatted_results = format_search_results(results, max_content_length=200)\n",
    "        print(formatted_results)\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Quick search function for interactive use\n",
    "def quick_search(query, show_content=True):\n",
    "    \"\"\"\n",
    "    Quick search function for interactive use in notebooks.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Search query\n",
    "        show_content (bool): Whether to show file content\n",
    "    \n",
    "    Returns:\n",
    "        None: Prints results directly\n",
    "    \"\"\"\n",
    "    results = search_repo_code(query, top_k=5, similarity_threshold= 0.5)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(results)} results for: '{query}'\\n\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. {result['file_path']} (Score: {result['similarity_score']:.3f})\")\n",
    "        if show_content:\n",
    "            content = result['content'][:300] + \"...\" if len(result['content']) > 300 else result['content']\n",
    "            print(f\"   Content preview: {content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohamed mowina\\Desktop\\Areeb\\DevOps-Analysis\\.venv\\lib\\site-packages\\vecs\\collection.py:506: UserWarning: Query does not have a covering index for cosine_distance. See Collection.create_index\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 results for: 'The generate text functionality'\n",
      "\n",
      "1. main.py (Score: 0.788)\n",
      "   Content preview: import sys\n",
      "from utils import load_env\n",
      "from workflow.graph import start_workflow, continue_workflow\n",
      "\n",
      "\n",
      "def main() -> None:\n",
      "    load_env()\n",
      "    if len(sys.argv) < 2:\n",
      "        print(\"Usage: python main.py \\\"Your post topic...\\\"\")\n",
      "        sys.exit(1)\n",
      "    topic = sys.argv[1]\n",
      "    thread_id = \"cli\"\n",
      "    state ...\n",
      "\n",
      "2. app.py (Score: 0.775)\n",
      "   Content preview: \"\"\"Chainlit GUI for LinkedIn Booster workflow.\"\"\"\n",
      "import chainlit as cl\n",
      "from typing import Dict, Any\n",
      "import asyncio\n",
      "from utils import load_env\n",
      "from workflow.graph import start_workflow, continue_workflow\n",
      "\n",
      "\n",
      "# Load environment variables\n",
      "load_env()\n",
      "\n",
      "# Store user sessions\n",
      "user_sessions: Dict[str, Dict[s...\n",
      "\n",
      "3. README.md (Score: 0.746)\n",
      "   Content preview: # LinkedIn-Booster 🚀\n",
      "\n",
      "An intelligent AI-powered LinkedIn post creation workflow that combines the power of LangChain, LangGraph, and multiple AI services to create engaging LinkedIn content with matching visuals.\n",
      "\n",
      "![LangGraph Workflow](images/LangGraph-workflow-diagram.png)\n",
      "\n",
      "## ✨ Features\n",
      "\n",
      "- **🤖 AI-...\n",
      "\n",
      "4. n8n/main (NO img2img).json (Score: 0.681)\n",
      "   Content preview: {\n",
      "  \"name\": \"main (NO img2img)\",\n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"parameters\": {\n",
      "        \"options\": {}\n",
      "      },\n",
      "      \"type\": \"@n8n/n8n-nodes-langchain.lmChatGoogleGemini\",\n",
      "      \"typeVersion\": 1,\n",
      "      \"position\": [\n",
      "        832,\n",
      "        352\n",
      "      ],\n",
      "      \"id\": \"575c62c0-07d4-4c73-9d70-4fdafe125e86\",\n",
      "    ...\n",
      "\n",
      "5. workflow/nodes.py (Score: 0.670)\n",
      "   Content preview: import os\n",
      "from typing import List\n",
      "\n",
      "from dotenv import load_dotenv\n",
      "load_dotenv()\n",
      "\n",
      "from langchain.chat_models import init_chat_model\n",
      "from langchain.schema import SystemMessage, HumanMessage\n",
      "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
      "from langgraph.errors import Gra...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quick_search(\"The generate text functionality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code Repository Search\n",
    "from langchain.tools import Tool\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "\n",
    "def code_repository_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search through code repositories using semantic similarity.\n",
    "    \n",
    "    This tool searches through stored repository code and documentation\n",
    "    to find relevant files and code snippets based on the query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query describing what you're looking for\n",
    "        \n",
    "    Returns:\n",
    "        str: JSON string containing search results with file paths, \n",
    "             similarity scores, and relevant code snippets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        vx = vecs.create_client(SUPABASE_DB_URL)\n",
    "        collection = vx.get_or_create_collection(name=\"repo_files\", dimension=1536)\n",
    "        \n",
    "        # Generate embedding for the query\n",
    "        query_embedding = model.embed_query(query)\n",
    "        \n",
    "        # Search for similar vectors (limit to top 5 for agent efficiency)\n",
    "        results = collection.query(\n",
    "            data=query_embedding,\n",
    "            limit=5,\n",
    "            include_value=True,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        \n",
    "        # Process results for agent consumption\n",
    "        agent_results = []\n",
    "        for result in results:\n",
    "            similarity_score = result[1]\n",
    "            metadata = result[2]\n",
    "            \n",
    "            # Only include results with decent similarity (>0.6)\n",
    "            if similarity_score >= 0.6:\n",
    "                # Truncate content for agent efficiency (max 1000 chars)\n",
    "                content = metadata.get('content', '')\n",
    "                if len(content) > 1000:\n",
    "                    content = content[:1000] + \"... [truncated]\"\n",
    "                \n",
    "                agent_results.append({\n",
    "                    'file_path': metadata.get('path', 'Unknown'),\n",
    "                    'repo_name': metadata.get('repo_name', 'Unknown'),\n",
    "                    'similarity_score': round(similarity_score, 3),\n",
    "                    'content_preview': content,\n",
    "                    'file_type': metadata.get('path', '').split('.')[-1] if '.' in metadata.get('path', '') else 'unknown'\n",
    "                })\n",
    "        \n",
    "        vx.disconnect()\n",
    "        \n",
    "        # Return structured JSON for agent\n",
    "        return json.dumps({\n",
    "            'query': query,\n",
    "            'results_found': len(agent_results),\n",
    "            'results': agent_results\n",
    "        }, indent=2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return json.dumps({\n",
    "            'error': f\"Search failed: {str(e)}\",\n",
    "            'query': query,\n",
    "            'results_found': 0,\n",
    "            'results': []\n",
    "        })\n",
    "\n",
    "# Create the LangChain Tool\n",
    "repository_search_tool = Tool(\n",
    "    name=\"repository_search\",\n",
    "    description=\"\"\"Search through code repositories to find relevant files, functions, and documentation. \n",
    "    Use this when you need to:\n",
    "    - Find specific code implementations\n",
    "    - Locate configuration files\n",
    "    - Search for documentation\n",
    "    - Find examples of how something is implemented\n",
    "    - Understand project structure\n",
    "    \n",
    "    Input should be a descriptive search query about what you're looking for.\"\"\",\n",
    "    func=code_repository_search\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohamed mowina\\Desktop\\Areeb\\DevOps-Analysis\\.venv\\lib\\site-packages\\vecs\\collection.py:506: UserWarning: Query does not have a covering index for cosine_distance. See Collection.create_index\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\n  \"query\": \"classify feedback\",\\n  \"results_found\": 5,\\n  \"results\": [\\n    {\\n      \"file_path\": \"workflow/graph.py\",\\n      \"repo_name\": \"tmpqv23rcta\",\\n      \"similarity_score\": 0.661,\\n      \"content_preview\": \"from langgraph.graph import StateGraph, END\\\\nfrom langgraph.checkpoint.memory import MemorySaver\\\\n\\\\nfrom workflow.state import WorkflowState\\\\nfrom workflow.nodes import (\\\\n    create_post_text,\\\\n    generate_image,\\\\n    classify_feedback,\\\\n    refine_text,\\\\n    refine_image,\\\\n    send_email,\\\\n)\\\\n\\\\n\\\\ndef route_feedback(state: WorkflowState) -> str:\\\\n    \\\\\"\\\\\"\\\\\"Route based on feedback classification.\\\\\"\\\\\"\\\\\"\\\\n    classification = state.classification.lower()\\\\n    if \\\\\"approved\\\\\" in classification:\\\\n        return \\\\\"approved\\\\\"\\\\n    if \\\\\"refine text\\\\\" in classification or \\\\\"refine_text\\\\\" in classification:\\\\n        return \\\\\"refine_text\\\\\"\\\\n    if \\\\\"refine image\\\\\" in classification or \\\\\"refine_image\\\\\" in classification:\\\\n        return \\\\\"refine_image\\\\\"\\\\n    if \\\\\"terminate\\\\\" in classification:\\\\n        return \\\\\"terminate\\\\\"\\\\n    return \\\\\"terminate\\\\\"\\\\n\\\\n\\\\ndef create_workflow():\\\\n    \\\\\"\\\\\"\\\\\"Create the LangGraph workflow with interrupts configured.\\\\\"\\\\\"\\\\\"\\\\n    workflow = StateGraph(WorkflowState)\\\\n    workflow.add_node(\\\\\"create_post\\\\\", create_post_text)\\\\n    workflow.... [truncated]\",\\n      \"file_type\": \"py\"\\n    },\\n    {\\n      \"file_path\": \"app.py\",\\n      \"repo_name\": \"tmpqv23rcta\",\\n      \"similarity_score\": 0.695,\\n      \"content_preview\": \"\\\\\"\\\\\"\\\\\"Chainlit GUI for LinkedIn Booster workflow.\\\\\"\\\\\"\\\\\"\\\\nimport chainlit as cl\\\\nfrom typing import Dict, Any\\\\nimport asyncio\\\\nfrom utils import load_env\\\\nfrom workflow.graph import start_workflow, continue_workflow\\\\n\\\\n\\\\n# Load environment variables\\\\nload_env()\\\\n\\\\n# Store user sessions\\\\nuser_sessions: Dict[str, Dict[str, Any]] = {}\\\\n\\\\n\\\\n@cl.on_chat_start\\\\nasync def start():\\\\n    \\\\\"\\\\\"\\\\\"Initialize the chat session.\\\\\"\\\\\"\\\\\"\\\\n    session_id = cl.user_session.get(\\\\\"id\\\\\")\\\\n    user_sessions[session_id] = {\\\\n        \\\\\"thread_id\\\\\": f\\\\\"chainlit_{session_id}\\\\\",\\\\n        \\\\\"current_state\\\\\": None,\\\\n        \\\\\"step\\\\\": \\\\\"initial\\\\\"\\\\n    }\\\\n    \\\\n    await cl.Message(\\\\n        content=\\\\\"\\\\ud83d\\\\ude80 **Welcome to LinkedIn Booster!**\\\\\\\\n\\\\\\\\n\\\\\"\\\\n                \\\\\"I\\'ll help you create engaging LinkedIn posts with AI-generated images.\\\\\\\\n\\\\\\\\n\\\\\"\\\\n                \\\\\"**How it works:**\\\\\\\\n\\\\\"\\\\n                \\\\\"1. Tell me your post topic or idea\\\\\\\\n\\\\\"\\\\n                \\\\\"2. I\\'ll generate a professional post and matching image\\\\\\\\n\\\\\"\\\\n                \\\\\"3. You can review and refine both text and image\\\\\\\\n\\\\\"\\\\n     ... [truncated]\",\\n      \"file_type\": \"py\"\\n    },\\n    {\\n      \"file_path\": \"workflow/nodes.py\",\\n      \"repo_name\": \"tmpqv23rcta\",\\n      \"similarity_score\": 0.732,\\n      \"content_preview\": \"import os\\\\nfrom typing import List\\\\n\\\\nfrom dotenv import load_dotenv\\\\nload_dotenv()\\\\n\\\\nfrom langchain.chat_models import init_chat_model\\\\nfrom langchain.schema import SystemMessage, HumanMessage\\\\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\\\\nfrom langgraph.errors import GraphRecursionError\\\\nfrom langgraph.prebuilt import create_react_agent\\\\n# Gmail imports will be done lazily in send_email function\\\\n\\\\nfrom workflow.state import WorkflowState\\\\nfrom workflow.tools import tavily_tool\\\\n\\\\n\\\\n# Initialize models\\\\ngemini_model = init_chat_model(\\\\\"gpt-4o-mini\\\\\", model_provider=\\\\\"openai\\\\\")\\\\n\\\\ndef create_post_text(state: WorkflowState) -> WorkflowState:\\\\n    \\\\\"\\\\\"\\\\\"Generate LinkedIn post text based on user input\\\\\"\\\\\"\\\\\"\\\\n    system_message = \\\\\"\\\\\"\\\\\"\\\\n    You are a professional LinkedIn content creator. Create engaging, professional LinkedIn posts.\\\\n\\\\n    Your tone should be:\\\\n    - Professional yet approachable\\\\n    - Concise and clear\\\\n    - Action-oriented with clear value\\\\n\\\\n    Format your post with:\\\\n   ... [truncated]\",\\n      \"file_type\": \"py\"\\n    },\\n    {\\n      \"file_path\": \"n8n/main (NO img2img).json\",\\n      \"repo_name\": \"tmpqv23rcta\",\\n      \"similarity_score\": 0.741,\\n      \"content_preview\": \"{\\\\n  \\\\\"name\\\\\": \\\\\"main (NO img2img)\\\\\",\\\\n  \\\\\"nodes\\\\\": [\\\\n    {\\\\n      \\\\\"parameters\\\\\": {\\\\n        \\\\\"options\\\\\": {}\\\\n      },\\\\n      \\\\\"type\\\\\": \\\\\"@n8n/n8n-nodes-langchain.lmChatGoogleGemini\\\\\",\\\\n      \\\\\"typeVersion\\\\\": 1,\\\\n      \\\\\"position\\\\\": [\\\\n        832,\\\\n        352\\\\n      ],\\\\n      \\\\\"id\\\\\": \\\\\"575c62c0-07d4-4c73-9d70-4fdafe125e86\\\\\",\\\\n      \\\\\"name\\\\\": \\\\\"Google Gemini Chat Model\\\\\",\\\\n      \\\\\"credentials\\\\\": {\\\\n        \\\\\"googlePalmApi\\\\\": {\\\\n          \\\\\"id\\\\\": \\\\\"mP5d64x2rkeTRzbi\\\\\",\\\\n          \\\\\"name\\\\\": \\\\\"Google Gemini(PaLM) Api account 2\\\\\"\\\\n        }\\\\n      }\\\\n    },\\\\n    {\\\\n      \\\\\"parameters\\\\\": {\\\\n        \\\\\"toolDescription\\\\\": \\\\\"Web search tool.\\\\\",\\\\n        \\\\\"method\\\\\": \\\\\"POST\\\\\",\\\\n        \\\\\"url\\\\\": \\\\\"https://api.tavily.com/search\\\\\",\\\\n        \\\\\"sendHeaders\\\\\": true,\\\\n        \\\\\"headerParameters\\\\\": {\\\\n          \\\\\"parameters\\\\\": [\\\\n            {\\\\n              \\\\\"name\\\\\": \\\\\"Authorization\\\\\",\\\\n              \\\\\"value\\\\\": \\\\\"Bearer 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\\\\\"\\\\n            }\\\\n          ]\\\\n        },\\\\n        \\\\\"sendBody\\\\\": true,\\\\n        \\\\\"bodyParameters\\\\\": {\\\\n          \\\\\"parameters\\\\\": [\\\\n            {\\\\n     ... [truncated]\",\\n      \"file_type\": \"json\"\\n    },\\n    {\\n      \"file_path\": \"README.md\",\\n      \"repo_name\": \"tmpqv23rcta\",\\n      \"similarity_score\": 0.774,\\n      \"content_preview\": \"# LinkedIn-Booster \\\\ud83d\\\\ude80\\\\n\\\\nAn intelligent AI-powered LinkedIn post creation workflow that combines the power of LangChain, LangGraph, and multiple AI services to create engaging LinkedIn content with matching visuals.\\\\n\\\\n![LangGraph Workflow](images/LangGraph-workflow-diagram.png)\\\\n\\\\n## \\\\u2728 Features\\\\n\\\\n- **\\\\ud83e\\\\udd16 AI-Powered Generation**: Google Gemini for high-quality LinkedIn post drafting\\\\n- **\\\\ud83d\\\\udd0d Web Research**: Tavily search integration for up-to-date context and insights\\\\n- **\\\\ud83d\\\\udd04 Interactive Feedback Loop**: Intelligent feedback classification and content refinement\\\\n- **\\\\ud83c\\\\udfa8 Image Generation**: DALL\\\\u00b7E 3 integration for compelling visual content\\\\n- **\\\\ud83d\\\\udce7 Email Delivery**: Gmail API integration for seamless content distribution\\\\n- **\\\\ud83d\\\\udda5\\\\ufe0f Multiple Interfaces**: CLI and web-based Chainlit UI options\\\\n- **\\\\ud83d\\\\udcca Workflow Visualization**: LangGraph Studio integration for workflow monitoring\\\\n\\\\n## \\\\ud83c\\\\udfd7\\\\ufe0f Architecture\\\\n\\\\nThe project follows a modular architecture with clear separation of concerns:\\\\n\\\\n```\\\\nLinkedIn-Booster/\\\\n\\\\u251c\\\\u2500\\\\u2500 \\\\ud83d\\\\udcc4 main.py ... [truncated]\",\\n      \"file_type\": \"md\"\\n    }\\n  ]\\n}'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_repository_search(\"classify feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohamed mowina\\Desktop\\Areeb\\DevOps-Analysis\\.venv\\lib\\site-packages\\vecs\\collection.py:506: UserWarning: Query does not have a covering index for cosine_distance. See Collection.create_index\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langgraph.errors import GraphRecursionError\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "max_iterations = 3\n",
    "recursion_limit = 2 * max_iterations + 1\n",
    "agent = create_react_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=[repository_search_tool]\n",
    ")\n",
    "\n",
    "try:\n",
    "    response = agent.invoke(\n",
    "        {\"messages\": [\n",
    "            {'role': \"system\", \"content\": \"\"\" \n",
    "            You are a helpfull DevOps assistant you are spetialized in searching the code repositories and analyze code.\"\"\"},\n",
    "            {\"role\": \"user\", \"content\": \"What part in the repo is responsible for the classifing the user feadback\"}]},\n",
    "        {\"recursion_limit\": recursion_limit},\n",
    "    )\n",
    "except GraphRecursionError:\n",
    "    print(\"Agent stopped due to max iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      " \n",
      "            You are a helpfull DevOps assistant you are spetialized in searching the code repositories and analyze code.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What part in the repo is responsible for the classifing the user feadback\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  repository_search (call_aL0YJEAr3mK5XGFuPudaDpSu)\n",
      " Call ID: call_aL0YJEAr3mK5XGFuPudaDpSu\n",
      "  Args:\n",
      "    __arg1: classifying user feedback\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: repository_search\n",
      "\n",
      "{\n",
      "  \"query\": \"classifying user feedback\",\n",
      "  \"results_found\": 5,\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"file_path\": \"workflow/graph.py\",\n",
      "      \"repo_name\": \"tmpqv23rcta\",\n",
      "      \"similarity_score\": 0.614,\n",
      "      \"content_preview\": \"from langgraph.graph import StateGraph, END\\nfrom langgraph.checkpoint.memory import MemorySaver\\n\\nfrom workflow.state import WorkflowState\\nfrom workflow.nodes import (\\n    create_post_text,\\n    generate_image,\\n    classify_feedback,\\n    refine_text,\\n    refine_image,\\n    send_email,\\n)\\n\\n\\ndef route_feedback(state: WorkflowState) -> str:\\n    \\\"\\\"\\\"Route based on feedback classification.\\\"\\\"\\\"\\n    classification = state.classification.lower()\\n    if \\\"approved\\\" in classification:\\n        return \\\"approved\\\"\\n    if \\\"refine text\\\" in classification or \\\"refine_text\\\" in classification:\\n        return \\\"refine_text\\\"\\n    if \\\"refine image\\\" in classification or \\\"refine_image\\\" in classification:\\n        return \\\"refine_image\\\"\\n    if \\\"terminate\\\" in classification:\\n        return \\\"terminate\\\"\\n    return \\\"terminate\\\"\\n\\n\\ndef create_workflow():\\n    \\\"\\\"\\\"Create the LangGraph workflow with interrupts configured.\\\"\\\"\\\"\\n    workflow = StateGraph(WorkflowState)\\n    workflow.add_node(\\\"create_post\\\", create_post_text)\\n    workflow.... [truncated]\",\n",
      "      \"file_type\": \"py\"\n",
      "    },\n",
      "    {\n",
      "      \"file_path\": \"app.py\",\n",
      "      \"repo_name\": \"tmpqv23rcta\",\n",
      "      \"similarity_score\": 0.7,\n",
      "      \"content_preview\": \"\\\"\\\"\\\"Chainlit GUI for LinkedIn Booster workflow.\\\"\\\"\\\"\\nimport chainlit as cl\\nfrom typing import Dict, Any\\nimport asyncio\\nfrom utils import load_env\\nfrom workflow.graph import start_workflow, continue_workflow\\n\\n\\n# Load environment variables\\nload_env()\\n\\n# Store user sessions\\nuser_sessions: Dict[str, Dict[str, Any]] = {}\\n\\n\\n@cl.on_chat_start\\nasync def start():\\n    \\\"\\\"\\\"Initialize the chat session.\\\"\\\"\\\"\\n    session_id = cl.user_session.get(\\\"id\\\")\\n    user_sessions[session_id] = {\\n        \\\"thread_id\\\": f\\\"chainlit_{session_id}\\\",\\n        \\\"current_state\\\": None,\\n        \\\"step\\\": \\\"initial\\\"\\n    }\\n    \\n    await cl.Message(\\n        content=\\\"\\ud83d\\ude80 **Welcome to LinkedIn Booster!**\\\\n\\\\n\\\"\\n                \\\"I'll help you create engaging LinkedIn posts with AI-generated images.\\\\n\\\\n\\\"\\n                \\\"**How it works:**\\\\n\\\"\\n                \\\"1. Tell me your post topic or idea\\\\n\\\"\\n                \\\"2. I'll generate a professional post and matching image\\\\n\\\"\\n                \\\"3. You can review and refine both text and image\\\\n\\\"\\n     ... [truncated]\",\n",
      "      \"file_type\": \"py\"\n",
      "    },\n",
      "    {\n",
      "      \"file_path\": \"n8n/main (NO img2img).json\",\n",
      "      \"repo_name\": \"tmpqv23rcta\",\n",
      "      \"similarity_score\": 0.706,\n",
      "      \"content_preview\": \"{\\n  \\\"name\\\": \\\"main (NO img2img)\\\",\\n  \\\"nodes\\\": [\\n    {\\n      \\\"parameters\\\": {\\n        \\\"options\\\": {}\\n      },\\n      \\\"type\\\": \\\"@n8n/n8n-nodes-langchain.lmChatGoogleGemini\\\",\\n      \\\"typeVersion\\\": 1,\\n      \\\"position\\\": [\\n        832,\\n        352\\n      ],\\n      \\\"id\\\": \\\"575c62c0-07d4-4c73-9d70-4fdafe125e86\\\",\\n      \\\"name\\\": \\\"Google Gemini Chat Model\\\",\\n      \\\"credentials\\\": {\\n        \\\"googlePalmApi\\\": {\\n          \\\"id\\\": \\\"mP5d64x2rkeTRzbi\\\",\\n          \\\"name\\\": \\\"Google Gemini(PaLM) Api account 2\\\"\\n        }\\n      }\\n    },\\n    {\\n      \\\"parameters\\\": {\\n        \\\"toolDescription\\\": \\\"Web search tool.\\\",\\n        \\\"method\\\": \\\"POST\\\",\\n        \\\"url\\\": \\\"https://api.tavily.com/search\\\",\\n        \\\"sendHeaders\\\": true,\\n        \\\"headerParameters\\\": {\\n          \\\"parameters\\\": [\\n            {\\n              \\\"name\\\": \\\"Authorization\\\",\\n              \\\"value\\\": \\\"Bearer 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\\\"\\n            }\\n          ]\\n        },\\n        \\\"sendBody\\\": true,\\n        \\\"bodyParameters\\\": {\\n          \\\"parameters\\\": [\\n            {\\n     ... [truncated]\",\n",
      "      \"file_type\": \"json\"\n",
      "    },\n",
      "    {\n",
      "      \"file_path\": \"workflow/nodes.py\",\n",
      "      \"repo_name\": \"tmpqv23rcta\",\n",
      "      \"similarity_score\": 0.71,\n",
      "      \"content_preview\": \"import os\\nfrom typing import List\\n\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain.schema import SystemMessage, HumanMessage\\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\\nfrom langgraph.errors import GraphRecursionError\\nfrom langgraph.prebuilt import create_react_agent\\n# Gmail imports will be done lazily in send_email function\\n\\nfrom workflow.state import WorkflowState\\nfrom workflow.tools import tavily_tool\\n\\n\\n# Initialize models\\ngemini_model = init_chat_model(\\\"gpt-4o-mini\\\", model_provider=\\\"openai\\\")\\n\\ndef create_post_text(state: WorkflowState) -> WorkflowState:\\n    \\\"\\\"\\\"Generate LinkedIn post text based on user input\\\"\\\"\\\"\\n    system_message = \\\"\\\"\\\"\\n    You are a professional LinkedIn content creator. Create engaging, professional LinkedIn posts.\\n\\n    Your tone should be:\\n    - Professional yet approachable\\n    - Concise and clear\\n    - Action-oriented with clear value\\n\\n    Format your post with:\\n   ... [truncated]\",\n",
      "      \"file_type\": \"py\"\n",
      "    },\n",
      "    {\n",
      "      \"file_path\": \"workflow/state.py\",\n",
      "      \"repo_name\": \"tmpqv23rcta\",\n",
      "      \"similarity_score\": 0.727,\n",
      "      \"content_preview\": \"from dataclasses import dataclass\\nfrom typing import Optional\\n\\n\\n@dataclass\\nclass WorkflowState:\\n    \\\"\\\"\\\"State management for the LinkedIn post creation workflow.\\\"\\\"\\\"\\n    user_input: str = \\\"\\\"\\n    post_text: str = \\\"\\\"\\n    image_prompt: str = \\\"\\\"\\n    image_data: Optional[bytes] = None\\n    image_url: str = \\\"\\\"\\n    user_feedback: str = \\\"\\\"\\n    classification: str = \\\"\\\"\\n    refined_text: str = \\\"\\\"\\n    refined_image_data: Optional[bytes] = None\\n    current_step: str = \\\"start\\\"\\n    error_message: str = \\\"\\\"\\n    final_post_ready: bool = False\\n\\n\\n\",\n",
      "      \"file_type\": \"py\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The repository contains several files that are involved in classifying user feedback. Here are the relevant parts:\n",
      "\n",
      "1. **File: `workflow/graph.py`**\n",
      "   - This file contains a function `route_feedback`, which routes based on feedback classification. It checks the classification string and routes it accordingly. \n",
      "   - **Function Preview**:\n",
      "     ```python\n",
      "     def route_feedback(state: WorkflowState) -> str:\n",
      "         classification = state.classification.lower()\n",
      "         # Logic to classify feedback\n",
      "     ```\n",
      "\n",
      "2. **File: `workflow/state.py`**\n",
      "   - This file defines the `WorkflowState` class, which includes a property `classification` that likely holds the classified feedback information.\n",
      "   - **Class Preview**:\n",
      "     ```python\n",
      "     @dataclass\n",
      "     class WorkflowState:\n",
      "         classification: str = \"\"\n",
      "     ```\n",
      "\n",
      "3. **File: `workflow/nodes.py`**\n",
      "   - Contains functions related to workflow nodes such as `create_post_text`, which may be called after classifying the user feedback, although it primarily focuses on generating post text.\n",
      "   \n",
      "4. **File: `app.py`**\n",
      "   - This file initializes the chat session and likely interfaces with the user's input and feedback classification indirectly.\n",
      "\n",
      "5. **File: `n8n/main (NO img2img).json`**\n",
      "   - This JSON file appears to involve workflow definitions but might not directly relate to user feedback classification.\n",
      "\n",
      "The primary logic for classifying user feedback is in **`workflow/graph.py`**, specifically within the `route_feedback` function. The overall state management that includes feedback classification can also be found in **`workflow/state.py`**.\n"
     ]
    }
   ],
   "source": [
    "for m in response['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: More specific tool for code analysis\n",
    "def analyze_code_structure(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze code repository structure and find specific implementations.\n",
    "    \n",
    "    Optimized for finding:\n",
    "    - Function definitions\n",
    "    - Class implementations  \n",
    "    - Configuration patterns\n",
    "    - API endpoints\n",
    "    - Database schemas\n",
    "    \n",
    "    Args:\n",
    "        query (str): What you want to analyze (e.g., \"main workflow functions\", \"API routes\", \"database models\")\n",
    "        \n",
    "    Returns:\n",
    "        str: Structured analysis results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect and search\n",
    "        vx = vecs.create_client(SUPABASE_DB_URL)\n",
    "        collection = vx.get_or_create_collection(name=\"repo_files\", dimension=1536)\n",
    "        \n",
    "        query_embedding = model.embed_query(query)\n",
    "        results = collection.query(\n",
    "            data=query_embedding,\n",
    "            limit=3,  # Fewer results for focused analysis\n",
    "            include_value=True,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        \n",
    "        analysis_results = {\n",
    "            'query': query,\n",
    "            'analysis': [],\n",
    "            'summary': ''\n",
    "        }\n",
    "        \n",
    "        for result in results:\n",
    "            similarity_score = result[1]\n",
    "            metadata = result[2]\n",
    "            \n",
    "            if similarity_score >= 0.65:  # Higher threshold for analysis\n",
    "                file_path = metadata.get('path', '')\n",
    "                content = metadata.get('content', '')\n",
    "                \n",
    "                # Extract key information based on file type\n",
    "                file_analysis = {\n",
    "                    'file': file_path,\n",
    "                    'type': file_path.split('.')[-1] if '.' in file_path else 'unknown',\n",
    "                    'relevance': round(similarity_score, 3),\n",
    "                    'key_elements': []\n",
    "                }\n",
    "                \n",
    "                # Simple pattern matching for common code elements\n",
    "                if '.py' in file_path:\n",
    "                    # Find function definitions\n",
    "                    import re\n",
    "                    functions = re.findall(r'def\\s+(\\w+)\\s*\\(', content)\n",
    "                    classes = re.findall(r'class\\s+(\\w+)\\s*[\\(:]', content)\n",
    "                    if functions:\n",
    "                        file_analysis['key_elements'].append(f\"Functions: {', '.join(functions[:5])}\")\n",
    "                    if classes:\n",
    "                        file_analysis['key_elements'].append(f\"Classes: {', '.join(classes[:3])}\")\n",
    "                \n",
    "                elif '.js' in file_path or '.ts' in file_path:\n",
    "                    # Find function definitions and exports\n",
    "                    functions = re.findall(r'function\\s+(\\w+)\\s*\\(', content)\n",
    "                    exports = re.findall(r'export\\s+(?:default\\s+)?(?:function\\s+)?(\\w+)', content)\n",
    "                    if functions:\n",
    "                        file_analysis['key_elements'].append(f\"Functions: {', '.join(functions[:5])}\")\n",
    "                    if exports:\n",
    "                        file_analysis['key_elements'].append(f\"Exports: {', '.join(exports[:3])}\")\n",
    "                \n",
    "                # Add content preview\n",
    "                preview = content[:500] + \"...\" if len(content) > 500 else content\n",
    "                file_analysis['preview'] = preview\n",
    "                \n",
    "                analysis_results['analysis'].append(file_analysis)\n",
    "        \n",
    "        # Generate summary\n",
    "        if analysis_results['analysis']:\n",
    "            file_count = len(analysis_results['analysis'])\n",
    "            file_types = list(set([item['type'] for item in analysis_results['analysis']]))\n",
    "            analysis_results['summary'] = f\"Found {file_count} relevant files of types: {', '.join(file_types)}\"\n",
    "        else:\n",
    "            analysis_results['summary'] = \"No relevant code found for the query\"\n",
    "        \n",
    "        vx.disconnect()\n",
    "        return json.dumps(analysis_results, indent=2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return json.dumps({\n",
    "            'error': f\"Analysis failed: {str(e)}\",\n",
    "            'query': query,\n",
    "            'analysis': [],\n",
    "            'summary': 'Analysis failed due to error'\n",
    "        })\n",
    "\n",
    "# Create the analysis tool\n",
    "code_analysis_tool = Tool(\n",
    "    name=\"code_analysis\",\n",
    "    description=\"\"\"Analyze code repository structure and implementations. \n",
    "    Use this when you need to:\n",
    "    - Understand how specific features are implemented\n",
    "    - Find function and class definitions\n",
    "    - Analyze code patterns and structure\n",
    "    - Get an overview of specific modules or components\n",
    "    \n",
    "    Input should describe what code structure or implementation you want to analyze.\"\"\",\n",
    "    func=analyze_code_structure\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Codebase Search Tool:\n",
      "==================================================\n",
      "\n",
      "Query: workflow implementation\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohamed mowina\\Desktop\\Areeb\\DevOps-Analysis\\.venv\\lib\\site-packages\\vecs\\collection.py:506: UserWarning: Query does not have a covering index for cosine_distance. See Collection.create_index\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 relevant files:\n",
      "\n",
      "File: workflow/graph.py (Score: 0.61)\n",
      "Content: Functions: route_feedback, create_workflow, start_workflow...\n",
      "\n",
      "File: workflow/nodes.py (Score: 0.62)\n",
      "Content: Functions: create_post_text, generate_image, classify_feedback...\n",
      "\n",
      "File: workflow/state.py (Score: 0.63)\n",
      "Content:  Classes: from, class...\n",
      "\n",
      "\n",
      "Query: main Python functions\n",
      "------------------------------\n",
      "Found 2 relevant files:\n",
      "\n",
      "File: main.py (Score: 0.66)\n",
      "Content: Functions: main...\n",
      "\n",
      "File: studio/langgraph.json (Score: 0.73)\n",
      "Content: {     \"dockerfile_lines\": [],     \"graphs\": {       \"chatbot\": \"./graph.py:app\"     },     \"env\": \"./.env\",     \"python_version\": \"3.11\",     \"depende...\n",
      "\n",
      "\n",
      "Query: configuration files\n",
      "------------------------------\n",
      "Found 3 relevant files:\n",
      "\n",
      "File: utils/config.py (Score: 0.72)\n",
      "Content: Functions: load_env Classes: from, class...\n",
      "\n",
      "File: studio/langgraph.json (Score: 0.73)\n",
      "Content: {     \"dockerfile_lines\": [],     \"graphs\": {       \"chatbot\": \"./graph.py:app\"     },     \"env\": \"./.env\",     \"python_version\": \"3.11\",     \"depende...\n",
      "\n",
      "File: n8n/main (NO img2img).json (Score: 0.76)\n",
      "Content: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}       },       \"type\": \"@n8n/n8n-nodes-langchain.lmCha...\n",
      "\n",
      "\n",
      "Query: API endpoints\n",
      "------------------------------\n",
      "Found 3 relevant files:\n",
      "\n",
      "File: n8n/main (NO img2img).json (Score: 0.68)\n",
      "Content: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}       },       \"type\": \"@n8n/n8n-nodes-langchain.lmCha...\n",
      "\n",
      "File: requirements.txt (Score: 0.76)\n",
      "Content: chainlit>=1.0.0 langchain>=0.1.0 langchain-community>=0.1.0 langchain-google-genai>=1.0.0 langchain-google-community>=0.1.0 langgraph>=0.0.40 requests...\n",
      "\n",
      "File: README.md (Score: 0.80)\n",
      "Content: # LinkedIn-Booster 🚀  An intelligent AI-powered LinkedIn post creation workflow that combines the po...\n",
      "\n",
      "\n",
      "Query: database connection\n",
      "------------------------------\n",
      "Found 3 relevant files:\n",
      "\n",
      "File: n8n/main (NO img2img).json (Score: 0.78)\n",
      "Content: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}       },       \"type\": \"@n8n/n8n-nodes-langchain.lmCha...\n",
      "\n",
      "File: workflow/__init__.py (Score: 0.83)\n",
      "Content: ...\n",
      "\n",
      "File: workflow/tools.py (Score: 0.84)\n",
      "Content: Functions: tavily_search...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simplified tool optimized for ReAct agents\n",
    "def search_codebase(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Simple codebase search tool optimized for ReAct agents.\n",
    "    Returns concise, actionable results that agents can easily process.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Search query for code, documentation, or configuration\n",
    "        \n",
    "    Returns:\n",
    "        str: Concise search results with file paths and key information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        vx = vecs.create_client(SUPABASE_DB_URL)\n",
    "        collection = vx.get_or_create_collection(name=\"repo_files\", dimension=1536)\n",
    "        \n",
    "        query_embedding = model.embed_query(query)\n",
    "        results = collection.query(\n",
    "            data=query_embedding,\n",
    "            limit=3,  # Keep it focused for agent efficiency\n",
    "            include_value=True,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        \n",
    "        if not results or len(results) == 0:\n",
    "            return f\"No relevant code found for query: '{query}'\"\n",
    "        \n",
    "        # Format results for agent consumption\n",
    "        formatted_results = []\n",
    "        for result in results:\n",
    "            similarity_score = result[1]\n",
    "            metadata = result[2]\n",
    "            \n",
    "            if similarity_score >= 0.6:\n",
    "                file_path = metadata.get('path', 'Unknown')\n",
    "                content = metadata.get('content', '')\n",
    "                \n",
    "                # Extract key lines or functions\n",
    "                key_info = \"\"\n",
    "                if '.py' in file_path:\n",
    "                    # Extract function/class definitions\n",
    "                    import re\n",
    "                    functions = re.findall(r'def\\s+(\\w+)', content)\n",
    "                    classes = re.findall(r'class\\s+(\\w+)', content)\n",
    "                    if functions:\n",
    "                        key_info = f\"Functions: {', '.join(functions[:3])}\"\n",
    "                    if classes:\n",
    "                        key_info += f\" Classes: {', '.join(classes[:2])}\"\n",
    "                elif '.md' in file_path:\n",
    "                    # Extract first few lines for documentation\n",
    "                    lines = content.split('\\n')[:3]\n",
    "                    key_info = ' '.join(lines).strip()[:100]\n",
    "                else:\n",
    "                    # General content preview\n",
    "                    key_info = content[:150].replace('\\n', ' ').strip()\n",
    "                \n",
    "                formatted_results.append(\n",
    "                    f\"File: {file_path} (Score: {similarity_score:.2f})\\n\"\n",
    "                    f\"Content: {key_info}...\"\n",
    "                )\n",
    "        \n",
    "        vx.disconnect()\n",
    "        \n",
    "        if not formatted_results:\n",
    "            return f\"No sufficiently relevant results found for: '{query}'\"\n",
    "        \n",
    "        return f\"Found {len(formatted_results)} relevant files:\\n\\n\" + \"\\n\\n\".join(formatted_results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Search error: {str(e)}\"\n",
    "\n",
    "# The optimal ReAct agent tool\n",
    "codebase_search_tool = Tool(\n",
    "    name=\"search_codebase\",\n",
    "    description=\"\"\"Search through code repositories to find relevant files and implementations.\n",
    "    \n",
    "    Use this tool when you need to:\n",
    "    - Find specific functions or classes\n",
    "    - Locate configuration files  \n",
    "    - Search documentation\n",
    "    - Understand how features are implemented\n",
    "    - Find code examples\n",
    "    \n",
    "    The tool returns file paths with relevance scores and key content snippets.\n",
    "    Input: A descriptive search query about what you're looking for.\"\"\",\n",
    "    func=search_codebase\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Test function for the tool\n",
    "def test_codebase_search():\n",
    "    \"\"\"Test the codebase search functionality.\"\"\"\n",
    "    test_queries = [\n",
    "        \"workflow implementation\",\n",
    "        \"main Python functions\", \n",
    "        \"configuration files\",\n",
    "        \"API endpoints\",\n",
    "        \"database connection\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing Codebase Search Tool:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(\"-\" * 30)\n",
    "        result = search_codebase(query)\n",
    "        print(result)\n",
    "        print()\n",
    "\n",
    "# Test the tool\n",
    "test_codebase_search()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='How does the workflow functions', additional_kwargs={}, response_metadata={}, id='61c3b5d1-a339-41a6-8db6-3b27d500d0e3')]}\n",
      "\u001b[1m[updates]\u001b[0m {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'function': {'arguments': '{\"__arg1\":\"workflow functions\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 147, 'total_tokens': 165, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKIxsCWmCgNIGkXEne1ZpjljplW', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--87467977-2c56-4adf-b52b-996840b20624-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow functions'}, 'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'type': 'tool_call'}], usage_metadata={'input_tokens': 147, 'output_tokens': 18, 'total_tokens': 165, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='How does the workflow functions', additional_kwargs={}, response_metadata={}, id='61c3b5d1-a339-41a6-8db6-3b27d500d0e3'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'function': {'arguments': '{\"__arg1\":\"workflow functions\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 147, 'total_tokens': 165, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKIxsCWmCgNIGkXEne1ZpjljplW', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--87467977-2c56-4adf-b52b-996840b20624-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow functions'}, 'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'type': 'tool_call'}], usage_metadata={'input_tokens': 147, 'output_tokens': 18, 'total_tokens': 165, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohamed mowina\\Desktop\\Areeb\\DevOps-Analysis\\.venv\\lib\\site-packages\\vecs\\collection.py:506: UserWarning: Query does not have a covering index for cosine_distance. See Collection.create_index\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[updates]\u001b[0m {'tools': {'messages': [ToolMessage(content='Found 3 relevant files:\\n\\nFile: main.py (Score: 0.61)\\nContent: Functions: main...\\n\\nFile: workflow/__init__.py (Score: 0.62)\\nContent: ...\\n\\nFile: workflow/graph.py (Score: 0.63)\\nContent: Functions: route_feedback, create_workflow, start_workflow...', name='search_codebase', id='8fb79245-f351-4d70-b1df-54c4a2fbfc36', tool_call_id='call_rMhuVcO5Ra7jnGzzBCMwfqHW')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='How does the workflow functions', additional_kwargs={}, response_metadata={}, id='61c3b5d1-a339-41a6-8db6-3b27d500d0e3'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'function': {'arguments': '{\"__arg1\":\"workflow functions\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 147, 'total_tokens': 165, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKIxsCWmCgNIGkXEne1ZpjljplW', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--87467977-2c56-4adf-b52b-996840b20624-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow functions'}, 'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'type': 'tool_call'}], usage_metadata={'input_tokens': 147, 'output_tokens': 18, 'total_tokens': 165, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Found 3 relevant files:\\n\\nFile: main.py (Score: 0.61)\\nContent: Functions: main...\\n\\nFile: workflow/__init__.py (Score: 0.62)\\nContent: ...\\n\\nFile: workflow/graph.py (Score: 0.63)\\nContent: Functions: route_feedback, create_workflow, start_workflow...', name='search_codebase', id='8fb79245-f351-4d70-b1df-54c4a2fbfc36', tool_call_id='call_rMhuVcO5Ra7jnGzzBCMwfqHW')]}\n",
      "\u001b[1m[updates]\u001b[0m {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Jb9qe6Gltidb8HZUdKrqfdnJ', 'function': {'arguments': '{\"__arg1\": \"main\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_rsbvLaiXafqidyaN11DDQoGC', 'function': {'arguments': '{\"__arg1\": \"route_feedback\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_9jAlF5eFb05pRIUD5PFK4yXA', 'function': {'arguments': '{\"__arg1\": \"create_workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_fjYLfl9tZe6FsIlRwbzdxPPy', 'function': {'arguments': '{\"__arg1\": \"start_workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 245, 'total_tokens': 334, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKPGvgyVlrFJhgYlHlBKUbTdIf4', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--c286e36f-edec-4299-9f09-4740e5a3a6d4-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'main'}, 'id': 'call_Jb9qe6Gltidb8HZUdKrqfdnJ', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'route_feedback'}, 'id': 'call_rsbvLaiXafqidyaN11DDQoGC', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'create_workflow'}, 'id': 'call_9jAlF5eFb05pRIUD5PFK4yXA', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'start_workflow'}, 'id': 'call_fjYLfl9tZe6FsIlRwbzdxPPy', 'type': 'tool_call'}], usage_metadata={'input_tokens': 245, 'output_tokens': 89, 'total_tokens': 334, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='How does the workflow functions', additional_kwargs={}, response_metadata={}, id='61c3b5d1-a339-41a6-8db6-3b27d500d0e3'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'function': {'arguments': '{\"__arg1\":\"workflow functions\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 147, 'total_tokens': 165, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKIxsCWmCgNIGkXEne1ZpjljplW', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--87467977-2c56-4adf-b52b-996840b20624-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow functions'}, 'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'type': 'tool_call'}], usage_metadata={'input_tokens': 147, 'output_tokens': 18, 'total_tokens': 165, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Found 3 relevant files:\\n\\nFile: main.py (Score: 0.61)\\nContent: Functions: main...\\n\\nFile: workflow/__init__.py (Score: 0.62)\\nContent: ...\\n\\nFile: workflow/graph.py (Score: 0.63)\\nContent: Functions: route_feedback, create_workflow, start_workflow...', name='search_codebase', id='8fb79245-f351-4d70-b1df-54c4a2fbfc36', tool_call_id='call_rMhuVcO5Ra7jnGzzBCMwfqHW'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Jb9qe6Gltidb8HZUdKrqfdnJ', 'function': {'arguments': '{\"__arg1\": \"main\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_rsbvLaiXafqidyaN11DDQoGC', 'function': {'arguments': '{\"__arg1\": \"route_feedback\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_9jAlF5eFb05pRIUD5PFK4yXA', 'function': {'arguments': '{\"__arg1\": \"create_workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_fjYLfl9tZe6FsIlRwbzdxPPy', 'function': {'arguments': '{\"__arg1\": \"start_workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 245, 'total_tokens': 334, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKPGvgyVlrFJhgYlHlBKUbTdIf4', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--c286e36f-edec-4299-9f09-4740e5a3a6d4-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'main'}, 'id': 'call_Jb9qe6Gltidb8HZUdKrqfdnJ', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'route_feedback'}, 'id': 'call_rsbvLaiXafqidyaN11DDQoGC', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'create_workflow'}, 'id': 'call_9jAlF5eFb05pRIUD5PFK4yXA', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'start_workflow'}, 'id': 'call_fjYLfl9tZe6FsIlRwbzdxPPy', 'type': 'tool_call'}], usage_metadata={'input_tokens': 245, 'output_tokens': 89, 'total_tokens': 334, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohamed mowina\\Desktop\\Areeb\\DevOps-Analysis\\.venv\\lib\\site-packages\\vecs\\collection.py:506: UserWarning: Query does not have a covering index for cosine_distance. See Collection.create_index\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[updates]\u001b[0m {'tools': {'messages': [ToolMessage(content=\"No sufficiently relevant results found for: 'create_workflow'\", name='search_codebase', tool_call_id='call_9jAlF5eFb05pRIUD5PFK4yXA')]}}\n",
      "\u001b[1m[updates]\u001b[0m {'tools': {'messages': [ToolMessage(content='Found 3 relevant files:\\n\\nFile: n8n/main (NO img2img).json (Score: 0.72)\\nContent: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}       },       \"type\": \"@n8n/n8n-nodes-langchain.lmCha...\\n\\nFile: main.py (Score: 0.77)\\nContent: Functions: main...\\n\\nFile: requirements.txt (Score: 0.80)\\nContent: chainlit>=1.0.0 langchain>=0.1.0 langchain-community>=0.1.0 langchain-google-genai>=1.0.0 langchain-google-community>=0.1.0 langgraph>=0.0.40 requests...', name='search_codebase', tool_call_id='call_Jb9qe6Gltidb8HZUdKrqfdnJ')]}}\n",
      "\u001b[1m[updates]\u001b[0m {'tools': {'messages': [ToolMessage(content=\"No sufficiently relevant results found for: 'start_workflow'\", name='search_codebase', tool_call_id='call_fjYLfl9tZe6FsIlRwbzdxPPy')]}}\n",
      "\u001b[1m[updates]\u001b[0m {'tools': {'messages': [ToolMessage(content='Found 3 relevant files:\\n\\nFile: workflow/graph.py (Score: 0.69)\\nContent: Functions: route_feedback, create_workflow, start_workflow...\\n\\nFile: n8n/main (NO img2img).json (Score: 0.79)\\nContent: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}       },       \"type\": \"@n8n/n8n-nodes-langchain.lmCha...\\n\\nFile: app.py (Score: 0.79)\\nContent: Functions: start, main, handle_initial_request...', name='search_codebase', tool_call_id='call_rsbvLaiXafqidyaN11DDQoGC')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='How does the workflow functions', additional_kwargs={}, response_metadata={}, id='61c3b5d1-a339-41a6-8db6-3b27d500d0e3'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'function': {'arguments': '{\"__arg1\":\"workflow functions\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 147, 'total_tokens': 165, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKIxsCWmCgNIGkXEne1ZpjljplW', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--87467977-2c56-4adf-b52b-996840b20624-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow functions'}, 'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'type': 'tool_call'}], usage_metadata={'input_tokens': 147, 'output_tokens': 18, 'total_tokens': 165, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Found 3 relevant files:\\n\\nFile: main.py (Score: 0.61)\\nContent: Functions: main...\\n\\nFile: workflow/__init__.py (Score: 0.62)\\nContent: ...\\n\\nFile: workflow/graph.py (Score: 0.63)\\nContent: Functions: route_feedback, create_workflow, start_workflow...', name='search_codebase', id='8fb79245-f351-4d70-b1df-54c4a2fbfc36', tool_call_id='call_rMhuVcO5Ra7jnGzzBCMwfqHW'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Jb9qe6Gltidb8HZUdKrqfdnJ', 'function': {'arguments': '{\"__arg1\": \"main\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_rsbvLaiXafqidyaN11DDQoGC', 'function': {'arguments': '{\"__arg1\": \"route_feedback\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_9jAlF5eFb05pRIUD5PFK4yXA', 'function': {'arguments': '{\"__arg1\": \"create_workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_fjYLfl9tZe6FsIlRwbzdxPPy', 'function': {'arguments': '{\"__arg1\": \"start_workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 245, 'total_tokens': 334, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKPGvgyVlrFJhgYlHlBKUbTdIf4', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--c286e36f-edec-4299-9f09-4740e5a3a6d4-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'main'}, 'id': 'call_Jb9qe6Gltidb8HZUdKrqfdnJ', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'route_feedback'}, 'id': 'call_rsbvLaiXafqidyaN11DDQoGC', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'create_workflow'}, 'id': 'call_9jAlF5eFb05pRIUD5PFK4yXA', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'start_workflow'}, 'id': 'call_fjYLfl9tZe6FsIlRwbzdxPPy', 'type': 'tool_call'}], usage_metadata={'input_tokens': 245, 'output_tokens': 89, 'total_tokens': 334, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Found 3 relevant files:\\n\\nFile: n8n/main (NO img2img).json (Score: 0.72)\\nContent: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}       },       \"type\": \"@n8n/n8n-nodes-langchain.lmCha...\\n\\nFile: main.py (Score: 0.77)\\nContent: Functions: main...\\n\\nFile: requirements.txt (Score: 0.80)\\nContent: chainlit>=1.0.0 langchain>=0.1.0 langchain-community>=0.1.0 langchain-google-genai>=1.0.0 langchain-google-community>=0.1.0 langgraph>=0.0.40 requests...', name='search_codebase', id='36f14f13-7bec-4523-993f-60a6f40c934d', tool_call_id='call_Jb9qe6Gltidb8HZUdKrqfdnJ'), ToolMessage(content='Found 3 relevant files:\\n\\nFile: workflow/graph.py (Score: 0.69)\\nContent: Functions: route_feedback, create_workflow, start_workflow...\\n\\nFile: n8n/main (NO img2img).json (Score: 0.79)\\nContent: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}       },       \"type\": \"@n8n/n8n-nodes-langchain.lmCha...\\n\\nFile: app.py (Score: 0.79)\\nContent: Functions: start, main, handle_initial_request...', name='search_codebase', id='1b702551-d4bf-4811-8005-fa1a53a2a004', tool_call_id='call_rsbvLaiXafqidyaN11DDQoGC'), ToolMessage(content=\"No sufficiently relevant results found for: 'create_workflow'\", name='search_codebase', id='13d76643-1763-46e9-8be2-81eda9584dd4', tool_call_id='call_9jAlF5eFb05pRIUD5PFK4yXA'), ToolMessage(content=\"No sufficiently relevant results found for: 'start_workflow'\", name='search_codebase', id='38f97e5c-4d67-43e7-9907-5798fab90e26', tool_call_id='call_fjYLfl9tZe6FsIlRwbzdxPPy')]}\n",
      "\u001b[1m[updates]\u001b[0m {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rqB9fvXtD7J3TnOOAFAYTxf4', 'function': {'arguments': '{\"__arg1\": \"workflow/graph.py\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_wt4VcgKLQi72WWpkRi1fsF3v', 'function': {'arguments': '{\"__arg1\": \"main.py\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_4OKWDEcgoNJj5PCbzjjkFCdF', 'function': {'arguments': '{\"__arg1\": \"app.py\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 696, 'total_tokens': 768, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKYRuc55EnHQ29fqx5q4Ytz3Fk2', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--b687657f-7487-46e7-9e64-e23df2ae1783-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow/graph.py'}, 'id': 'call_rqB9fvXtD7J3TnOOAFAYTxf4', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'main.py'}, 'id': 'call_wt4VcgKLQi72WWpkRi1fsF3v', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'app.py'}, 'id': 'call_4OKWDEcgoNJj5PCbzjjkFCdF', 'type': 'tool_call'}], usage_metadata={'input_tokens': 696, 'output_tokens': 72, 'total_tokens': 768, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='How does the workflow functions', additional_kwargs={}, response_metadata={}, id='61c3b5d1-a339-41a6-8db6-3b27d500d0e3'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'function': {'arguments': '{\"__arg1\":\"workflow functions\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 147, 'total_tokens': 165, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKIxsCWmCgNIGkXEne1ZpjljplW', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--87467977-2c56-4adf-b52b-996840b20624-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow functions'}, 'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'type': 'tool_call'}], usage_metadata={'input_tokens': 147, 'output_tokens': 18, 'total_tokens': 165, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Found 3 relevant files:\\n\\nFile: main.py (Score: 0.61)\\nContent: Functions: main...\\n\\nFile: workflow/__init__.py (Score: 0.62)\\nContent: ...\\n\\nFile: workflow/graph.py (Score: 0.63)\\nContent: Functions: route_feedback, create_workflow, start_workflow...', name='search_codebase', id='8fb79245-f351-4d70-b1df-54c4a2fbfc36', tool_call_id='call_rMhuVcO5Ra7jnGzzBCMwfqHW'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Jb9qe6Gltidb8HZUdKrqfdnJ', 'function': {'arguments': '{\"__arg1\": \"main\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_rsbvLaiXafqidyaN11DDQoGC', 'function': {'arguments': '{\"__arg1\": \"route_feedback\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_9jAlF5eFb05pRIUD5PFK4yXA', 'function': {'arguments': '{\"__arg1\": \"create_workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_fjYLfl9tZe6FsIlRwbzdxPPy', 'function': {'arguments': '{\"__arg1\": \"start_workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 245, 'total_tokens': 334, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKPGvgyVlrFJhgYlHlBKUbTdIf4', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--c286e36f-edec-4299-9f09-4740e5a3a6d4-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'main'}, 'id': 'call_Jb9qe6Gltidb8HZUdKrqfdnJ', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'route_feedback'}, 'id': 'call_rsbvLaiXafqidyaN11DDQoGC', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'create_workflow'}, 'id': 'call_9jAlF5eFb05pRIUD5PFK4yXA', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'start_workflow'}, 'id': 'call_fjYLfl9tZe6FsIlRwbzdxPPy', 'type': 'tool_call'}], usage_metadata={'input_tokens': 245, 'output_tokens': 89, 'total_tokens': 334, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Found 3 relevant files:\\n\\nFile: n8n/main (NO img2img).json (Score: 0.72)\\nContent: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}       },       \"type\": \"@n8n/n8n-nodes-langchain.lmCha...\\n\\nFile: main.py (Score: 0.77)\\nContent: Functions: main...\\n\\nFile: requirements.txt (Score: 0.80)\\nContent: chainlit>=1.0.0 langchain>=0.1.0 langchain-community>=0.1.0 langchain-google-genai>=1.0.0 langchain-google-community>=0.1.0 langgraph>=0.0.40 requests...', name='search_codebase', id='36f14f13-7bec-4523-993f-60a6f40c934d', tool_call_id='call_Jb9qe6Gltidb8HZUdKrqfdnJ'), ToolMessage(content='Found 3 relevant files:\\n\\nFile: workflow/graph.py (Score: 0.69)\\nContent: Functions: route_feedback, create_workflow, start_workflow...\\n\\nFile: n8n/main (NO img2img).json (Score: 0.79)\\nContent: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}       },       \"type\": \"@n8n/n8n-nodes-langchain.lmCha...\\n\\nFile: app.py (Score: 0.79)\\nContent: Functions: start, main, handle_initial_request...', name='search_codebase', id='1b702551-d4bf-4811-8005-fa1a53a2a004', tool_call_id='call_rsbvLaiXafqidyaN11DDQoGC'), ToolMessage(content=\"No sufficiently relevant results found for: 'create_workflow'\", name='search_codebase', id='13d76643-1763-46e9-8be2-81eda9584dd4', tool_call_id='call_9jAlF5eFb05pRIUD5PFK4yXA'), ToolMessage(content=\"No sufficiently relevant results found for: 'start_workflow'\", name='search_codebase', id='38f97e5c-4d67-43e7-9907-5798fab90e26', tool_call_id='call_fjYLfl9tZe6FsIlRwbzdxPPy'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rqB9fvXtD7J3TnOOAFAYTxf4', 'function': {'arguments': '{\"__arg1\": \"workflow/graph.py\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_wt4VcgKLQi72WWpkRi1fsF3v', 'function': {'arguments': '{\"__arg1\": \"main.py\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_4OKWDEcgoNJj5PCbzjjkFCdF', 'function': {'arguments': '{\"__arg1\": \"app.py\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 696, 'total_tokens': 768, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKYRuc55EnHQ29fqx5q4Ytz3Fk2', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--b687657f-7487-46e7-9e64-e23df2ae1783-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow/graph.py'}, 'id': 'call_rqB9fvXtD7J3TnOOAFAYTxf4', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'main.py'}, 'id': 'call_wt4VcgKLQi72WWpkRi1fsF3v', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'app.py'}, 'id': 'call_4OKWDEcgoNJj5PCbzjjkFCdF', 'type': 'tool_call'}], usage_metadata={'input_tokens': 696, 'output_tokens': 72, 'total_tokens': 768, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohamed mowina\\Desktop\\Areeb\\DevOps-Analysis\\.venv\\lib\\site-packages\\vecs\\collection.py:506: UserWarning: Query does not have a covering index for cosine_distance. See Collection.create_index\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[updates]\u001b[0m {'tools': {'messages': [ToolMessage(content=\"No sufficiently relevant results found for: 'workflow/graph.py'\", name='search_codebase', tool_call_id='call_rqB9fvXtD7J3TnOOAFAYTxf4')]}}\n",
      "\u001b[1m[updates]\u001b[0m {'tools': {'messages': [ToolMessage(content='Found 2 relevant files:\\n\\nFile: studio/__init__.py (Score: 0.61)\\nContent: ...\\n\\nFile: studio/langgraph.json (Score: 0.67)\\nContent: {     \"dockerfile_lines\": [],     \"graphs\": {       \"chatbot\": \"./graph.py:app\"     },     \"env\": \"./.env\",     \"python_version\": \"3.11\",     \"depende...', name='search_codebase', tool_call_id='call_wt4VcgKLQi72WWpkRi1fsF3v')]}}\n",
      "\u001b[1m[updates]\u001b[0m {'tools': {'messages': [ToolMessage(content='Found 3 relevant files:\\n\\nFile: studio/__init__.py (Score: 0.63)\\nContent: ...\\n\\nFile: main.py (Score: 0.67)\\nContent: Functions: main...\\n\\nFile: studio/langgraph.json (Score: 0.69)\\nContent: {     \"dockerfile_lines\": [],     \"graphs\": {       \"chatbot\": \"./graph.py:app\"     },     \"env\": \"./.env\",     \"python_version\": \"3.11\",     \"depende...', name='search_codebase', tool_call_id='call_4OKWDEcgoNJj5PCbzjjkFCdF')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='How does the workflow functions', additional_kwargs={}, response_metadata={}, id='61c3b5d1-a339-41a6-8db6-3b27d500d0e3'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'function': {'arguments': '{\"__arg1\":\"workflow functions\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 147, 'total_tokens': 165, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKIxsCWmCgNIGkXEne1ZpjljplW', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--87467977-2c56-4adf-b52b-996840b20624-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow functions'}, 'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'type': 'tool_call'}], usage_metadata={'input_tokens': 147, 'output_tokens': 18, 'total_tokens': 165, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Found 3 relevant files:\\n\\nFile: main.py (Score: 0.61)\\nContent: Functions: main...\\n\\nFile: workflow/__init__.py (Score: 0.62)\\nContent: ...\\n\\nFile: workflow/graph.py (Score: 0.63)\\nContent: Functions: route_feedback, create_workflow, start_workflow...', name='search_codebase', id='8fb79245-f351-4d70-b1df-54c4a2fbfc36', tool_call_id='call_rMhuVcO5Ra7jnGzzBCMwfqHW'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Jb9qe6Gltidb8HZUdKrqfdnJ', 'function': {'arguments': '{\"__arg1\": \"main\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_rsbvLaiXafqidyaN11DDQoGC', 'function': {'arguments': '{\"__arg1\": \"route_feedback\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_9jAlF5eFb05pRIUD5PFK4yXA', 'function': {'arguments': '{\"__arg1\": \"create_workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_fjYLfl9tZe6FsIlRwbzdxPPy', 'function': {'arguments': '{\"__arg1\": \"start_workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 245, 'total_tokens': 334, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKPGvgyVlrFJhgYlHlBKUbTdIf4', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--c286e36f-edec-4299-9f09-4740e5a3a6d4-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'main'}, 'id': 'call_Jb9qe6Gltidb8HZUdKrqfdnJ', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'route_feedback'}, 'id': 'call_rsbvLaiXafqidyaN11DDQoGC', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'create_workflow'}, 'id': 'call_9jAlF5eFb05pRIUD5PFK4yXA', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'start_workflow'}, 'id': 'call_fjYLfl9tZe6FsIlRwbzdxPPy', 'type': 'tool_call'}], usage_metadata={'input_tokens': 245, 'output_tokens': 89, 'total_tokens': 334, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Found 3 relevant files:\\n\\nFile: n8n/main (NO img2img).json (Score: 0.72)\\nContent: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}       },       \"type\": \"@n8n/n8n-nodes-langchain.lmCha...\\n\\nFile: main.py (Score: 0.77)\\nContent: Functions: main...\\n\\nFile: requirements.txt (Score: 0.80)\\nContent: chainlit>=1.0.0 langchain>=0.1.0 langchain-community>=0.1.0 langchain-google-genai>=1.0.0 langchain-google-community>=0.1.0 langgraph>=0.0.40 requests...', name='search_codebase', id='36f14f13-7bec-4523-993f-60a6f40c934d', tool_call_id='call_Jb9qe6Gltidb8HZUdKrqfdnJ'), ToolMessage(content='Found 3 relevant files:\\n\\nFile: workflow/graph.py (Score: 0.69)\\nContent: Functions: route_feedback, create_workflow, start_workflow...\\n\\nFile: n8n/main (NO img2img).json (Score: 0.79)\\nContent: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}       },       \"type\": \"@n8n/n8n-nodes-langchain.lmCha...\\n\\nFile: app.py (Score: 0.79)\\nContent: Functions: start, main, handle_initial_request...', name='search_codebase', id='1b702551-d4bf-4811-8005-fa1a53a2a004', tool_call_id='call_rsbvLaiXafqidyaN11DDQoGC'), ToolMessage(content=\"No sufficiently relevant results found for: 'create_workflow'\", name='search_codebase', id='13d76643-1763-46e9-8be2-81eda9584dd4', tool_call_id='call_9jAlF5eFb05pRIUD5PFK4yXA'), ToolMessage(content=\"No sufficiently relevant results found for: 'start_workflow'\", name='search_codebase', id='38f97e5c-4d67-43e7-9907-5798fab90e26', tool_call_id='call_fjYLfl9tZe6FsIlRwbzdxPPy'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rqB9fvXtD7J3TnOOAFAYTxf4', 'function': {'arguments': '{\"__arg1\": \"workflow/graph.py\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_wt4VcgKLQi72WWpkRi1fsF3v', 'function': {'arguments': '{\"__arg1\": \"main.py\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_4OKWDEcgoNJj5PCbzjjkFCdF', 'function': {'arguments': '{\"__arg1\": \"app.py\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 696, 'total_tokens': 768, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKYRuc55EnHQ29fqx5q4Ytz3Fk2', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--b687657f-7487-46e7-9e64-e23df2ae1783-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow/graph.py'}, 'id': 'call_rqB9fvXtD7J3TnOOAFAYTxf4', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'main.py'}, 'id': 'call_wt4VcgKLQi72WWpkRi1fsF3v', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'app.py'}, 'id': 'call_4OKWDEcgoNJj5PCbzjjkFCdF', 'type': 'tool_call'}], usage_metadata={'input_tokens': 696, 'output_tokens': 72, 'total_tokens': 768, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content=\"No sufficiently relevant results found for: 'workflow/graph.py'\", name='search_codebase', id='467c6af9-0c2b-49e0-b318-3a2709497c0a', tool_call_id='call_rqB9fvXtD7J3TnOOAFAYTxf4'), ToolMessage(content='Found 2 relevant files:\\n\\nFile: studio/__init__.py (Score: 0.61)\\nContent: ...\\n\\nFile: studio/langgraph.json (Score: 0.67)\\nContent: {     \"dockerfile_lines\": [],     \"graphs\": {       \"chatbot\": \"./graph.py:app\"     },     \"env\": \"./.env\",     \"python_version\": \"3.11\",     \"depende...', name='search_codebase', id='6d8a0590-9e06-4cbd-bc7f-d9dbc8d5dac7', tool_call_id='call_wt4VcgKLQi72WWpkRi1fsF3v'), ToolMessage(content='Found 3 relevant files:\\n\\nFile: studio/__init__.py (Score: 0.63)\\nContent: ...\\n\\nFile: main.py (Score: 0.67)\\nContent: Functions: main...\\n\\nFile: studio/langgraph.json (Score: 0.69)\\nContent: {     \"dockerfile_lines\": [],     \"graphs\": {       \"chatbot\": \"./graph.py:app\"     },     \"env\": \"./.env\",     \"python_version\": \"3.11\",     \"depende...', name='search_codebase', id='a0719b06-fe2e-4d9b-a427-afe1fb683660', tool_call_id='call_4OKWDEcgoNJj5PCbzjjkFCdF')]}\n",
      "\u001b[1m[updates]\u001b[0m {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_bWVQ5uOn6JJq3e4aTR5QMijo', 'function': {'arguments': '{\"__arg1\":\"workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 1012, 'total_tokens': 1029, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKhUQz23iEROlebqfjqwkiKpgTZ', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--3470795e-0a06-4e7e-99a5-2c2298d7aa7f-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow'}, 'id': 'call_bWVQ5uOn6JJq3e4aTR5QMijo', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1012, 'output_tokens': 17, 'total_tokens': 1029, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='How does the workflow functions', additional_kwargs={}, response_metadata={}, id='61c3b5d1-a339-41a6-8db6-3b27d500d0e3'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'function': {'arguments': '{\"__arg1\":\"workflow functions\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 147, 'total_tokens': 165, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKIxsCWmCgNIGkXEne1ZpjljplW', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--87467977-2c56-4adf-b52b-996840b20624-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow functions'}, 'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'type': 'tool_call'}], usage_metadata={'input_tokens': 147, 'output_tokens': 18, 'total_tokens': 165, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Found 3 relevant files:\\n\\nFile: main.py (Score: 0.61)\\nContent: Functions: main...\\n\\nFile: workflow/__init__.py (Score: 0.62)\\nContent: ...\\n\\nFile: workflow/graph.py (Score: 0.63)\\nContent: Functions: route_feedback, create_workflow, start_workflow...', name='search_codebase', id='8fb79245-f351-4d70-b1df-54c4a2fbfc36', tool_call_id='call_rMhuVcO5Ra7jnGzzBCMwfqHW'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Jb9qe6Gltidb8HZUdKrqfdnJ', 'function': {'arguments': '{\"__arg1\": \"main\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_rsbvLaiXafqidyaN11DDQoGC', 'function': {'arguments': '{\"__arg1\": \"route_feedback\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_9jAlF5eFb05pRIUD5PFK4yXA', 'function': {'arguments': '{\"__arg1\": \"create_workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_fjYLfl9tZe6FsIlRwbzdxPPy', 'function': {'arguments': '{\"__arg1\": \"start_workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 245, 'total_tokens': 334, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKPGvgyVlrFJhgYlHlBKUbTdIf4', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--c286e36f-edec-4299-9f09-4740e5a3a6d4-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'main'}, 'id': 'call_Jb9qe6Gltidb8HZUdKrqfdnJ', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'route_feedback'}, 'id': 'call_rsbvLaiXafqidyaN11DDQoGC', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'create_workflow'}, 'id': 'call_9jAlF5eFb05pRIUD5PFK4yXA', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'start_workflow'}, 'id': 'call_fjYLfl9tZe6FsIlRwbzdxPPy', 'type': 'tool_call'}], usage_metadata={'input_tokens': 245, 'output_tokens': 89, 'total_tokens': 334, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Found 3 relevant files:\\n\\nFile: n8n/main (NO img2img).json (Score: 0.72)\\nContent: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}       },       \"type\": \"@n8n/n8n-nodes-langchain.lmCha...\\n\\nFile: main.py (Score: 0.77)\\nContent: Functions: main...\\n\\nFile: requirements.txt (Score: 0.80)\\nContent: chainlit>=1.0.0 langchain>=0.1.0 langchain-community>=0.1.0 langchain-google-genai>=1.0.0 langchain-google-community>=0.1.0 langgraph>=0.0.40 requests...', name='search_codebase', id='36f14f13-7bec-4523-993f-60a6f40c934d', tool_call_id='call_Jb9qe6Gltidb8HZUdKrqfdnJ'), ToolMessage(content='Found 3 relevant files:\\n\\nFile: workflow/graph.py (Score: 0.69)\\nContent: Functions: route_feedback, create_workflow, start_workflow...\\n\\nFile: n8n/main (NO img2img).json (Score: 0.79)\\nContent: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}       },       \"type\": \"@n8n/n8n-nodes-langchain.lmCha...\\n\\nFile: app.py (Score: 0.79)\\nContent: Functions: start, main, handle_initial_request...', name='search_codebase', id='1b702551-d4bf-4811-8005-fa1a53a2a004', tool_call_id='call_rsbvLaiXafqidyaN11DDQoGC'), ToolMessage(content=\"No sufficiently relevant results found for: 'create_workflow'\", name='search_codebase', id='13d76643-1763-46e9-8be2-81eda9584dd4', tool_call_id='call_9jAlF5eFb05pRIUD5PFK4yXA'), ToolMessage(content=\"No sufficiently relevant results found for: 'start_workflow'\", name='search_codebase', id='38f97e5c-4d67-43e7-9907-5798fab90e26', tool_call_id='call_fjYLfl9tZe6FsIlRwbzdxPPy'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rqB9fvXtD7J3TnOOAFAYTxf4', 'function': {'arguments': '{\"__arg1\": \"workflow/graph.py\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_wt4VcgKLQi72WWpkRi1fsF3v', 'function': {'arguments': '{\"__arg1\": \"main.py\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_4OKWDEcgoNJj5PCbzjjkFCdF', 'function': {'arguments': '{\"__arg1\": \"app.py\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 696, 'total_tokens': 768, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKYRuc55EnHQ29fqx5q4Ytz3Fk2', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--b687657f-7487-46e7-9e64-e23df2ae1783-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow/graph.py'}, 'id': 'call_rqB9fvXtD7J3TnOOAFAYTxf4', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'main.py'}, 'id': 'call_wt4VcgKLQi72WWpkRi1fsF3v', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'app.py'}, 'id': 'call_4OKWDEcgoNJj5PCbzjjkFCdF', 'type': 'tool_call'}], usage_metadata={'input_tokens': 696, 'output_tokens': 72, 'total_tokens': 768, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content=\"No sufficiently relevant results found for: 'workflow/graph.py'\", name='search_codebase', id='467c6af9-0c2b-49e0-b318-3a2709497c0a', tool_call_id='call_rqB9fvXtD7J3TnOOAFAYTxf4'), ToolMessage(content='Found 2 relevant files:\\n\\nFile: studio/__init__.py (Score: 0.61)\\nContent: ...\\n\\nFile: studio/langgraph.json (Score: 0.67)\\nContent: {     \"dockerfile_lines\": [],     \"graphs\": {       \"chatbot\": \"./graph.py:app\"     },     \"env\": \"./.env\",     \"python_version\": \"3.11\",     \"depende...', name='search_codebase', id='6d8a0590-9e06-4cbd-bc7f-d9dbc8d5dac7', tool_call_id='call_wt4VcgKLQi72WWpkRi1fsF3v'), ToolMessage(content='Found 3 relevant files:\\n\\nFile: studio/__init__.py (Score: 0.63)\\nContent: ...\\n\\nFile: main.py (Score: 0.67)\\nContent: Functions: main...\\n\\nFile: studio/langgraph.json (Score: 0.69)\\nContent: {     \"dockerfile_lines\": [],     \"graphs\": {       \"chatbot\": \"./graph.py:app\"     },     \"env\": \"./.env\",     \"python_version\": \"3.11\",     \"depende...', name='search_codebase', id='a0719b06-fe2e-4d9b-a427-afe1fb683660', tool_call_id='call_4OKWDEcgoNJj5PCbzjjkFCdF'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_bWVQ5uOn6JJq3e4aTR5QMijo', 'function': {'arguments': '{\"__arg1\":\"workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 1012, 'total_tokens': 1029, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKhUQz23iEROlebqfjqwkiKpgTZ', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--3470795e-0a06-4e7e-99a5-2c2298d7aa7f-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow'}, 'id': 'call_bWVQ5uOn6JJq3e4aTR5QMijo', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1012, 'output_tokens': 17, 'total_tokens': 1029, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohamed mowina\\Desktop\\Areeb\\DevOps-Analysis\\.venv\\lib\\site-packages\\vecs\\collection.py:506: UserWarning: Query does not have a covering index for cosine_distance. See Collection.create_index\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[updates]\u001b[0m {'tools': {'messages': [ToolMessage(content='Found 3 relevant files:\\n\\nFile: workflow/graph.py (Score: 0.62)\\nContent: Functions: route_feedback, create_workflow, start_workflow...\\n\\nFile: workflow/__init__.py (Score: 0.63)\\nContent: ...\\n\\nFile: main.py (Score: 0.64)\\nContent: Functions: main...', name='search_codebase', id='f70807af-6efb-4c46-aaff-9820479ba4a7', tool_call_id='call_bWVQ5uOn6JJq3e4aTR5QMijo')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='How does the workflow functions', additional_kwargs={}, response_metadata={}, id='61c3b5d1-a339-41a6-8db6-3b27d500d0e3'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'function': {'arguments': '{\"__arg1\":\"workflow functions\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 147, 'total_tokens': 165, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKIxsCWmCgNIGkXEne1ZpjljplW', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--87467977-2c56-4adf-b52b-996840b20624-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow functions'}, 'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'type': 'tool_call'}], usage_metadata={'input_tokens': 147, 'output_tokens': 18, 'total_tokens': 165, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Found 3 relevant files:\\n\\nFile: main.py (Score: 0.61)\\nContent: Functions: main...\\n\\nFile: workflow/__init__.py (Score: 0.62)\\nContent: ...\\n\\nFile: workflow/graph.py (Score: 0.63)\\nContent: Functions: route_feedback, create_workflow, start_workflow...', name='search_codebase', id='8fb79245-f351-4d70-b1df-54c4a2fbfc36', tool_call_id='call_rMhuVcO5Ra7jnGzzBCMwfqHW'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Jb9qe6Gltidb8HZUdKrqfdnJ', 'function': {'arguments': '{\"__arg1\": \"main\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_rsbvLaiXafqidyaN11DDQoGC', 'function': {'arguments': '{\"__arg1\": \"route_feedback\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_9jAlF5eFb05pRIUD5PFK4yXA', 'function': {'arguments': '{\"__arg1\": \"create_workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_fjYLfl9tZe6FsIlRwbzdxPPy', 'function': {'arguments': '{\"__arg1\": \"start_workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 245, 'total_tokens': 334, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKPGvgyVlrFJhgYlHlBKUbTdIf4', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--c286e36f-edec-4299-9f09-4740e5a3a6d4-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'main'}, 'id': 'call_Jb9qe6Gltidb8HZUdKrqfdnJ', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'route_feedback'}, 'id': 'call_rsbvLaiXafqidyaN11DDQoGC', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'create_workflow'}, 'id': 'call_9jAlF5eFb05pRIUD5PFK4yXA', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'start_workflow'}, 'id': 'call_fjYLfl9tZe6FsIlRwbzdxPPy', 'type': 'tool_call'}], usage_metadata={'input_tokens': 245, 'output_tokens': 89, 'total_tokens': 334, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Found 3 relevant files:\\n\\nFile: n8n/main (NO img2img).json (Score: 0.72)\\nContent: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}       },       \"type\": \"@n8n/n8n-nodes-langchain.lmCha...\\n\\nFile: main.py (Score: 0.77)\\nContent: Functions: main...\\n\\nFile: requirements.txt (Score: 0.80)\\nContent: chainlit>=1.0.0 langchain>=0.1.0 langchain-community>=0.1.0 langchain-google-genai>=1.0.0 langchain-google-community>=0.1.0 langgraph>=0.0.40 requests...', name='search_codebase', id='36f14f13-7bec-4523-993f-60a6f40c934d', tool_call_id='call_Jb9qe6Gltidb8HZUdKrqfdnJ'), ToolMessage(content='Found 3 relevant files:\\n\\nFile: workflow/graph.py (Score: 0.69)\\nContent: Functions: route_feedback, create_workflow, start_workflow...\\n\\nFile: n8n/main (NO img2img).json (Score: 0.79)\\nContent: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}       },       \"type\": \"@n8n/n8n-nodes-langchain.lmCha...\\n\\nFile: app.py (Score: 0.79)\\nContent: Functions: start, main, handle_initial_request...', name='search_codebase', id='1b702551-d4bf-4811-8005-fa1a53a2a004', tool_call_id='call_rsbvLaiXafqidyaN11DDQoGC'), ToolMessage(content=\"No sufficiently relevant results found for: 'create_workflow'\", name='search_codebase', id='13d76643-1763-46e9-8be2-81eda9584dd4', tool_call_id='call_9jAlF5eFb05pRIUD5PFK4yXA'), ToolMessage(content=\"No sufficiently relevant results found for: 'start_workflow'\", name='search_codebase', id='38f97e5c-4d67-43e7-9907-5798fab90e26', tool_call_id='call_fjYLfl9tZe6FsIlRwbzdxPPy'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rqB9fvXtD7J3TnOOAFAYTxf4', 'function': {'arguments': '{\"__arg1\": \"workflow/graph.py\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_wt4VcgKLQi72WWpkRi1fsF3v', 'function': {'arguments': '{\"__arg1\": \"main.py\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_4OKWDEcgoNJj5PCbzjjkFCdF', 'function': {'arguments': '{\"__arg1\": \"app.py\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 696, 'total_tokens': 768, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKYRuc55EnHQ29fqx5q4Ytz3Fk2', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--b687657f-7487-46e7-9e64-e23df2ae1783-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow/graph.py'}, 'id': 'call_rqB9fvXtD7J3TnOOAFAYTxf4', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'main.py'}, 'id': 'call_wt4VcgKLQi72WWpkRi1fsF3v', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'app.py'}, 'id': 'call_4OKWDEcgoNJj5PCbzjjkFCdF', 'type': 'tool_call'}], usage_metadata={'input_tokens': 696, 'output_tokens': 72, 'total_tokens': 768, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content=\"No sufficiently relevant results found for: 'workflow/graph.py'\", name='search_codebase', id='467c6af9-0c2b-49e0-b318-3a2709497c0a', tool_call_id='call_rqB9fvXtD7J3TnOOAFAYTxf4'), ToolMessage(content='Found 2 relevant files:\\n\\nFile: studio/__init__.py (Score: 0.61)\\nContent: ...\\n\\nFile: studio/langgraph.json (Score: 0.67)\\nContent: {     \"dockerfile_lines\": [],     \"graphs\": {       \"chatbot\": \"./graph.py:app\"     },     \"env\": \"./.env\",     \"python_version\": \"3.11\",     \"depende...', name='search_codebase', id='6d8a0590-9e06-4cbd-bc7f-d9dbc8d5dac7', tool_call_id='call_wt4VcgKLQi72WWpkRi1fsF3v'), ToolMessage(content='Found 3 relevant files:\\n\\nFile: studio/__init__.py (Score: 0.63)\\nContent: ...\\n\\nFile: main.py (Score: 0.67)\\nContent: Functions: main...\\n\\nFile: studio/langgraph.json (Score: 0.69)\\nContent: {     \"dockerfile_lines\": [],     \"graphs\": {       \"chatbot\": \"./graph.py:app\"     },     \"env\": \"./.env\",     \"python_version\": \"3.11\",     \"depende...', name='search_codebase', id='a0719b06-fe2e-4d9b-a427-afe1fb683660', tool_call_id='call_4OKWDEcgoNJj5PCbzjjkFCdF'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_bWVQ5uOn6JJq3e4aTR5QMijo', 'function': {'arguments': '{\"__arg1\":\"workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 1012, 'total_tokens': 1029, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKhUQz23iEROlebqfjqwkiKpgTZ', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--3470795e-0a06-4e7e-99a5-2c2298d7aa7f-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow'}, 'id': 'call_bWVQ5uOn6JJq3e4aTR5QMijo', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1012, 'output_tokens': 17, 'total_tokens': 1029, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Found 3 relevant files:\\n\\nFile: workflow/graph.py (Score: 0.62)\\nContent: Functions: route_feedback, create_workflow, start_workflow...\\n\\nFile: workflow/__init__.py (Score: 0.63)\\nContent: ...\\n\\nFile: main.py (Score: 0.64)\\nContent: Functions: main...', name='search_codebase', id='f70807af-6efb-4c46-aaff-9820479ba4a7', tool_call_id='call_bWVQ5uOn6JJq3e4aTR5QMijo')]}\n",
      "\u001b[1m[updates]\u001b[0m {'agent': {'messages': [AIMessage(content=\"To understand how the workflow functions are implemented, let's focus on the relevant code snippets and files found during the searches:\\n\\n### Key Files and Functions\\n\\n1. **`workflow/graph.py`**\\n   - This file contains several important functions related to workflow management:\\n     - **`route_feedback`**: Likely handles feedback routing within the workflow process.\\n     - **`create_workflow`**: Responsible for initializing or setting up new workflows.\\n     - **`start_workflow`**: Begins the execution of a defined workflow.\\n\\n2. **`main.py`**\\n   - Contains the main execution logic of the application, including initializing the application and possibly invoking the workflow functions.\\n\\n3. **`workflow/__init__.py`**\\n   - This file typically initializes the workflow module and may import key functions from `graph.py` for use elsewhere in the application.\\n\\n### Overview of Functions\\n\\n- **`create_workflow`**: \\n   - This function is essential for defining a new workflow, where it might accept parameters that dictate its structure (such as nodes, edges, and conditions for execution). The function likely sets up the workflow data structure and prepares it for execution.\\n  \\n- **`start_workflow`**: \\n   - Invoked to kick off the execution of a previously defined workflow. This function probably handles necessary checks (like validation) and executes the tasks or nodes defined in the workflow sequence.\\n\\n- **`route_feedback`**: \\n   - Designed to manage the feedback process, which can include sending information back from completed tasks in the workflow for logging, updates, or decision-making.\\n\\n### Code Structure Recommendations\\n\\nIf you want to explore the actual code implementation, look into `workflow/graph.py` for the definitions of `create_workflow`, `start_workflow`, and `route_feedback`. You will likely find detailed comments and logic that will clarify how each function interacts within the workflow system.\\n\\n### Additional Considerations\\n\\nFor a more integrative view of how these functions operate within the entire application:\\n- Check how `main.py` orchestrates the workflow initiation.\\n- Review any dependencies in `requirements.txt`, as these can often provide insight into what libraries or frameworks are being used that might affect workflow behavior.\\n\\nYou may also want to run the application to observe how these functions are invoked in real time during execution. This can be especially valuable to visualize their impact in a live scenario.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 486, 'prompt_tokens': 1109, 'total_tokens': 1595, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKnJL34z4N6HRiMgl8vXQwNvuro', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--6082f8ba-9064-40bc-9ef5-6caa6f325683-0', usage_metadata={'input_tokens': 1109, 'output_tokens': 486, 'total_tokens': 1595, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content='How does the workflow functions', additional_kwargs={}, response_metadata={}, id='61c3b5d1-a339-41a6-8db6-3b27d500d0e3'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'function': {'arguments': '{\"__arg1\":\"workflow functions\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 147, 'total_tokens': 165, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKIxsCWmCgNIGkXEne1ZpjljplW', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--87467977-2c56-4adf-b52b-996840b20624-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow functions'}, 'id': 'call_rMhuVcO5Ra7jnGzzBCMwfqHW', 'type': 'tool_call'}], usage_metadata={'input_tokens': 147, 'output_tokens': 18, 'total_tokens': 165, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Found 3 relevant files:\\n\\nFile: main.py (Score: 0.61)\\nContent: Functions: main...\\n\\nFile: workflow/__init__.py (Score: 0.62)\\nContent: ...\\n\\nFile: workflow/graph.py (Score: 0.63)\\nContent: Functions: route_feedback, create_workflow, start_workflow...', name='search_codebase', id='8fb79245-f351-4d70-b1df-54c4a2fbfc36', tool_call_id='call_rMhuVcO5Ra7jnGzzBCMwfqHW'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Jb9qe6Gltidb8HZUdKrqfdnJ', 'function': {'arguments': '{\"__arg1\": \"main\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_rsbvLaiXafqidyaN11DDQoGC', 'function': {'arguments': '{\"__arg1\": \"route_feedback\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_9jAlF5eFb05pRIUD5PFK4yXA', 'function': {'arguments': '{\"__arg1\": \"create_workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_fjYLfl9tZe6FsIlRwbzdxPPy', 'function': {'arguments': '{\"__arg1\": \"start_workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 245, 'total_tokens': 334, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKPGvgyVlrFJhgYlHlBKUbTdIf4', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--c286e36f-edec-4299-9f09-4740e5a3a6d4-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'main'}, 'id': 'call_Jb9qe6Gltidb8HZUdKrqfdnJ', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'route_feedback'}, 'id': 'call_rsbvLaiXafqidyaN11DDQoGC', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'create_workflow'}, 'id': 'call_9jAlF5eFb05pRIUD5PFK4yXA', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'start_workflow'}, 'id': 'call_fjYLfl9tZe6FsIlRwbzdxPPy', 'type': 'tool_call'}], usage_metadata={'input_tokens': 245, 'output_tokens': 89, 'total_tokens': 334, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Found 3 relevant files:\\n\\nFile: n8n/main (NO img2img).json (Score: 0.72)\\nContent: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}       },       \"type\": \"@n8n/n8n-nodes-langchain.lmCha...\\n\\nFile: main.py (Score: 0.77)\\nContent: Functions: main...\\n\\nFile: requirements.txt (Score: 0.80)\\nContent: chainlit>=1.0.0 langchain>=0.1.0 langchain-community>=0.1.0 langchain-google-genai>=1.0.0 langchain-google-community>=0.1.0 langgraph>=0.0.40 requests...', name='search_codebase', id='36f14f13-7bec-4523-993f-60a6f40c934d', tool_call_id='call_Jb9qe6Gltidb8HZUdKrqfdnJ'), ToolMessage(content='Found 3 relevant files:\\n\\nFile: workflow/graph.py (Score: 0.69)\\nContent: Functions: route_feedback, create_workflow, start_workflow...\\n\\nFile: n8n/main (NO img2img).json (Score: 0.79)\\nContent: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}       },       \"type\": \"@n8n/n8n-nodes-langchain.lmCha...\\n\\nFile: app.py (Score: 0.79)\\nContent: Functions: start, main, handle_initial_request...', name='search_codebase', id='1b702551-d4bf-4811-8005-fa1a53a2a004', tool_call_id='call_rsbvLaiXafqidyaN11DDQoGC'), ToolMessage(content=\"No sufficiently relevant results found for: 'create_workflow'\", name='search_codebase', id='13d76643-1763-46e9-8be2-81eda9584dd4', tool_call_id='call_9jAlF5eFb05pRIUD5PFK4yXA'), ToolMessage(content=\"No sufficiently relevant results found for: 'start_workflow'\", name='search_codebase', id='38f97e5c-4d67-43e7-9907-5798fab90e26', tool_call_id='call_fjYLfl9tZe6FsIlRwbzdxPPy'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rqB9fvXtD7J3TnOOAFAYTxf4', 'function': {'arguments': '{\"__arg1\": \"workflow/graph.py\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_wt4VcgKLQi72WWpkRi1fsF3v', 'function': {'arguments': '{\"__arg1\": \"main.py\"}', 'name': 'search_codebase'}, 'type': 'function'}, {'id': 'call_4OKWDEcgoNJj5PCbzjjkFCdF', 'function': {'arguments': '{\"__arg1\": \"app.py\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 696, 'total_tokens': 768, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKYRuc55EnHQ29fqx5q4Ytz3Fk2', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--b687657f-7487-46e7-9e64-e23df2ae1783-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow/graph.py'}, 'id': 'call_rqB9fvXtD7J3TnOOAFAYTxf4', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'main.py'}, 'id': 'call_wt4VcgKLQi72WWpkRi1fsF3v', 'type': 'tool_call'}, {'name': 'search_codebase', 'args': {'__arg1': 'app.py'}, 'id': 'call_4OKWDEcgoNJj5PCbzjjkFCdF', 'type': 'tool_call'}], usage_metadata={'input_tokens': 696, 'output_tokens': 72, 'total_tokens': 768, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content=\"No sufficiently relevant results found for: 'workflow/graph.py'\", name='search_codebase', id='467c6af9-0c2b-49e0-b318-3a2709497c0a', tool_call_id='call_rqB9fvXtD7J3TnOOAFAYTxf4'), ToolMessage(content='Found 2 relevant files:\\n\\nFile: studio/__init__.py (Score: 0.61)\\nContent: ...\\n\\nFile: studio/langgraph.json (Score: 0.67)\\nContent: {     \"dockerfile_lines\": [],     \"graphs\": {       \"chatbot\": \"./graph.py:app\"     },     \"env\": \"./.env\",     \"python_version\": \"3.11\",     \"depende...', name='search_codebase', id='6d8a0590-9e06-4cbd-bc7f-d9dbc8d5dac7', tool_call_id='call_wt4VcgKLQi72WWpkRi1fsF3v'), ToolMessage(content='Found 3 relevant files:\\n\\nFile: studio/__init__.py (Score: 0.63)\\nContent: ...\\n\\nFile: main.py (Score: 0.67)\\nContent: Functions: main...\\n\\nFile: studio/langgraph.json (Score: 0.69)\\nContent: {     \"dockerfile_lines\": [],     \"graphs\": {       \"chatbot\": \"./graph.py:app\"     },     \"env\": \"./.env\",     \"python_version\": \"3.11\",     \"depende...', name='search_codebase', id='a0719b06-fe2e-4d9b-a427-afe1fb683660', tool_call_id='call_4OKWDEcgoNJj5PCbzjjkFCdF'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_bWVQ5uOn6JJq3e4aTR5QMijo', 'function': {'arguments': '{\"__arg1\":\"workflow\"}', 'name': 'search_codebase'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 1012, 'total_tokens': 1029, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKhUQz23iEROlebqfjqwkiKpgTZ', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--3470795e-0a06-4e7e-99a5-2c2298d7aa7f-0', tool_calls=[{'name': 'search_codebase', 'args': {'__arg1': 'workflow'}, 'id': 'call_bWVQ5uOn6JJq3e4aTR5QMijo', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1012, 'output_tokens': 17, 'total_tokens': 1029, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Found 3 relevant files:\\n\\nFile: workflow/graph.py (Score: 0.62)\\nContent: Functions: route_feedback, create_workflow, start_workflow...\\n\\nFile: workflow/__init__.py (Score: 0.63)\\nContent: ...\\n\\nFile: main.py (Score: 0.64)\\nContent: Functions: main...', name='search_codebase', id='f70807af-6efb-4c46-aaff-9820479ba4a7', tool_call_id='call_bWVQ5uOn6JJq3e4aTR5QMijo'), AIMessage(content=\"To understand how the workflow functions are implemented, let's focus on the relevant code snippets and files found during the searches:\\n\\n### Key Files and Functions\\n\\n1. **`workflow/graph.py`**\\n   - This file contains several important functions related to workflow management:\\n     - **`route_feedback`**: Likely handles feedback routing within the workflow process.\\n     - **`create_workflow`**: Responsible for initializing or setting up new workflows.\\n     - **`start_workflow`**: Begins the execution of a defined workflow.\\n\\n2. **`main.py`**\\n   - Contains the main execution logic of the application, including initializing the application and possibly invoking the workflow functions.\\n\\n3. **`workflow/__init__.py`**\\n   - This file typically initializes the workflow module and may import key functions from `graph.py` for use elsewhere in the application.\\n\\n### Overview of Functions\\n\\n- **`create_workflow`**: \\n   - This function is essential for defining a new workflow, where it might accept parameters that dictate its structure (such as nodes, edges, and conditions for execution). The function likely sets up the workflow data structure and prepares it for execution.\\n  \\n- **`start_workflow`**: \\n   - Invoked to kick off the execution of a previously defined workflow. This function probably handles necessary checks (like validation) and executes the tasks or nodes defined in the workflow sequence.\\n\\n- **`route_feedback`**: \\n   - Designed to manage the feedback process, which can include sending information back from completed tasks in the workflow for logging, updates, or decision-making.\\n\\n### Code Structure Recommendations\\n\\nIf you want to explore the actual code implementation, look into `workflow/graph.py` for the definitions of `create_workflow`, `start_workflow`, and `route_feedback`. You will likely find detailed comments and logic that will clarify how each function interacts within the workflow system.\\n\\n### Additional Considerations\\n\\nFor a more integrative view of how these functions operate within the entire application:\\n- Check how `main.py` orchestrates the workflow initiation.\\n- Review any dependencies in `requirements.txt`, as these can often provide insight into what libraries or frameworks are being used that might affect workflow behavior.\\n\\nYou may also want to run the application to observe how these functions are invoked in real time during execution. This can be especially valuable to visualize their impact in a live scenario.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 486, 'prompt_tokens': 1109, 'total_tokens': 1595, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFcKnJL34z4N6HRiMgl8vXQwNvuro', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--6082f8ba-9064-40bc-9ef5-6caa6f325683-0', usage_metadata={'input_tokens': 1109, 'output_tokens': 486, 'total_tokens': 1595, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How does the workflow functions\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  search_codebase (call_rMhuVcO5Ra7jnGzzBCMwfqHW)\n",
      " Call ID: call_rMhuVcO5Ra7jnGzzBCMwfqHW\n",
      "  Args:\n",
      "    __arg1: workflow functions\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search_codebase\n",
      "\n",
      "Found 3 relevant files:\n",
      "\n",
      "File: main.py (Score: 0.61)\n",
      "Content: Functions: main...\n",
      "\n",
      "File: workflow/__init__.py (Score: 0.62)\n",
      "Content: ...\n",
      "\n",
      "File: workflow/graph.py (Score: 0.63)\n",
      "Content: Functions: route_feedback, create_workflow, start_workflow...\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  search_codebase (call_Jb9qe6Gltidb8HZUdKrqfdnJ)\n",
      " Call ID: call_Jb9qe6Gltidb8HZUdKrqfdnJ\n",
      "  Args:\n",
      "    __arg1: main\n",
      "  search_codebase (call_rsbvLaiXafqidyaN11DDQoGC)\n",
      " Call ID: call_rsbvLaiXafqidyaN11DDQoGC\n",
      "  Args:\n",
      "    __arg1: route_feedback\n",
      "  search_codebase (call_9jAlF5eFb05pRIUD5PFK4yXA)\n",
      " Call ID: call_9jAlF5eFb05pRIUD5PFK4yXA\n",
      "  Args:\n",
      "    __arg1: create_workflow\n",
      "  search_codebase (call_fjYLfl9tZe6FsIlRwbzdxPPy)\n",
      " Call ID: call_fjYLfl9tZe6FsIlRwbzdxPPy\n",
      "  Args:\n",
      "    __arg1: start_workflow\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search_codebase\n",
      "\n",
      "Found 3 relevant files:\n",
      "\n",
      "File: n8n/main (NO img2img).json (Score: 0.72)\n",
      "Content: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}       },       \"type\": \"@n8n/n8n-nodes-langchain.lmCha...\n",
      "\n",
      "File: main.py (Score: 0.77)\n",
      "Content: Functions: main...\n",
      "\n",
      "File: requirements.txt (Score: 0.80)\n",
      "Content: chainlit>=1.0.0 langchain>=0.1.0 langchain-community>=0.1.0 langchain-google-genai>=1.0.0 langchain-google-community>=0.1.0 langgraph>=0.0.40 requests...\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search_codebase\n",
      "\n",
      "Found 3 relevant files:\n",
      "\n",
      "File: workflow/graph.py (Score: 0.69)\n",
      "Content: Functions: route_feedback, create_workflow, start_workflow...\n",
      "\n",
      "File: n8n/main (NO img2img).json (Score: 0.79)\n",
      "Content: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}       },       \"type\": \"@n8n/n8n-nodes-langchain.lmCha...\n",
      "\n",
      "File: app.py (Score: 0.79)\n",
      "Content: Functions: start, main, handle_initial_request...\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search_codebase\n",
      "\n",
      "No sufficiently relevant results found for: 'create_workflow'\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search_codebase\n",
      "\n",
      "No sufficiently relevant results found for: 'start_workflow'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  search_codebase (call_rqB9fvXtD7J3TnOOAFAYTxf4)\n",
      " Call ID: call_rqB9fvXtD7J3TnOOAFAYTxf4\n",
      "  Args:\n",
      "    __arg1: workflow/graph.py\n",
      "  search_codebase (call_wt4VcgKLQi72WWpkRi1fsF3v)\n",
      " Call ID: call_wt4VcgKLQi72WWpkRi1fsF3v\n",
      "  Args:\n",
      "    __arg1: main.py\n",
      "  search_codebase (call_4OKWDEcgoNJj5PCbzjjkFCdF)\n",
      " Call ID: call_4OKWDEcgoNJj5PCbzjjkFCdF\n",
      "  Args:\n",
      "    __arg1: app.py\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search_codebase\n",
      "\n",
      "No sufficiently relevant results found for: 'workflow/graph.py'\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search_codebase\n",
      "\n",
      "Found 2 relevant files:\n",
      "\n",
      "File: studio/__init__.py (Score: 0.61)\n",
      "Content: ...\n",
      "\n",
      "File: studio/langgraph.json (Score: 0.67)\n",
      "Content: {     \"dockerfile_lines\": [],     \"graphs\": {       \"chatbot\": \"./graph.py:app\"     },     \"env\": \"./.env\",     \"python_version\": \"3.11\",     \"depende...\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search_codebase\n",
      "\n",
      "Found 3 relevant files:\n",
      "\n",
      "File: studio/__init__.py (Score: 0.63)\n",
      "Content: ...\n",
      "\n",
      "File: main.py (Score: 0.67)\n",
      "Content: Functions: main...\n",
      "\n",
      "File: studio/langgraph.json (Score: 0.69)\n",
      "Content: {     \"dockerfile_lines\": [],     \"graphs\": {       \"chatbot\": \"./graph.py:app\"     },     \"env\": \"./.env\",     \"python_version\": \"3.11\",     \"depende...\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  search_codebase (call_bWVQ5uOn6JJq3e4aTR5QMijo)\n",
      " Call ID: call_bWVQ5uOn6JJq3e4aTR5QMijo\n",
      "  Args:\n",
      "    __arg1: workflow\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search_codebase\n",
      "\n",
      "Found 3 relevant files:\n",
      "\n",
      "File: workflow/graph.py (Score: 0.62)\n",
      "Content: Functions: route_feedback, create_workflow, start_workflow...\n",
      "\n",
      "File: workflow/__init__.py (Score: 0.63)\n",
      "Content: ...\n",
      "\n",
      "File: main.py (Score: 0.64)\n",
      "Content: Functions: main...\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "To understand how the workflow functions are implemented, let's focus on the relevant code snippets and files found during the searches:\n",
      "\n",
      "### Key Files and Functions\n",
      "\n",
      "1. **`workflow/graph.py`**\n",
      "   - This file contains several important functions related to workflow management:\n",
      "     - **`route_feedback`**: Likely handles feedback routing within the workflow process.\n",
      "     - **`create_workflow`**: Responsible for initializing or setting up new workflows.\n",
      "     - **`start_workflow`**: Begins the execution of a defined workflow.\n",
      "\n",
      "2. **`main.py`**\n",
      "   - Contains the main execution logic of the application, including initializing the application and possibly invoking the workflow functions.\n",
      "\n",
      "3. **`workflow/__init__.py`**\n",
      "   - This file typically initializes the workflow module and may import key functions from `graph.py` for use elsewhere in the application.\n",
      "\n",
      "### Overview of Functions\n",
      "\n",
      "- **`create_workflow`**: \n",
      "   - This function is essential for defining a new workflow, where it might accept parameters that dictate its structure (such as nodes, edges, and conditions for execution). The function likely sets up the workflow data structure and prepares it for execution.\n",
      "  \n",
      "- **`start_workflow`**: \n",
      "   - Invoked to kick off the execution of a previously defined workflow. This function probably handles necessary checks (like validation) and executes the tasks or nodes defined in the workflow sequence.\n",
      "\n",
      "- **`route_feedback`**: \n",
      "   - Designed to manage the feedback process, which can include sending information back from completed tasks in the workflow for logging, updates, or decision-making.\n",
      "\n",
      "### Code Structure Recommendations\n",
      "\n",
      "If you want to explore the actual code implementation, look into `workflow/graph.py` for the definitions of `create_workflow`, `start_workflow`, and `route_feedback`. You will likely find detailed comments and logic that will clarify how each function interacts within the workflow system.\n",
      "\n",
      "### Additional Considerations\n",
      "\n",
      "For a more integrative view of how these functions operate within the entire application:\n",
      "- Check how `main.py` orchestrates the workflow initiation.\n",
      "- Review any dependencies in `requirements.txt`, as these can often provide insight into what libraries or frameworks are being used that might affect workflow behavior.\n",
      "\n",
      "You may also want to run the application to observe how these functions are invoked in real time during execution. This can be especially valuable to visualize their impact in a live scenario.\n"
     ]
    }
   ],
   "source": [
    "def create_langgraph_agent():\n",
    "    \"\"\"\n",
    "    Create a LangGraph agent which handles parsing more robustly.\n",
    "    \"\"\"\n",
    "    from langgraph.prebuilt import create_react_agent\n",
    "    \n",
    "    # Initialize LLM\n",
    "    #llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "    \n",
    "    # Create LangGraph agent (more robust)\n",
    "    agent = create_react_agent(\n",
    "        model = \"openai:gpt-4o-mini\",\n",
    "        tools=[codebase_search_tool],\n",
    "        prompt = \"You are a helpful coding assistant. Use the search tool to find relevant code and provide detailed answers.\",\n",
    "        debug = True\n",
    "    )\n",
    "    \n",
    "    return agent\n",
    "\n",
    "agent = create_langgraph_agent()\n",
    "\n",
    "response = agent.invoke({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"How does the workflow functions\"}\n",
    "    ],\n",
    "    \"recursion_limit\": recursion_limit\n",
    "})\n",
    "\n",
    "for m in response['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To understand how the workflow functions are implemented, let's focus on the relevant code snippets and files found during the searches:\n",
       "\n",
       "### Key Files and Functions\n",
       "\n",
       "1. **`workflow/graph.py`**\n",
       "   - This file contains several important functions related to workflow management:\n",
       "     - **`route_feedback`**: Likely handles feedback routing within the workflow process.\n",
       "     - **`create_workflow`**: Responsible for initializing or setting up new workflows.\n",
       "     - **`start_workflow`**: Begins the execution of a defined workflow.\n",
       "\n",
       "2. **`main.py`**\n",
       "   - Contains the main execution logic of the application, including initializing the application and possibly invoking the workflow functions.\n",
       "\n",
       "3. **`workflow/__init__.py`**\n",
       "   - This file typically initializes the workflow module and may import key functions from `graph.py` for use elsewhere in the application.\n",
       "\n",
       "### Overview of Functions\n",
       "\n",
       "- **`create_workflow`**: \n",
       "   - This function is essential for defining a new workflow, where it might accept parameters that dictate its structure (such as nodes, edges, and conditions for execution). The function likely sets up the workflow data structure and prepares it for execution.\n",
       "  \n",
       "- **`start_workflow`**: \n",
       "   - Invoked to kick off the execution of a previously defined workflow. This function probably handles necessary checks (like validation) and executes the tasks or nodes defined in the workflow sequence.\n",
       "\n",
       "- **`route_feedback`**: \n",
       "   - Designed to manage the feedback process, which can include sending information back from completed tasks in the workflow for logging, updates, or decision-making.\n",
       "\n",
       "### Code Structure Recommendations\n",
       "\n",
       "If you want to explore the actual code implementation, look into `workflow/graph.py` for the definitions of `create_workflow`, `start_workflow`, and `route_feedback`. You will likely find detailed comments and logic that will clarify how each function interacts within the workflow system.\n",
       "\n",
       "### Additional Considerations\n",
       "\n",
       "For a more integrative view of how these functions operate within the entire application:\n",
       "- Check how `main.py` orchestrates the workflow initiation.\n",
       "- Review any dependencies in `requirements.txt`, as these can often provide insight into what libraries or frameworks are being used that might affect workflow behavior.\n",
       "\n",
       "You may also want to run the application to observe how these functions are invoked in real time during execution. This can be especially valuable to visualize their impact in a live scenario."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response['messages'][-1].content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools Play Ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodebaseTools:\n",
    "    \"\"\"Collection of tools for codebase search and analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, supabase_url: str, embedding_model: str = \"text-embedding-3-small\"):\n",
    "        self.supabase_url = supabase_url\n",
    "        self.embeddings = OpenAIEmbeddings(model=embedding_model)\n",
    "        self.collection_name = \"repo_files\"\n",
    "        self.dimension = 1536\n",
    "    \n",
    "    def search_codebase(self, query: str) -> str:\n",
    "        \"\"\"Search through code repositories to find relevant files and implementations.\"\"\"\n",
    "        try:\n",
    "            vx = vecs.create_client(self.supabase_url)\n",
    "            collection = vx.get_or_create_collection(\n",
    "                name=self.collection_name, \n",
    "                dimension=self.dimension\n",
    "            )\n",
    "            \n",
    "            query_embedding = self.embeddings.embed_query(query)\n",
    "            results = collection.query(\n",
    "                data=query_embedding,\n",
    "                limit=5,\n",
    "                include_value=True,\n",
    "                include_metadata=True\n",
    "            )\n",
    "            \n",
    "            if not results or len(results) == 0:\n",
    "                return f\"No relevant code found for query: '{query}'\"\n",
    "            \n",
    "            formatted_results = []\n",
    "            for result in results:\n",
    "                cosine_distance = result[1]\n",
    "                metadata = result[2]\n",
    "                \n",
    "                # Convert distance to similarity: similarity = 1 - distance\n",
    "                # For cosine distance: 0 = identical, 1 = completely different\n",
    "                similarity_score = 1 - cosine_distance\n",
    "                \n",
    "                if similarity_score >= 0.3:  # Adjusted threshold for similarity\n",
    "                    file_path = metadata.get('path', 'Unknown')\n",
    "                    content = metadata.get('content', '')\n",
    "                    repo_name = metadata.get('repo_name', 'Unknown')\n",
    "                    \n",
    "                    # Extract key information based on file type\n",
    "                    key_info = self._extract_file_info(file_path, content)\n",
    "                    \n",
    "                    formatted_results.append(\n",
    "                        f\"Repository: {repo_name}\\n\"\n",
    "                        f\"File: {file_path} (Score: {similarity_score:.2f})\\n\"\n",
    "                        f\"Content: {key_info}...\"\n",
    "                    )\n",
    "            \n",
    "            vx.disconnect()\n",
    "            \n",
    "            if not formatted_results:\n",
    "                return f\"No sufficiently relevant results found for: '{query}'\"\n",
    "            \n",
    "            return f\"Found {len(formatted_results)} relevant files:\\n\\n\" + \"\\n\\n\".join(formatted_results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Search error: {str(e)}\"\n",
    "\n",
    "    def list_directories(self, repo_name: str = \"\") -> list:\n",
    "        \"\"\"List all file paths (indices) in the repositories.\"\"\"\n",
    "        try:\n",
    "            vx = vecs.create_client(self.supabase_url)\n",
    "            collection = vx.get_or_create_collection(\n",
    "                name=self.collection_name, \n",
    "                dimension=self.dimension\n",
    "            )\n",
    "            \n",
    "            # Get all file paths\n",
    "            results = collection.query(\n",
    "                data=[0.0] * self.dimension,\n",
    "                limit=1000,\n",
    "                include_metadata=False\n",
    "            )\n",
    "            \n",
    "            vx.disconnect()\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            return [f\"Error: {str(e)}\"]\n",
    "\n",
    "    def get_file_content(self, file_path: str, repo_name: str = \"\") -> str:\n",
    "        \"\"\"Get the full content of a specific file using direct fetch by ID.\"\"\"\n",
    "        try:\n",
    "            vx = vecs.create_client(self.supabase_url)\n",
    "            collection = vx.get_or_create_collection(\n",
    "                name=self.collection_name, \n",
    "                dimension=self.dimension\n",
    "            )\n",
    "            \n",
    "            # The file_path IS the ID, so fetch directly\n",
    "            result = collection.fetch(ids=[file_path])\n",
    "            \n",
    "            if result and len(result) > 0:\n",
    "                metadata = result[0][2]  # Get metadata from fetch result\n",
    "                content = metadata.get('content', '')\n",
    "                current_path = metadata.get('path', '')\n",
    "                current_repo = metadata.get('repo_name', '')\n",
    "                vx.disconnect()\n",
    "                return f\"File: {current_repo}:{current_path}\\n\\n{content}\"\n",
    "            \n",
    "            vx.disconnect()\n",
    "            return f\"File '{file_path}' not found\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"File content error: {str(e)}\"\n",
    "    \n",
    "    def analyze_code_structure(self, query: str) -> str:\n",
    "        \"\"\"Analyze code structure and patterns across the codebase.\"\"\"\n",
    "        try:\n",
    "            vx = vecs.create_client(self.supabase_url)\n",
    "            collection = vx.get_or_create_collection(\n",
    "                name=self.collection_name, \n",
    "                dimension=self.dimension\n",
    "            )\n",
    "            \n",
    "            query_embedding = self.embeddings.embed_query(query)\n",
    "            results = collection.query(\n",
    "                data=query_embedding,\n",
    "                limit=10,\n",
    "                include_value=True,\n",
    "                include_metadata=True\n",
    "            )\n",
    "            \n",
    "            if not results:\n",
    "                return f\"No code found for analysis: '{query}'\"\n",
    "            \n",
    "            # Debug: Show similarity scores\n",
    "            debug_info = []\n",
    "            for i, result in enumerate(results[:3]):\n",
    "                if len(result) > 2:\n",
    "                    cosine_distance = result[1]\n",
    "                    similarity_score = 1 - cosine_distance\n",
    "                    metadata = result[2]\n",
    "                    file_path = metadata.get('path', 'Unknown')\n",
    "                    debug_info.append(f\"Result {i+1}: {file_path} (similarity: {similarity_score:.3f})\")\n",
    "            \n",
    "            debug_text = \"Debug - Top results:\\n\" + \"\\n\".join(debug_info) + \"\\n\\n\"\n",
    "            \n",
    "            analysis = {\n",
    "                'functions': set(),\n",
    "                'classes': set(),\n",
    "                'imports': set(),\n",
    "                'patterns': [],\n",
    "                'files_analyzed': 0\n",
    "            }\n",
    "            \n",
    "            for result in results:\n",
    "                if len(result) > 2:\n",
    "                    cosine_distance = result[1]\n",
    "                    similarity_score = 1 - cosine_distance\n",
    "                    if similarity_score >= 0.2:  # Lower threshold to include more results\n",
    "                        metadata = result[2]\n",
    "                        content = metadata.get('content', '')\n",
    "                        file_path = metadata.get('path', '')\n",
    "                        \n",
    "                        analysis['files_analyzed'] += 1\n",
    "                        \n",
    "                        # Extract Python patterns\n",
    "                        if file_path.endswith('.py'):\n",
    "                            functions = re.findall(r'def\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*\\(', content)\n",
    "                            classes = re.findall(r'class\\s+([A-Z][a-zA-Z0-9_]*)\\s*[\\(:]', content)\n",
    "                            imports = re.findall(r'(?:from\\s+(\\S+)\\s+)?import\\s+([^\\n]+)', content)\n",
    "                            \n",
    "                            analysis['functions'].update(functions)\n",
    "                            analysis['classes'].update(classes)\n",
    "                            for imp in imports:\n",
    "                                if imp[0]:  # from X import Y\n",
    "                                    analysis['imports'].add(f\"from {imp[0]} import {imp[1]}\")\n",
    "                                else:  # import X\n",
    "                                    analysis['imports'].add(f\"import {imp[1]}\")\n",
    "                        \n",
    "                        # Extract JavaScript/TypeScript patterns\n",
    "                        elif file_path.endswith(('.js', '.ts', '.jsx', '.tsx')):\n",
    "                            functions = re.findall(r'(?:function\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*\\(|([a-zA-Z_][a-zA-Z0-9_]*)\\s*=\\s*(?:async\\s+)?(?:function|\\([^)]*\\)\\s*=>))', content)\n",
    "                            classes = re.findall(r'class\\s+([A-Z][a-zA-Z0-9_]*)\\s*[\\{]', content)\n",
    "                            imports = re.findall(r'import\\s+.*?from\\s+[\\'\"]([^\\'\"]+)[\\'\"]', content)\n",
    "                            \n",
    "                            for func_match in functions:\n",
    "                                func_name = func_match[0] or func_match[1]\n",
    "                                if func_name:\n",
    "                                    analysis['functions'].add(func_name)\n",
    "                            \n",
    "                            analysis['classes'].update(classes)\n",
    "                            analysis['imports'].update(imports)\n",
    "            \n",
    "            vx.disconnect()\n",
    "            \n",
    "            # Format analysis results\n",
    "            result_parts = [\n",
    "                f\"Code Structure Analysis for: '{query}'\",\n",
    "                f\"Files analyzed: {analysis['files_analyzed']}\",\n",
    "                \"\"\n",
    "            ]\n",
    "            \n",
    "            if analysis['functions']:\n",
    "                result_parts.append(f\"Functions found ({len(analysis['functions'])}):\")\n",
    "                result_parts.extend([f\"  - {func}\" for func in sorted(list(analysis['functions']))[:10]])\n",
    "                if len(analysis['functions']) > 10:\n",
    "                    result_parts.append(f\"  ... and {len(analysis['functions']) - 10} more\")\n",
    "                result_parts.append(\"\")\n",
    "            \n",
    "            if analysis['classes']:\n",
    "                result_parts.append(f\"Classes found ({len(analysis['classes'])}):\")\n",
    "                result_parts.extend([f\"  - {cls}\" for cls in sorted(list(analysis['classes']))[:10]])\n",
    "                result_parts.append(\"\")\n",
    "            \n",
    "            if analysis['imports']:\n",
    "                result_parts.append(f\"Key imports ({len(analysis['imports'])}):\")\n",
    "                result_parts.extend([f\"  - {imp}\" for imp in sorted(list(analysis['imports']))[:10]])\n",
    "                if len(analysis['imports']) > 10:\n",
    "                    result_parts.append(f\"  ... and {len(analysis['imports']) - 10} more\")\n",
    "            \n",
    "            return debug_text + \"\\n\".join(result_parts)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Code analysis error: {str(e)}\"\n",
    "    \n",
    "    def _extract_file_info(self, file_path: str, content: str) -> str:\n",
    "        \"\"\"Extract key information from file content based on file type.\"\"\"\n",
    "        if '.py' in file_path:\n",
    "            functions = re.findall(r'def\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*\\(', content)\n",
    "            classes = re.findall(r'class\\s+([A-Z][a-zA-Z0-9_]*)\\s*[\\(:]', content)\n",
    "            key_info = \"\"\n",
    "            if functions:\n",
    "                key_info = f\"Functions: {', '.join(functions[:3])}\"\n",
    "            if classes:\n",
    "                key_info += f\" Classes: {', '.join(classes[:2])}\"\n",
    "            clean_content = content[:150].replace('\\n', ' ').strip()\n",
    "            return key_info or clean_content\n",
    "        \n",
    "        elif file_path.endswith(('.js', '.ts', '.jsx', '.tsx')):\n",
    "            functions = re.findall(r'(?:function\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*\\(|([a-zA-Z_][a-zA-Z0-9_]*)\\s*=\\s*(?:async\\s+)?(?:function|\\([^)]*\\)\\s*=>))', content)\n",
    "            func_names = [f[0] or f[1] for f in functions if f[0] or f[1]]\n",
    "            classes = re.findall(r'class\\s+([A-Z][a-zA-Z0-9_]*)\\s*[\\{]', content)\n",
    "            key_info = \"\"\n",
    "            if func_names:\n",
    "                key_info = f\"Functions: {', '.join(func_names[:3])}\"\n",
    "            if classes:\n",
    "                key_info += f\" Classes: {', '.join(classes[:2])}\"\n",
    "            clean_content = content[:150].replace('\\n', ' ').strip()\n",
    "            return key_info or clean_content\n",
    "        \n",
    "        elif '.md' in file_path:\n",
    "            lines = content.split('\\n')[:3]\n",
    "            return ' '.join(lines).strip()[:200]\n",
    "        \n",
    "        elif file_path.endswith(('.json', '.yaml', '.yml')):\n",
    "            clean_content = content[:100].replace('\\n', ' ').strip()\n",
    "            return f\"Config file: {clean_content}\"\n",
    "        \n",
    "        else:\n",
    "            clean_content = content[:150].replace('\\n', ' ').strip()\n",
    "            return clean_content\n",
    "    \n",
    "    def _is_in_directory(self, file_path: str, target_directory: str) -> bool:\n",
    "        \"\"\"Check if a file path is within the target directory.\"\"\"\n",
    "        # Normalize paths\n",
    "        file_dir = os.path.dirname(file_path).replace('\\\\', '/')\n",
    "        target_dir = target_directory.strip('/').replace('\\\\', '/')\n",
    "        \n",
    "        if not target_dir:  # Root directory\n",
    "            return '/' not in file_dir\n",
    "        \n",
    "        # Check if file is in the directory or subdirectory\n",
    "        return file_dir == target_dir or file_dir.startswith(target_dir + '/')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment variables loaded successfully\n",
      "✅ CodebaseTools initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Get your Supabase URL from environment variables\n",
    "supabase_url = os.getenv(\"SUPABASE_DB_URL\")\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not supabase_url:\n",
    "    print(\"❌ Error: SUPABASE_DB_URL environment variable not set\")\n",
    "elif not openai_key:\n",
    "    print(\"❌ Error: OPENAI_API_KEY environment variable not set\")\n",
    "else:\n",
    "    print(\"✅ Environment variables loaded successfully\")\n",
    "    \n",
    "    # Initialize the tools\n",
    "    tools = CodebaseTools(supabase_url)\n",
    "    print(\"✅ CodebaseTools initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The `vecs.Collection` class represents a collection of vectors within a PostgreSQL database with pgvector support.\n",
      "    It provides methods to manage (create, delete, fetch, upsert), index, and perform similarity searches on these vector collections.\n",
      "\n",
      "    The collections are stored in separate tables in the database, with each vector associated with an identifier and optional metadata.\n",
      "\n",
      "    Example usage:\n",
      "\n",
      "        with vecs.create_client(DB_CONNECTION) as vx:\n",
      "            collection = vx.create_collection(name=\"docs\", dimension=3)\n",
      "            collection.upsert([(\"id1\", [1, 1, 1], {\"key\": \"value\"})])\n",
      "            # Further operations on 'collection'\n",
      "\n",
      "    Public Attributes:\n",
      "        name: The name of the vector collection.\n",
      "        dimension: The dimension of vectors in the collection.\n",
      "\n",
      "    Note: Some methods of this class can raise exceptions from the `vecs.exc` module if errors occur.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "collection = vx.get_or_create_collection(name=\"repo_files\", dimension=1536)\n",
    "\n",
    "# get the indeces of the collection\n",
    "print(collection.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tmpqv23rcta/README.md',\n",
       " 'tmpqv23rcta/app.py',\n",
       " 'tmpqv23rcta/main.py',\n",
       " 'tmpqv23rcta/requirements.txt',\n",
       " 'tmpqv23rcta/n8n/main (NO img2img).json',\n",
       " 'tmpqv23rcta/studio/__init__.py',\n",
       " 'tmpqv23rcta/studio/graph.py',\n",
       " 'tmpqv23rcta/studio/langgraph.json',\n",
       " 'tmpqv23rcta/utils/config.py',\n",
       " 'tmpqv23rcta/workflow/__init__.py',\n",
       " 'tmpqv23rcta/workflow/graph.py',\n",
       " 'tmpqv23rcta/workflow/nodes.py',\n",
       " 'tmpqv23rcta/workflow/state.py',\n",
       " 'tmpqv23rcta/workflow/tools.py']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.list_directories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Code Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 relevant files:\n",
      "\n",
      "Repository: tmpqv23rcta\n",
      "File: workflow/tools.py (Score: 0.49)\n",
      "Content: Functions: tavily_search...\n",
      "\n",
      "Repository: tmpqv23rcta\n",
      "File: n8n/main (NO img2img).json (Score: 0.24)\n",
      "Content: Config file: {   \"name\": \"main (NO img2img)\",   \"nodes\": [     {       \"parameters\": {         \"options\": {}...\n",
      "\n",
      "Repository: tmpqv23rcta\n",
      "File: README.md (Score: 0.21)\n",
      "Content: # LinkedIn-Booster 🚀  An intelligent AI-powered LinkedIn post creation workflow that combines the power of LangChain, LangGraph, and multiple AI services to create engaging LinkedIn content with match...\n",
      "\n",
      "Repository: tmpqv23rcta\n",
      "File: requirements.txt (Score: 0.20)\n",
      "Content: chainlit>=1.0.0 langchain>=0.1.0 langchain-community>=0.1.0 langchain-google-genai>=1.0.0 langchain-google-community>=0.1.0 langgraph>=0.0.40 requests...\n"
     ]
    }
   ],
   "source": [
    "results = tools.search_codebase(\"The web search tool\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = model.embed_query(\"The web search tool\")\n",
    "\n",
    "results = collection.query(\n",
    "            data=query_embedding,\n",
    "            limit=5,\n",
    "            include_value=True,\n",
    "            include_metadata=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tmpqv23rcta/workflow/tools.py', 0.511380403555607),\n",
       " ('tmpqv23rcta/n8n/main (NO img2img).json', 0.764289990919228),\n",
       " ('tmpqv23rcta/README.md', 0.789835356503363),\n",
       " ('tmpqv23rcta/requirements.txt', 0.795650249849252),\n",
       " ('tmpqv23rcta/workflow/nodes.py', 0.829359775261906)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Executes a similarity search in the collection.\n",
      "\n",
      "        The return type is dependent on arguments *include_value* and *include_metadata*\n",
      "\n",
      "        Args:\n",
      "            data (Any): The vector to use as the query.\n",
      "            limit (int, optional): The maximum number of results to return. Defaults to 10.\n",
      "            filters (Optional[Dict], optional): Filters to apply to the search. Defaults to None.\n",
      "            measure (Union[IndexMeasure, str], optional): The distance measure to use for the search. Defaults to 'cosine_distance'.\n",
      "            include_value (bool, optional): Whether to include the distance value in the results. Defaults to False.\n",
      "            include_metadata (bool, optional): Whether to include the metadata in the results. Defaults to False.\n",
      "            probes (Optional[Int], optional): Number of ivfflat index lists to query. Higher increases accuracy but decreases speed\n",
      "            ef_search (Optional[Int], optional): Size of the dynamic candidate list for HNSW index search. Higher increases accuracy but decreases speed\n",
      "            skip_adapter (bool, optional): When True, skips any associated adapter and queries using a literal vector provided to *data*\n",
      "\n",
      "        Returns:\n",
      "            Union[List[Record], List[str]]: The result of the similarity search.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(collection.query.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get File Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\"Standalone graph file for LangGraph Studio.\"\"\"\\nimport sys\\nimport os\\nfrom typing import Optional\\nfrom dataclasses import dataclass\\n\\n# Add the parent directory to the path so we can import from workflow\\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\\n\\nfrom langgraph.graph import StateGraph, END\\n\\n# Define state directly to avoid import conflicts\\n@dataclass\\nclass WorkflowState:\\n    \"\"\"State management for the LinkedIn post creation workflow.\"\"\"\\n    user_input: str = \"\"\\n    post_text: str = \"\"\\n    image_prompt: str = \"\"\\n    image_data: Optional[bytes] = None\\n    image_url: str = \"\"\\n    user_feedback: str = \"\"\\n    classification: str = \"\"\\n    refined_text: str = \"\"\\n    refined_image_data: Optional[bytes] = None\\n    current_step: str = \"start\"\\n    error_message: str = \"\"\\n    final_post_ready: bool = False\\n\\n# Import nodes after defining state\\ntry:\\n    from workflow.nodes import (\\n        create_post_text,\\n        generate_image,\\n        classify_feedback,\\n        refine_text,\\n        refine_image,\\n        send_email,\\n    )\\nexcept ImportError as e:\\n    # Fallback dummy functions for studio\\n    def create_post_text(state: WorkflowState) -> WorkflowState:\\n        state.post_text = \"Sample LinkedIn post created\"\\n        return state\\n    \\n    def generate_image(state: WorkflowState) -> WorkflowState:\\n        state.image_url = \"https://example.com/image.jpg\"\\n        return state\\n    \\n    def classify_feedback(state: WorkflowState) -> WorkflowState:\\n        state.classification = \"approved\"\\n        return state\\n    \\n    def refine_text(state: WorkflowState) -> WorkflowState:\\n        state.refined_text = \"Refined post text\"\\n        return state\\n    \\n    def refine_image(state: WorkflowState) -> WorkflowState:\\n        state.refined_image_data = b\"refined_image_data\"\\n        return state\\n    \\n    def send_email(state: WorkflowState) -> WorkflowState:\\n        state.final_post_ready = True\\n        return state\\n\\n\\ndef route_feedback(state: WorkflowState) -> str:\\n    \"\"\"Route based on feedback classification.\"\"\"\\n    classification = state.classification.lower()\\n    if \"approved\" in classification:\\n        return \"approved\"\\n    if \"refine text\" in classification or \"refine_text\" in classification:\\n        return \"refine_text\"\\n    if \"refine image\" in classification or \"refine_image\" in classification:\\n        return \"refine_image\"\\n    if \"terminate\" in classification:\\n        return \"terminate\"\\n    return \"terminate\"\\n\\n\\ndef create_studio_workflow():\\n    \"\"\"Create the LangGraph workflow for Studio (without checkpointer).\"\"\"\\n    workflow = StateGraph(WorkflowState)\\n    workflow.add_node(\"create_post\", create_post_text)\\n    workflow.add_node(\"generate_image\", generate_image)\\n    workflow.add_node(\"classify_feedback\", classify_feedback)\\n    workflow.add_node(\"refine_text\", refine_text)\\n    workflow.add_node(\"refine_image\", refine_image)\\n    workflow.add_node(\"send_email\", send_email)\\n    workflow.set_entry_point(\"create_post\")\\n    workflow.add_edge(\"create_post\", \"generate_image\")\\n    workflow.add_edge(\"generate_image\", \"classify_feedback\")\\n    workflow.add_conditional_edges(\\n        \"classify_feedback\",\\n        route_feedback,\\n        {\\n            \"approved\": \"send_email\",\\n            \"refine_text\": \"refine_text\",\\n            \"refine_image\": \"refine_image\",\\n            \"terminate\": END,\\n        },\\n    )\\n    workflow.add_edge(\"refine_text\", \"classify_feedback\")\\n    workflow.add_edge(\"refine_image\", \"classify_feedback\")\\n    workflow.add_edge(\"send_email\", END)\\n    \\n    # Compile without checkpointer for LangGraph Studio\\n    app = workflow.compile(\\n        interrupt_after=[\"generate_image\", \"refine_text\", \"refine_image\"],\\n    )\\n    return app\\n\\n\\n# Create the app for LangGraph Studio\\napp = create_studio_workflow()\\n\\n# Export the app for LangGraph Studio\\n__all__ = [\"app\"]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.fetch(ids=[\"tmpqv23rcta/studio/graph.py\"])[0][2].get('content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tmpqv23rcta/workflow/tools.py', array([-0.01585723,  0.01547127, -0.02408358, ...,  0.01043181,\n",
       "         0.00584997, -0.03328033], shape=(1536,), dtype=float32), {'path': 'workflow/tools.py', 'content': 'import os\\nimport requests\\nfrom typing import Any, Dict\\n\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\ ... (980 characters truncated) ... cription=\"Search the web for current information\",\\n    func=tavily_search\\n)\\n', 'branches': ['main'], 'repo_name': 'tmpqv23rcta', 'commit_count': 6})]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.fetch(ids=[\"tmpqv23rcta/workflow/tools.py\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Fetches vectors from the collection by their identifiers.\n",
      "\n",
      "        Args:\n",
      "            ids (Iterable[str]): An iterable of vector identifiers.\n",
      "\n",
      "        Returns:\n",
      "            List[Record]: A list of the fetched vectors.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(collection.fetch.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tools.get_file_content(\"tmpqv23rcta/studio/graph.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: tmpqv23rcta:studio/graph.py\n",
      "\n",
      "\"\"\"Standalone graph file for LangGraph Studio.\"\"\"\n",
      "import sys\n",
      "import os\n",
      "from typing import Optional\n",
      "from dataclasses import dataclass\n",
      "\n",
      "# Add the parent directory to the path so we can import from workflow\n",
      "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
      "\n",
      "from langgraph.graph import StateGraph, END\n",
      "\n",
      "# Define state directly to avoid import conflicts\n",
      "@dataclass\n",
      "class WorkflowState:\n",
      "    \"\"\"State management for the LinkedIn post creation workflow.\"\"\"\n",
      "    user_input: str = \"\"\n",
      "    post_text: str = \"\"\n",
      "    image_prompt: str = \"\"\n",
      "    image_data: Optional[bytes] = None\n",
      "    image_url: str = \"\"\n",
      "    user_feedback: str = \"\"\n",
      "    classification: str = \"\"\n",
      "    refined_text: str = \"\"\n",
      "    refined_image_data: Optional[bytes] = None\n",
      "    current_step: str = \"start\"\n",
      "    error_message: str = \"\"\n",
      "    final_post_ready: bool = False\n",
      "\n",
      "# Import nodes after defining state\n",
      "try:\n",
      "    from workflow.nodes import (\n",
      "        create_post_text,\n",
      "        generate_image,\n",
      "        classify_feedback,\n",
      "        refine_text,\n",
      "        refine_image,\n",
      "        send_email,\n",
      "    )\n",
      "except ImportError as e:\n",
      "    # Fallback dummy functions for studio\n",
      "    def create_post_text(state: WorkflowState) -> WorkflowState:\n",
      "        state.post_text = \"Sample LinkedIn post created\"\n",
      "        return state\n",
      "    \n",
      "    def generate_image(state: WorkflowState) -> WorkflowState:\n",
      "        state.image_url = \"https://example.com/image.jpg\"\n",
      "        return state\n",
      "    \n",
      "    def classify_feedback(state: WorkflowState) -> WorkflowState:\n",
      "        state.classification = \"approved\"\n",
      "        return state\n",
      "    \n",
      "    def refine_text(state: WorkflowState) -> WorkflowState:\n",
      "        state.refined_text = \"Refined post text\"\n",
      "        return state\n",
      "    \n",
      "    def refine_image(state: WorkflowState) -> WorkflowState:\n",
      "        state.refined_image_data = b\"refined_image_data\"\n",
      "        return state\n",
      "    \n",
      "    def send_email(state: WorkflowState) -> WorkflowState:\n",
      "        state.final_post_ready = True\n",
      "        return state\n",
      "\n",
      "\n",
      "def route_feedback(state: WorkflowState) -> str:\n",
      "    \"\"\"Route based on feedback classification.\"\"\"\n",
      "    classification = state.classification.lower()\n",
      "    if \"approved\" in classification:\n",
      "        return \"approved\"\n",
      "    if \"refine text\" in classification or \"refine_text\" in classification:\n",
      "        return \"refine_text\"\n",
      "    if \"refine image\" in classification or \"refine_image\" in classification:\n",
      "        return \"refine_image\"\n",
      "    if \"terminate\" in classification:\n",
      "        return \"terminate\"\n",
      "    return \"terminate\"\n",
      "\n",
      "\n",
      "def create_studio_workflow():\n",
      "    \"\"\"Create the LangGraph workflow for Studio (without checkpointer).\"\"\"\n",
      "    workflow = StateGraph(WorkflowState)\n",
      "    workflow.add_node(\"create_post\", create_post_text)\n",
      "    workflow.add_node(\"generate_image\", generate_image)\n",
      "    workflow.add_node(\"classify_feedback\", classify_feedback)\n",
      "    workflow.add_node(\"refine_text\", refine_text)\n",
      "    workflow.add_node(\"refine_image\", refine_image)\n",
      "    workflow.add_node(\"send_email\", send_email)\n",
      "    workflow.set_entry_point(\"create_post\")\n",
      "    workflow.add_edge(\"create_post\", \"generate_image\")\n",
      "    workflow.add_edge(\"generate_image\", \"classify_feedback\")\n",
      "    workflow.add_conditional_edges(\n",
      "        \"classify_feedback\",\n",
      "        route_feedback,\n",
      "        {\n",
      "            \"approved\": \"send_email\",\n",
      "            \"refine_text\": \"refine_text\",\n",
      "            \"refine_image\": \"refine_image\",\n",
      "            \"terminate\": END,\n",
      "        },\n",
      "    )\n",
      "    workflow.add_edge(\"refine_text\", \"classify_feedback\")\n",
      "    workflow.add_edge(\"refine_image\", \"classify_feedback\")\n",
      "    workflow.add_edge(\"send_email\", END)\n",
      "    \n",
      "    # Compile without checkpointer for LangGraph Studio\n",
      "    app = workflow.compile(\n",
      "        interrupt_after=[\"generate_image\", \"refine_text\", \"refine_image\"],\n",
      "    )\n",
      "    return app\n",
      "\n",
      "\n",
      "# Create the app for LangGraph Studio\n",
      "app = create_studio_workflow()\n",
      "\n",
      "# Export the app for LangGraph Studio\n",
      "__all__ = [\"app\"]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Code Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tools.analyze_code_structure(\"Image generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug - Top results:\n",
      "Result 1: n8n/main (NO img2img).json (similarity: 0.299)\n",
      "Result 2: workflow/nodes.py (similarity: 0.270)\n",
      "Result 3: README.md (similarity: 0.228)\n",
      "\n",
      "Code Structure Analysis for: 'Image generation'\n",
      "Files analyzed: 3\n",
      "\n",
      "Functions found (6):\n",
      "  - classify_feedback\n",
      "  - create_post_text\n",
      "  - generate_image\n",
      "  - refine_image\n",
      "  - refine_text\n",
      "  - send_email\n",
      "\n",
      "Key imports (13):\n",
      "  - from dotenv import load_dotenv\n",
      "  - from langchain.chat_models import init_chat_model\n",
      "  - from langchain.schema import SystemMessage, HumanMessage\n",
      "  - from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
      "  - from langchain_google_community import GmailToolkit\n",
      "  - from langchain_google_community.gmail.utils import (\n",
      "  - from langgraph.errors import GraphRecursionError\n",
      "  - from langgraph.prebuilt import create_react_agent\n",
      "  - from typing import List\n",
      "  - from workflow.state import WorkflowState\n",
      "  ... and 3 more\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-sitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree_sitter_python as tspython\n",
    "from tree_sitter import Language, Parser\n",
    "\n",
    "PY_LANGUAGE = Language(tspython.language())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser(PY_LANGUAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = parser.parse(\n",
    "    bytes(\n",
    "        '''\n",
    "        import os\n",
    "        from langchain_openai import OpenAIEmbeddings\n",
    "        from dotenv import load_dotenv\n",
    "        import vecs\n",
    "        import json\n",
    "\n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "\n",
    "\n",
    "        def get_embedding_model():\n",
    "            \"\"\"Initialize and return the OpenAI embedding model.\"\"\"\n",
    "            return OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "\n",
    "        def get_vector_client():\n",
    "            \"\"\"Initialize and return the Supabase vector client.\"\"\"\n",
    "            supabase_db_url = os.getenv(\"SUPABASE_DB_URL\")\n",
    "            if not supabase_db_url:\n",
    "                raise ValueError(\"SUPABASE_DB_URL environment variable not set\")\n",
    "            return vecs.create_client(supabase_db_url)\n",
    "\n",
    "\n",
    "        def store_repo_in_own_collection(repo_info, refresh=False):\n",
    "            \"\"\"\n",
    "            Store repository information in a dedicated vector collection.\n",
    "            \n",
    "            Args:\n",
    "                repo_info (dict): Repository information containing files and metadata\n",
    "                refresh (bool): Whether to delete existing collection before creating new one\n",
    "            \n",
    "            Returns:\n",
    "                str: Name of the collection created\n",
    "            \"\"\"\n",
    "            repo_name = repo_info['repo_name']\n",
    "            \n",
    "            # Initialize clients\n",
    "            vx = get_vector_client()\n",
    "            embeddings = get_embedding_model()\n",
    "            \n",
    "            if refresh:\n",
    "                # Drop existing collection if it exists\n",
    "                try:\n",
    "                    vx.delete_collection(repo_name)\n",
    "                    print(f\"🗑️ Old collection '{repo_name}' deleted.\")\n",
    "                except Exception:\n",
    "                    pass  # collection may not exist yet\n",
    "            \n",
    "            # Create/get a dedicated collection for this repo\n",
    "            collection = vx.get_or_create_collection(name=repo_name, dimension=1536)\n",
    "            \n",
    "            vectors_to_upsert = []\n",
    "            \n",
    "            for file in repo_info['files']:\n",
    "                content = file['content']\n",
    "                \n",
    "                # Generate embedding for file content\n",
    "                embedding = embeddings.embed_query(content)\n",
    "                \n",
    "                # Create unique ID for this file\n",
    "                unique_id = f\"{repo_name}/{file['path']}\"\n",
    "                \n",
    "                # Prepare metadata (store full content like your working version)\n",
    "                metadata = {\n",
    "                    'repo_name': repo_name,\n",
    "                    'path': file['path'],\n",
    "                    'content': content,  # Store the full content for retrieval\n",
    "                    'commit_count': repo_info['commit_count'],\n",
    "                    'branches': repo_info['branches']  # List is JSON-serializable, don't stringify\n",
    "                }\n",
    "                \n",
    "                vectors_to_upsert.append((unique_id, embedding, metadata))\n",
    "            \n",
    "            # Upsert all vectors at once\n",
    "            collection.upsert(vectors_to_upsert)\n",
    "            \n",
    "            print(f\"✅ Stored {len(vectors_to_upsert)} files into collection '{repo_name}'\")\n",
    "            return repo_name\n",
    "\n",
    "\n",
    "        def search_repo_collection(repo_name, query, limit=5):\n",
    "            \"\"\"\n",
    "            Search within a specific repository collection.\n",
    "            \n",
    "            Args:\n",
    "                repo_name (str): Name of the repository collection\n",
    "                query (str): Search query\n",
    "                limit (int): Maximum number of results to return\n",
    "            \n",
    "            Returns:\n",
    "                list: Search results with metadata\n",
    "            \"\"\"\n",
    "            vx = get_vector_client()\n",
    "            embeddings = get_embedding_model()\n",
    "            \n",
    "            try:\n",
    "                collection = vx.get_collection(name=repo_name)\n",
    "                query_embedding = embeddings.embed_query(query)\n",
    "                \n",
    "                # Query the collection\n",
    "                results = collection.query(\n",
    "                    data=query_embedding,\n",
    "                    limit=limit,\n",
    "                    include_metadata=True,\n",
    "                    include_value=True\n",
    "                )\n",
    "                \n",
    "                # Convert results to a more usable format\n",
    "                # vecs returns SQLAlchemy Row objects with: (id, metadata)\n",
    "                formatted_results = []\n",
    "                for i, result in enumerate(results):\n",
    "                    formatted_result = {\n",
    "                        'id': result[0] if len(result) > 0 else None,\n",
    "                        'cos_distance': result[1],\n",
    "                        'metadata': result[2] if len(result) > 1 else {}\n",
    "                    }\n",
    "                    formatted_results.append(formatted_result)\n",
    "                \n",
    "                return formatted_results\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error searching collection '{repo_name}': {e}\")\n",
    "                return []\n",
    "\n",
    "\n",
    "        def list_repo_collections():\n",
    "            \"\"\"\n",
    "            List all available repository collections.\n",
    "            \n",
    "            Returns:\n",
    "                list: Names of all collections\n",
    "            \"\"\"\n",
    "            vx = get_vector_client()\n",
    "            try:\n",
    "                collections = vx.list_collections()\n",
    "                return [col.name for col in collections]\n",
    "            except Exception as e:\n",
    "                print(f\"Error listing collections: {e}\")\n",
    "                return []\n",
    "\n",
    "\n",
    "        def delete_repo_collection(repo_name):\n",
    "            \"\"\"\n",
    "            Delete a repository collection.\n",
    "            \n",
    "            Args:\n",
    "                repo_name (str): Name of the repository collection to delete\n",
    "            \n",
    "            Returns:\n",
    "                bool: True if successful, False otherwise\n",
    "            \"\"\"\n",
    "            vx = get_vector_client()\n",
    "            try:\n",
    "                vx.delete_collection(repo_name)\n",
    "                print(f\"🗑️ Collection '{repo_name}' deleted successfully.\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting collection '{repo_name}': {e}\")\n",
    "                return False\n",
    "\n",
    "\n",
    "        # Example usage\n",
    "        if __name__ == \"__main__\":\n",
    "            # This would typically be called with repo_info from git_utils\n",
    "            # store_repo_in_own_collection(repo_info)\n",
    "            pass\n",
    "\n",
    "        def debug_search_results(repo_name, query, limit=2):\n",
    "            \"\"\"\n",
    "            Debug function to inspect the raw search results format.\n",
    "            \n",
    "            Args:\n",
    "                repo_name (str): Name of the repository collection\n",
    "                query (str): Search query\n",
    "                limit (int): Maximum number of results to return\n",
    "            \"\"\"\n",
    "            vx = get_vector_client()\n",
    "            embeddings = get_embedding_model()\n",
    "            \n",
    "            try:\n",
    "                collection = vx.get_collection(name=repo_name)\n",
    "                query_embedding = embeddings.embed_query(query)\n",
    "                \n",
    "                print(f\"🔍 Debug search in '{repo_name}' for query: '{query}'\")\n",
    "                \n",
    "                # Query the collection\n",
    "                raw_results = collection.query(\n",
    "                    data=query_embedding,\n",
    "                    limit=limit,\n",
    "                    include_metadata=True\n",
    "                )\n",
    "                \n",
    "                print(f\"📊 Raw results type: {type(raw_results)}\")\n",
    "                print(f\"📊 Raw results length: {len(raw_results) if hasattr(raw_results, '__len__') else 'N/A'}\")\n",
    "                \n",
    "                if raw_results:\n",
    "                    print(f\"📊 First result type: {type(raw_results[0])}\")\n",
    "                    print(f\"📊 First result: {raw_results[0]}\")\n",
    "                    \n",
    "                    if len(raw_results[0]) > 0:\n",
    "                        print(f\"📊 First result parts:\")\n",
    "                        for i, part in enumerate(raw_results[0]):\n",
    "                            print(f\"   Part {i}: {type(part)} = {part}\")\n",
    "                \n",
    "                return raw_results\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Debug error: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                return None\n",
    "        def get_file_content(repo_name, file_path):\n",
    "            \"\"\"\n",
    "            Get the full content of a specific file from the vector database.\n",
    "            \n",
    "            Args:\n",
    "                repo_name (str): Name of the repository collection\n",
    "                file_path (str): Path of the file to retrieve\n",
    "            \n",
    "            Returns:\n",
    "                str: File content or None if not found\n",
    "            \"\"\"\n",
    "            vx = get_vector_client()\n",
    "            \n",
    "            try:\n",
    "                collection = vx.get_collection(name=repo_name)\n",
    "                \n",
    "                # Search for the specific file by ID\n",
    "                file_id = f\"{repo_name}/{file_path}\"\n",
    "                \n",
    "                # Use a simple query to find the exact file\n",
    "                # Since we can't query by ID directly, we'll search and filter\n",
    "                embeddings = get_embedding_model()\n",
    "                dummy_query = embeddings.embed_query(\"content\")  # Dummy query\n",
    "                \n",
    "                results = collection.query(\n",
    "                    data=dummy_query,\n",
    "                    limit=100,  # Get more results to find our file\n",
    "                    include_metadata=True\n",
    "                )\n",
    "                \n",
    "                # Find the specific file\n",
    "                for result in results:\n",
    "                    if result[0] == file_id:  # Match the ID\n",
    "                        metadata = result[1] if len(result) > 1 else {}\n",
    "                        return metadata.get('content', '')\n",
    "                \n",
    "                return None\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving file content: {e}\")\n",
    "                return None\n",
    "\n",
    "\n",
    "        def search_with_content(repo_name, query, limit=5):\n",
    "            \"\"\"\n",
    "            Search and return results with full content included.\n",
    "            \n",
    "            Args:\n",
    "                repo_name (str): Name of the repository collection\n",
    "                query (str): Search query\n",
    "                limit (int): Maximum number of results to return\n",
    "            \n",
    "            Returns:\n",
    "                list: Search results with full content\n",
    "            \"\"\"\n",
    "            results = search_repo_collection(repo_name, query, limit)\n",
    "            \n",
    "            # Add full content to each result\n",
    "            for result in results:\n",
    "                metadata = result.get('metadata', {})\n",
    "                file_path = metadata.get('path', '')\n",
    "                \n",
    "                if file_path:\n",
    "                    full_content = get_file_content(repo_name, file_path)\n",
    "                    if full_content:\n",
    "                        result['full_content'] = full_content\n",
    "            \n",
    "            return results''',\n",
    "                \"utf8\"\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Point(row=1, column=8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node = tree.root_node\n",
    "root_node.start_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting nodes in the tree\n",
    "root_node = tree.root_node\n",
    "assert root_node.type == \"module\"\n",
    "assert root_node.start_point == (1, 8)\n",
    "assert root_node.end_point == (279, 18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(module (import_statement name: (dotted_name (identifier))) (import_from_statement module_name: (dotted_name (identifier)) name: (dotted_name (identifier))) (import_from_statement module_name: (dotted_name (identifier)) name: (dotted_name (identifier))) (import_statement name: (dotted_name (identifier))) (import_statement name: (dotted_name (identifier))) (comment) (expression_statement (call function: (identifier) arguments: (argument_list))) (function_definition name: (identifier) parameters: (parameters) body: (block (expression_statement (string (string_start) (string_content) (string_end))) (return_statement (call function: (identifier) arguments: (argument_list (keyword_argument name: (identifier) value: (string (string_start) (string_content) (string_end)))))))) (function_definition name: (identifier) parameters: (parameters) body: (block (expression_statement (string (string_start) (string_content) (string_end))) (expression_statement (assignment left: (identifier) right: (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (string (string_start) (string_content) (string_end)))))) (if_statement condition: (not_operator argument: (identifier)) consequence: (block (raise_statement (call function: (identifier) arguments: (argument_list (string (string_start) (string_content) (string_end))))))) (return_statement (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (identifier)))))) (function_definition name: (identifier) parameters: (parameters (identifier) (default_parameter name: (identifier) value: (false))) body: (block (expression_statement (string (string_start) (string_content) (string_end))) (expression_statement (assignment left: (identifier) right: (subscript value: (identifier) subscript: (string (string_start) (string_content) (string_end))))) (comment) (expression_statement (assignment left: (identifier) right: (call function: (identifier) arguments: (argument_list)))) (expression_statement (assignment left: (identifier) right: (call function: (identifier) arguments: (argument_list)))) (if_statement condition: (identifier) (comment) consequence: (block (try_statement body: (block (expression_statement (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (identifier)))) (expression_statement (call function: (identifier) arguments: (argument_list (string (string_start) (string_content) (interpolation expression: (identifier)) (string_content) (string_end)))))) (except_clause value: (identifier) (block (pass_statement) (comment)))))) (comment) (expression_statement (assignment left: (identifier) right: (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (keyword_argument name: (identifier) value: (identifier)) (keyword_argument name: (identifier) value: (integer)))))) (expression_statement (assignment left: (identifier) right: (list))) (for_statement left: (identifier) right: (subscript value: (identifier) subscript: (string (string_start) (string_content) (string_end))) body: (block (expression_statement (assignment left: (identifier) right: (subscript value: (identifier) subscript: (string (string_start) (string_content) (string_end))))) (comment) (expression_statement (assignment left: (identifier) right: (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (identifier))))) (comment) (expression_statement (assignment left: (identifier) right: (string (string_start) (interpolation expression: (identifier)) (string_content) (interpolation expression: (subscript value: (identifier) subscript: (string (string_start) (string_content) (string_end)))) (string_end)))) (comment) (expression_statement (assignment left: (identifier) right: (dictionary (pair key: (string (string_start) (string_content) (string_end)) value: (identifier)) (pair key: (string (string_start) (string_content) (string_end)) value: (subscript value: (identifier) subscript: (string (string_start) (string_content) (string_end)))) (pair key: (string (string_start) (string_content) (string_end)) value: (identifier)) (comment) (pair key: (string (string_start) (string_content) (string_end)) value: (subscript value: (identifier) subscript: (string (string_start) (string_content) (string_end)))) (pair key: (string (string_start) (string_content) (string_end)) value: (subscript value: (identifier) subscript: (string (string_start) (string_content) (string_end)))) (comment)))) (expression_statement (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (tuple (identifier) (identifier) (identifier))))))) (comment) (expression_statement (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (identifier)))) (expression_statement (call function: (identifier) arguments: (argument_list (string (string_start) (string_content) (interpolation expression: (call function: (identifier) arguments: (argument_list (identifier)))) (string_content) (interpolation expression: (identifier)) (string_content) (string_end))))) (return_statement (identifier)))) (function_definition name: (identifier) parameters: (parameters (identifier) (identifier) (default_parameter name: (identifier) value: (integer))) body: (block (expression_statement (string (string_start) (string_content) (string_end))) (expression_statement (assignment left: (identifier) right: (call function: (identifier) arguments: (argument_list)))) (expression_statement (assignment left: (identifier) right: (call function: (identifier) arguments: (argument_list)))) (try_statement body: (block (expression_statement (assignment left: (identifier) right: (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (keyword_argument name: (identifier) value: (identifier)))))) (expression_statement (assignment left: (identifier) right: (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (identifier))))) (comment) (expression_statement (assignment left: (identifier) right: (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (keyword_argument name: (identifier) value: (identifier)) (keyword_argument name: (identifier) value: (identifier)) (keyword_argument name: (identifier) value: (true)) (keyword_argument name: (identifier) value: (true)))))) (comment) (comment) (expression_statement (assignment left: (identifier) right: (list))) (for_statement left: (pattern_list (identifier) (identifier)) right: (call function: (identifier) arguments: (argument_list (identifier))) body: (block (expression_statement (assignment left: (identifier) right: (dictionary (pair key: (string (string_start) (string_content) (string_end)) value: (conditional_expression (subscript value: (identifier) subscript: (integer)) (comparison_operator (call function: (identifier) arguments: (argument_list (identifier))) (integer)) (none))) (pair key: (string (string_start) (string_content) (string_end)) value: (subscript value: (identifier) subscript: (integer))) (pair key: (string (string_start) (string_content) (string_end)) value: (conditional_expression (subscript value: (identifier) subscript: (integer)) (comparison_operator (call function: (identifier) arguments: (argument_list (identifier))) (integer)) (dictionary)))))) (expression_statement (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (identifier)))))) (return_statement (identifier))) (except_clause value: (as_pattern (identifier) alias: (as_pattern_target (identifier))) (block (expression_statement (call function: (identifier) arguments: (argument_list (string (string_start) (string_content) (interpolation expression: (identifier)) (string_content) (interpolation expression: (identifier)) (string_end))))) (return_statement (list))))))) (function_definition name: (identifier) parameters: (parameters) body: (block (expression_statement (string (string_start) (string_content) (string_end))) (expression_statement (assignment left: (identifier) right: (call function: (identifier) arguments: (argument_list)))) (try_statement body: (block (expression_statement (assignment left: (identifier) right: (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list)))) (return_statement (list_comprehension body: (attribute object: (identifier) attribute: (identifier)) (for_in_clause left: (identifier) right: (identifier))))) (except_clause value: (as_pattern (identifier) alias: (as_pattern_target (identifier))) (block (expression_statement (call function: (identifier) arguments: (argument_list (string (string_start) (string_content) (interpolation expression: (identifier)) (string_end))))) (return_statement (list))))))) (function_definition name: (identifier) parameters: (parameters (identifier)) body: (block (expression_statement (string (string_start) (string_content) (string_end))) (expression_statement (assignment left: (identifier) right: (call function: (identifier) arguments: (argument_list)))) (try_statement body: (block (expression_statement (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (identifier)))) (expression_statement (call function: (identifier) arguments: (argument_list (string (string_start) (string_content) (interpolation expression: (identifier)) (string_content) (string_end))))) (return_statement (true))) (except_clause value: (as_pattern (identifier) alias: (as_pattern_target (identifier))) (block (expression_statement (call function: (identifier) arguments: (argument_list (string (string_start) (string_content) (interpolation expression: (identifier)) (string_content) (interpolation expression: (identifier)) (string_end))))) (return_statement (false))))))) (comment) (if_statement condition: (comparison_operator (identifier) (string (string_start) (string_content) (string_end))) (comment) (comment) consequence: (block (pass_statement))) (function_definition name: (identifier) parameters: (parameters (identifier) (identifier) (default_parameter name: (identifier) value: (integer))) body: (block (expression_statement (string (string_start) (string_content) (string_end))) (expression_statement (assignment left: (identifier) right: (call function: (identifier) arguments: (argument_list)))) (expression_statement (assignment left: (identifier) right: (call function: (identifier) arguments: (argument_list)))) (try_statement body: (block (expression_statement (assignment left: (identifier) right: (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (keyword_argument name: (identifier) value: (identifier)))))) (expression_statement (assignment left: (identifier) right: (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (identifier))))) (expression_statement (call function: (identifier) arguments: (argument_list (string (string_start) (string_content) (interpolation expression: (identifier)) (string_content) (interpolation expression: (identifier)) (string_content) (string_end))))) (comment) (expression_statement (assignment left: (identifier) right: (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (keyword_argument name: (identifier) value: (identifier)) (keyword_argument name: (identifier) value: (identifier)) (keyword_argument name: (identifier) value: (true)))))) (expression_statement (call function: (identifier) arguments: (argument_list (string (string_start) (string_content) (interpolation expression: (call function: (identifier) arguments: (argument_list (identifier)))) (string_end))))) (expression_statement (call function: (identifier) arguments: (argument_list (string (string_start) (string_content) (interpolation expression: (conditional_expression (call function: (identifier) arguments: (argument_list (identifier))) (call function: (identifier) arguments: (argument_list (identifier) (string (string_start) (string_content) (string_end)))) (string (string_start) (string_content) (string_end)))) (string_end))))) (if_statement condition: (identifier) consequence: (block (expression_statement (call function: (identifier) arguments: (argument_list (string (string_start) (string_content) (interpolation expression: (call function: (identifier) arguments: (argument_list (subscript value: (identifier) subscript: (integer))))) (string_end))))) (expression_statement (call function: (identifier) arguments: (argument_list (string (string_start) (string_content) (interpolation expression: (subscript value: (identifier) subscript: (integer))) (string_end))))) (if_statement condition: (comparison_operator (call function: (identifier) arguments: (argument_list (subscript value: (identifier) subscript: (integer)))) (integer)) consequence: (block (expression_statement (call function: (identifier) arguments: (argument_list (string (string_start) (string_content) (string_end))))) (for_statement left: (pattern_list (identifier) (identifier)) right: (call function: (identifier) arguments: (argument_list (subscript value: (identifier) subscript: (integer)))) body: (block (expression_statement (call function: (identifier) arguments: (argument_list (string (string_start) (string_content) (interpolation expression: (identifier)) (string_content) (interpolation expression: (call function: (identifier) arguments: (argument_list (identifier)))) (string_content) (interpolation expression: (identifier)) (string_end))))))))))) (return_statement (identifier))) (except_clause value: (as_pattern (identifier) alias: (as_pattern_target (identifier))) (block (expression_statement (call function: (identifier) arguments: (argument_list (string (string_start) (string_content) (interpolation expression: (identifier)) (string_end))))) (import_statement name: (dotted_name (identifier))) (expression_statement (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list))) (return_statement (none))))))) (function_definition name: (identifier) parameters: (parameters (identifier) (identifier)) body: (block (expression_statement (string (string_start) (string_content) (string_end))) (expression_statement (assignment left: (identifier) right: (call function: (identifier) arguments: (argument_list)))) (try_statement body: (block (expression_statement (assignment left: (identifier) right: (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (keyword_argument name: (identifier) value: (identifier)))))) (comment) (expression_statement (assignment left: (identifier) right: (string (string_start) (interpolation expression: (identifier)) (string_content) (interpolation expression: (identifier)) (string_end)))) (comment) (comment) (expression_statement (assignment left: (identifier) right: (call function: (identifier) arguments: (argument_list)))) (expression_statement (assignment left: (identifier) right: (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (string (string_start) (string_content) (string_end)))))) (comment) (expression_statement (assignment left: (identifier) right: (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (keyword_argument name: (identifier) value: (identifier)) (keyword_argument name: (identifier) value: (integer)) (comment) (keyword_argument name: (identifier) value: (true)))))) (comment) (for_statement left: (identifier) right: (identifier) body: (block (if_statement condition: (comparison_operator (subscript value: (identifier) subscript: (integer)) (identifier)) (comment) consequence: (block (expression_statement (assignment left: (identifier) right: (conditional_expression (subscript value: (identifier) subscript: (integer)) (comparison_operator (call function: (identifier) arguments: (argument_list (identifier))) (integer)) (dictionary)))) (return_statement (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (string (string_start) (string_content) (string_end)) (string (string_start) (string_end))))))))) (return_statement (none))) (except_clause value: (as_pattern (identifier) alias: (as_pattern_target (identifier))) (block (expression_statement (call function: (identifier) arguments: (argument_list (string (string_start) (string_content) (interpolation expression: (identifier)) (string_end))))) (return_statement (none))))))) (function_definition name: (identifier) parameters: (parameters (identifier) (identifier) (default_parameter name: (identifier) value: (integer))) body: (block (expression_statement (string (string_start) (string_content) (string_end))) (expression_statement (assignment left: (identifier) right: (call function: (identifier) arguments: (argument_list (identifier) (identifier) (identifier))))) (comment) (for_statement left: (identifier) right: (identifier) body: (block (expression_statement (assignment left: (identifier) right: (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (string (string_start) (string_content) (string_end)) (dictionary))))) (expression_statement (assignment left: (identifier) right: (call function: (attribute object: (identifier) attribute: (identifier)) arguments: (argument_list (string (string_start) (string_content) (string_end)) (string (string_start) (string_end)))))) (if_statement condition: (identifier) consequence: (block (expression_statement (assignment left: (identifier) right: (call function: (identifier) arguments: (argument_list (identifier) (identifier))))) (if_statement condition: (identifier) consequence: (block (expression_statement (assignment left: (subscript value: (identifier) subscript: (string (string_start) (string_content) (string_end))) right: (identifier))))))))) (return_statement (identifier)))))\n"
     ]
    }
   ],
   "source": [
    "print(root_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain pre-built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nfunc ',\n",
       " '\\nclass ',\n",
       " '\\nstruct ',\n",
       " '\\nenum ',\n",
       " '\\nif ',\n",
       " '\\nfor ',\n",
       " '\\nwhile ',\n",
       " '\\ndo ',\n",
       " '\\nswitch ',\n",
       " '\\ncase ',\n",
       " '\\n\\n',\n",
       " '\\n',\n",
       " ' ',\n",
       " '']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view seperators of swift for example\n",
    "RecursiveCharacterTextSplitter.get_separators_for_language('swift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cpp',\n",
       " 'go',\n",
       " 'java',\n",
       " 'kotlin',\n",
       " 'js',\n",
       " 'ts',\n",
       " 'php',\n",
       " 'proto',\n",
       " 'python',\n",
       " 'rst',\n",
       " 'ruby',\n",
       " 'rust',\n",
       " 'scala',\n",
       " 'swift',\n",
       " 'markdown',\n",
       " 'latex',\n",
       " 'html',\n",
       " 'sol',\n",
       " 'csharp',\n",
       " 'cobol',\n",
       " 'c',\n",
       " 'lua',\n",
       " 'perl',\n",
       " 'haskell',\n",
       " 'elixir',\n",
       " 'powershell',\n",
       " 'visualbasic6']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Supported languages\n",
    "[e.value for e in Language]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='def hello_world():\\n    print(\"Hello, World!\")'),\n",
       " Document(metadata={}, page_content='# Call the function\\nhello_world()')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='import os\\nfrom langchain_openai import OpenAIEmbeddings\\nfrom dotenv import load_dotenv\\nimport vecs\\nimport json\\n\\n# Load environment variables\\nload_dotenv()\\n\\n\\ndef get_embedding_model():\\n    \"\"\"Initialize and return the OpenAI embedding model.\"\"\"\\n    return OpenAIEmbeddings(model=\"text-embedding-3-small\")\\n\\n\\ndef get_vector_client():\\n    \"\"\"Initialize and return the Supabase vector client.\"\"\"\\n    supabase_db_url = os.getenv(\"SUPABASE_DB_URL\")\\n    if not supabase_db_url:\\n        raise ValueError(\"SUPABASE_DB_URL environment variable not set\")\\n    return vecs.create_client(supabase_db_url)\\n\\n\\ndef store_repo_in_own_collection(repo_info, refresh=False):\\n    \"\"\"\\n    Store repository information in a dedicated vector collection.\\n    \\n    Args:\\n        repo_info (dict): Repository information containing files and metadata\\n        refresh (bool): Whether to delete existing collection before creating new one\\n    \\n    Returns:\\n        str: Name of the collection created\\n    \"\"\"\\n    repo_name = repo_info[\\'repo_name\\']\\n    \\n    # Initialize clients\\n    vx = get_vector_client()\\n    embeddings = get_embedding_model()\\n    \\n    if refresh:\\n        # Drop existing collection if it exists\\n        try:\\n            vx.delete_collection(repo_name)\\n            print(f\"🗑️ Old collection \\'{repo_name}\\' deleted.\")\\n        except Exception:\\n            pass  # collection may not exist yet\\n    \\n    # Create/get a dedicated collection for this repo\\n    collection = vx.get_or_create_collection(name=repo_name, dimension=1536)\\n    \\n    vectors_to_upsert = []\\n    \\n    for file in repo_info[\\'files\\']:\\n        content = file[\\'content\\']\\n        \\n        # Generate embedding for file content\\n        embedding = embeddings.embed_query(content)\\n        \\n        # Create unique ID for this file\\n        unique_id = f\"{repo_name}/{file[\\'path\\']}\"\\n        \\n        # Prepare metadata (store full content like your working version)\\n        metadata = {\\n            \\'repo_name\\': repo_name,\\n            \\'path\\': file[\\'path\\'],\\n            \\'content\\': content,  # Store the full content for retrieval\\n            \\'commit_count\\': repo_info[\\'commit_count\\'],\\n            \\'branches\\': repo_info[\\'branches\\']  # List is JSON-serializable, don\\'t stringify\\n        }\\n        \\n        vectors_to_upsert.append((unique_id, embedding, metadata))\\n    \\n    # Upsert all vectors at once\\n    collection.upsert(vectors_to_upsert)\\n    \\n    print(f\"✅ Stored {len(vectors_to_upsert)} files into collection \\'{repo_name}\\'\")\\n    return repo_name'),\n",
       " Document(metadata={}, page_content='def search_repo_collection(repo_name, query, limit=5):\\n    \"\"\"\\n    Search within a specific repository collection.\\n    \\n    Args:\\n        repo_name (str): Name of the repository collection\\n        query (str): Search query\\n        limit (int): Maximum number of results to return\\n    \\n    Returns:\\n        list: Search results with metadata\\n    \"\"\"\\n    vx = get_vector_client()\\n    embeddings = get_embedding_model()\\n    \\n    try:\\n        collection = vx.get_collection(name=repo_name)\\n        query_embedding = embeddings.embed_query(query)\\n        \\n        # Query the collection\\n        results = collection.query(\\n            data=query_embedding,\\n            limit=limit,\\n            include_metadata=True,\\n            include_value=True\\n        )\\n        \\n        # Convert results to a more usable format\\n        # vecs returns SQLAlchemy Row objects with: (id, metadata)\\n        formatted_results = []\\n        for i, result in enumerate(results):\\n            formatted_result = {\\n                \\'id\\': result[0] if len(result) > 0 else None,\\n                \\'cos_distance\\': result[1],\\n                \\'metadata\\': result[2] if len(result) > 1 else {}\\n            }\\n            formatted_results.append(formatted_result)\\n        \\n        return formatted_results\\n        \\n    except Exception as e:\\n        print(f\"Error searching collection \\'{repo_name}\\': {e}\")\\n        return []\\n\\n\\ndef list_repo_collections():\\n    \"\"\"\\n    List all available repository collections.\\n    \\n    Returns:\\n        list: Names of all collections\\n    \"\"\"\\n    vx = get_vector_client()\\n    try:\\n        collections = vx.list_collections()\\n        return [col.name for col in collections]\\n    except Exception as e:\\n        print(f\"Error listing collections: {e}\")\\n        return []\\n\\n\\ndef delete_repo_collection(repo_name):\\n    \"\"\"\\n    Delete a repository collection.\\n    \\n    Args:\\n        repo_name (str): Name of the repository collection to delete\\n    \\n    Returns:\\n        bool: True if successful, False otherwise\\n    \"\"\"\\n    vx = get_vector_client()\\n    try:\\n        vx.delete_collection(repo_name)\\n        print(f\"🗑️ Collection \\'{repo_name}\\' deleted successfully.\")\\n        return True\\n    except Exception as e:\\n        print(f\"Error deleting collection \\'{repo_name}\\': {e}\")\\n        return False\\n\\n\\n# Example usage\\nif __name__ == \"__main__\":\\n    # This would typically be called with repo_info from git_utils\\n    # store_repo_in_own_collection(repo_info)\\n    pass'),\n",
       " Document(metadata={}, page_content='def debug_search_results(repo_name, query, limit=2):\\n    \"\"\"\\n    Debug function to inspect the raw search results format.\\n    \\n    Args:\\n        repo_name (str): Name of the repository collection\\n        query (str): Search query\\n        limit (int): Maximum number of results to return\\n    \"\"\"\\n    vx = get_vector_client()\\n    embeddings = get_embedding_model()\\n    \\n    try:\\n        collection = vx.get_collection(name=repo_name)\\n        query_embedding = embeddings.embed_query(query)\\n        \\n        print(f\"🔍 Debug search in \\'{repo_name}\\' for query: \\'{query}\\'\")\\n        \\n        # Query the collection\\n        raw_results = collection.query(\\n            data=query_embedding,\\n            limit=limit,\\n            include_metadata=True\\n        )\\n        \\n        print(f\"📊 Raw results type: {type(raw_results)}\")\\n        print(f\"📊 Raw results length: {len(raw_results) if hasattr(raw_results, \\'__len__\\') else \\'N/A\\'}\")\\n        \\n        if raw_results:\\n            print(f\"📊 First result type: {type(raw_results[0])}\")\\n            print(f\"📊 First result: {raw_results[0]}\")\\n            \\n            if len(raw_results[0]) > 0:\\n                print(f\"📊 First result parts:\")\\n                for i, part in enumerate(raw_results[0]):\\n                    print(f\"   Part {i}: {type(part)} = {part}\")\\n        \\n        return raw_results\\n        \\n    except Exception as e:\\n        print(f\"❌ Debug error: {e}\")\\n        import traceback\\n        traceback.print_exc()\\n        return None\\ndef get_file_content(repo_name, file_path):\\n    \"\"\"\\n    Get the full content of a specific file from the vector database.\\n    \\n    Args:\\n        repo_name (str): Name of the repository collection\\n        file_path (str): Path of the file to retrieve\\n    \\n    Returns:\\n        str: File content or None if not found\\n    \"\"\"\\n    vx = get_vector_client()\\n    \\n    try:\\n        collection = vx.get_collection(name=repo_name)\\n        \\n        # Search for the specific file by ID\\n        file_id = f\"{repo_name}/{file_path}\"\\n        \\n        # Use a simple query to find the exact file\\n        # Since we can\\'t query by ID directly, we\\'ll search and filter\\n        embeddings = get_embedding_model()\\n        dummy_query = embeddings.embed_query(\"content\")  # Dummy query\\n        \\n        results = collection.query(\\n            data=dummy_query,\\n            limit=100,  # Get more results to find our file\\n            include_metadata=True\\n        )\\n        \\n        # Find the specific file\\n        for result in results:\\n            if result[0] == file_id:  # Match the ID\\n                metadata = result[1] if len(result) > 1 else {}\\n                return metadata.get(\\'content\\', \\'\\')\\n        \\n        return None\\n        \\n    except Exception as e:\\n        print(f\"Error retrieving file content: {e}\")\\n        return None'),\n",
       " Document(metadata={}, page_content='def search_with_content(repo_name, query, limit=5):\\n    \"\"\"\\n    Search and return results with full content included.\\n    \\n    Args:\\n        repo_name (str): Name of the repository collection\\n        query (str): Search query\\n        limit (int): Maximum number of results to return\\n    \\n    Returns:\\n        list: Search results with full content\\n    \"\"\"\\n    results = search_repo_collection(repo_name, query, limit)\\n    \\n    # Add full content to each result\\n    for result in results:\\n        metadata = result.get(\\'metadata\\', {})\\n        file_path = metadata.get(\\'path\\', \\'\\')\\n        \\n        if file_path:\\n            full_content = get_file_content(repo_name, file_path)\\n            if full_content:\\n                result[\\'full_content\\'] = full_content\\n    \\n    return results')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = r\"vector_utils.py\"\n",
    "\n",
    "# Read the file as text to pass to a function\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=3000, chunk_overlap=300\n",
    ")\n",
    "\n",
    "python_docs = python_splitter.create_documents([file_content])\n",
    "python_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
